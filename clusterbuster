#!/bin/bash

# Copyright 2019-2022 Robert Krawitz/Red Hat
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Find our helpers
function finddir() {
    local path_to_file
    path_to_file=$(readlink -f "$0")
    if [[ -z $path_to_file ]] ; then
	return 1
    elif [[ -d $path_to_file ]] ; then
	echo "$path_to_file/"
    elif [[ -e $path_to_file ]] ; then
	echo "${path_to_file%/*}/"
    else
	return 1
    fi
    return 0
}

declare __realsc__=
declare __topsc__
if [[ -z ${__topsc__:-} ]] ; then
    export __topsc__="${0##*/}"
    # shellcheck disable=SC2155
    export __topdir__="$(finddir "$0")"
    [[ -z $__topdir__ ]] && fatal "Can't find directory for $0"
fi

function clean_startup() {
    [[ -f $__realsc__ ]] && rm -f "$__realsc__"
}

# This allows us to edit the script while another instance is running
# since this script sticks around until the user exits the spawned shell.
# It's fine for the running script to be removed, since the shell still
# has its open file descriptor.
if [[ $# = 0 || $1 != "--DoIt=$0" ]] ; then
    tmpsc=$(mktemp -t "${__topsc__}".XXXXXXXXXX)
    [[ -z $tmpsc || ! -f $tmpsc || -L $tmpsc ]] && fatal "Can't create temporary script file"
    trap clean_startup EXIT SIGHUP SIGINT SIGQUIT SIGTERM
    PATH+=${PATH:+:}$__topdir__
    cat "$0" > "$tmpsc"
    chmod +x "$tmpsc"
    exec "$tmpsc" "--DoIt=$tmpsc" "$@"
else
    __realsc__=${1#--DoIt=}
    clean_startup
    export -n __topsc__ __topdir__
    shift
fi

# Unfortunate that there's no way to tell shellcheck to always source
# specific files.
# shellcheck disable=SC2034

# See the bash change log, differences between 4.4-beta2 and 4.4-rc2:
# a.  Using ${a[@]} or ${a[*]} with an array without any assigned elements when
#     the nounset option is enabled no longer throws an unbound variable error.
if (( BASH_VERSINFO[0] >= 5 || (BASH_VERSINFO[0] == 4 && BASH_VERSINFO[1] >= 4) )) ; then
    set -u
else
    cat 1>&2 <<EOF
Warning: bash version at least 4.4 is recommended for using ${__topsc__##*/}.
Actual version is ${BASH_VERSION}
EOF
fi

declare ___arg
for ___arg in "$@" ; do
    if [[ "${___arg:-}" = '--force-abort'* ]] ; then
	echo "*** Warning: will abort on any shell error!" 1>&2
	set -e
	set -o errtrace
	#trap 'killthemall "Caught error, aborting!"' ERR
	break
    fi
done

declare -i namespaces=1
declare -i use_namespaces=1
declare -i deps_per_namespace=1
declare -i remove_namespaces=1
declare -i secrets=0
declare -i replicas=1
declare -i parallel=1
declare -i first_deployment=0
declare -i sleep_between_secrets=0
declare -i sleep_between_configmaps=0
declare -i sleep_between_namespaces=0
declare -i sleep_between_deployments=0
declare -i parallel_secrets=0
declare -i parallel_configmaps=0
declare -i parallel_namespaces=0
declare -i parallel_deployments=0
declare -i parallel_log_retrieval=50
declare -i retrieve_successful_logs=0
declare -i objs_per_call=1
declare -i objs_per_call_secrets=0
declare -i objs_per_call_configmaps=0
declare -i objs_per_call_namespaces=0
declare -i objs_per_call_deployments=0
declare -i containers_per_pod=1
declare -i sleeptime=0
declare -i doit=1
declare -i objs_item_count=0
declare -i port=7777
declare -i sync_port=7778
declare -i drop_cache_port=7779
declare -i sync_ns_port=7753
declare -i sync_in_first_namespace=0
declare sync_host=
declare -i affinity=0
declare -i sync_affinity=2
declare -A pin_nodes=()
declare -A runtime_classes=()
declare runtime_class
declare -i verbose=0
declare -i wait_for_secrets=1
declare -i bytes_transfer=0
declare -i bytes_transfer_max=0
# shellcheck disable=SC2034
declare -i default_bytes_transfer=1000000000
declare -i workload_run_time=0
declare -i workload_run_time_max=0
declare -i exit_at_end=1
declare -i report_object_creation=1
declare -A objects_created=()
declare baseoffset=0
declare -i metrics_epoch=0
declare requested_workload=
declare basename=${CLUSTERBUSTER_DEFAULT_BASENAME:-clusterbuster}
declare deployment_type=pod
declare basetime
declare opt
declare -r nl=$'\n'
declare -a resource_requests=()
declare -a resource_limits=()
declare -A namespaces_in_use=()
declare -a namespaces_to_create=()
declare sync_namespace=
declare -i scale_ns=0
declare -i scale_deployments=1
declare -i sync_start=1
declare -a emptydirs=()
declare -a volumes=()
declare -A volume_mount_paths=()
declare -A volume_types=()
declare -A volume_type_keys=()
declare -A volume_scoped_names=()
declare common_workdir=
declare -i report=0
declare report_format=summary
declare -i precleanup=1
declare -i cleanup=0
declare -i cleanup_always=0
declare -i timeout=0
declare pathdir=${__topdir__%/*}
declare -a unknown_opts=()
declare -a unknown_opt_names=()
declare -i total_objects_created=0
# <name, filename with contents>
declare -a configmap_files=()
declare -a tolerations=()
declare configmap_mount_dir=/etc/clusterbuster
declare system_configmap_mount_dir=/var/lib/clusterbuster
declare -i has_system_configmap=0
declare node_selector='node-role.kubernetes.io/worker'
declare -i processes_per_pod=1
declare -i emptydir_volumes=0
declare -i take_prometheus_snapshot=0
declare first_start_timestamp=
declare prometheus_starting_timestamp=
declare prometheus_exact_starting_timestamp=
declare second_start_timestamp=
declare prometheus_ending_timestamp=
declare -i target_data_rate=0
declare job_name=
# shellcheck disable=2155
declare -i job_start_time=$(date +%s)
declare global_sync_service=
declare -i predelay=0
declare -i postdelay=0
declare -r default_metrics_file="metrics-default.yaml"
declare metrics_file=default
declare -i drop_node_cache=0
declare -i drop_all_node_cache=0
declare -i headless_services=1
declare -i virtiofsd_writeback=0
declare -i virtiofsd_direct=1
declare -i virtiofsd_threadpoolsize=0
declare -a virtiofsd_args=()
declare -i liveness_probe_frequency=0
declare -i liveness_probe_sleep_time=0
declare -i metrics_support=-1
declare -i pod_start_timeout=60
declare pod_prefix=
declare arch=
declare failure_status=Fail
declare -i create_pods_privileged=0
declare -i wait_forever=0

declare -r sync_flag_file="/tmp/syncfile";
declare -r sync_error_file="/tmp/syncerror";
declare -r controller_timestamp_file="/tmp/timing.json";
# shellcheck disable=2155
declare uuid=$(uuidgen -r)
declare xuuid=$uuid

declare kata_runtime_class=kata
declare image_pull_policy=
declare container_image=

declare accumulateddata=
declare -a plurals=('s' '')
declare artifactdir=
declare -a saved_argv=("${__topsc__:-0}" "$@")
declare -a processed_options=("${__topsc__:-0}")
declare -a pod_annotations=()
declare -A injected_errors=()
declare cb_tempdir=
declare force_cleanup_timeout=
declare default_namespace_policy=restricted

# vm related variables
declare -i vm_cores=1
declare -i vm_threads=1
declare -i vm_sockets=1
declare vm_grace_period=30
declare vm_image='quay.io/kubevirt/fedora-container-disk-images:35'
declare vm_evict_migrate=1

declare OC=${OC:-${KUBECTL:-}}
OC=${OC:-$(type -p oc)}
OC=${OC:-$(type -p kubectl)}	# kubectl might not work, though...

function fatal() {
    echo "$*" 1>&2
    exit 1
}

function warn() {
    echo "$*" 1>&2
}

if [[ -z "$OC" ]] ; then
    fatal "Can't find kubectl or oc"
fi

declare __libdir__=${__topdir__}/lib/clusterbuster
declare __podfile_dir__=${__libdir__}/pod_files
declare __workloaddir__=${__libdir__}/workloads

[[ -d "$__libdir__" ]] || fatal "Can't find my library dir!"
[[ -d "$__workloaddir__" ]] || fatal "Can't find my workload directory $__workloaddir__!"

. "${__libdir__}"/libclusterbuster.sh

function _helpmsg() {
    local opt
    for opt in "$@" ; do
	echo "Unknown option $opt"
    done
cat <<EOF
Clusterbuster is a tool to permit you to load a configurable workload
onto an OpenShift cluster.  ClusterBuster focuses primarily on workload
scalability, including synchronization of multi-instance workloads.

Usage: ${__topsc__:-0} [options] [name]

    Help:
       -h              Print basic help information.
       -H              Print extended help.

    Options:
       -B basename     Base name of pods.  Default is
                       \$CLUSTERBUSTER_DEFAULT_BASENAME if defined or
                       otherwise 'clusterbuster'.
                       All objects are labeled with this name.
       -E              Don't exit after all operations are complete.
       -e              Exit after all operations are complete (default).
       -f jobfile      Job file containing settings
                       A number of examples are provided in the
                       examples/clusterbuster directory.
       -n              Print what would be done without doing it
       -o              Specify report format, as --report-format
       -P workload     workload (mandatory), one of:
$(print_workloads '                       - ')
       -q              Do not print verbose log messages (default)
       -Q              Don't report creation of individual objects (default
                       report them)
       -v              Print verbose log messages.
       --opt[=val]     Set the specified option.
                       Use ${__topsc__##*/} -H to list the available options.
EOF
}

function _help_extended() {
    if [[ -z "$*" ]] ; then
	_helpmsg "$@"
	cat <<'EOF'

Extended Options:
EOF
    fi
    cat <<EOF
    General Options (short equivalents):
       --doit=<1,0>     Run the command or not (default 1) (inverse of -n)
       --jobname=name   Name of the job, for logging purposes.
                        Defaults to the workload name
       --workload=type  Specify the workload (-P) (mandatory)
       --basename=name  Specify the base name for any namespaces (-B)
       --namespaces=N   Number of namespaces
       --jobfile=jobfile
                        Process job file (-f)
       --sync           Synchronize start of workload instances (default yes)
       --precleanup     Clean up any prior objects
       --cleanup        Clean up generated objects unless there's a failure
       --cleanup-always Clean up generated objects even if there is a failure
       --wait-forever   Don't exit if sync pod dies
       --remove-namespaces=<1,0>
                        Remove namespaces when cleaning up objects.  Only
                        applies when using clusterbuster-created namespaces.
       --predelay=N     Delay for the specified time after workload
                        starts before it starts running.
       --postdelay=N    Delay for the specified time after workload
                        completes.
       --timeout=N      Time out reporting after N seconds
       --report_object_creation=<1,0>
                        Report creation of individual objects (default 1)
                        (inverse of -Q)
       --uuid=<uuid>    Use the specified UUID for the run.  Default is to
                        generate a random-based UUID.
       --exit_at_end    Exit upon completion of workload (-e/-E)
       --verbose        Print verbose log messages (-v)
       --arch=<architecture>
                        Use the specified architecture.  Default the
                        architecture of this platform.
       --containerimage=<image>
                        Use the specified container image.

    Reporting Options:
       --report=<format>
                        Print report in specified format.  Meaning of
                        report types is by type.  Default is summary
                        if not specified; if that is not reported,
                        raw format will be used.
                        - none
$(list_report_formats '                        - ')
                        The following workloads support reporting:
$(print_workloads_supporting_reporting '                        - ')
       --artifactdir=<dir>
                        Save artifacts to <dir>.  <dir> can have embedded
                        format codes:
                        %n              Job name
                        %s              Timestamp of run
                        %w              Workload
                        %{var}          Variable's value is substituted
                        %{var[item]}    to reference an array variable
                        %{var:-default} to use a default value if not set
       --prometheus-snapshot
                        Take a Prometheus snapshot and save to the
                        artifacts directory
       --metrics[=<file>]
                        benchmark-runner compatible metrics file
                        for metrics extraction.  If empty or 'none',
                        no metrics extraction is done.  If 'default',
                        the default
                        ($default_metrics_file)
                        is used.
       --metrics-epoch=<seconds>
                        Number of seconds to look back for metrics prior
                        to start of run (default $metrics_epoch)
       --force-no-metrics
                        Do not attempt anything that would use metrics
                        or the prometheus pod.
       --failure-status=<status>
                        Failures should be reported as specified rather
                        than "Fail"
       --pod-start-timeout=<seconds>
                        Wait specified time for pods to come on line.
                        Default $pod_start_timeout
       --retrieve-successful-logs=<0|1>
                        If retrieving artifacts, retrieve logs for all
                        pods, not just failing pods.  Default $retrieve_successful_logs.
       --parallel-logs=n
                        If retrieving artifacts, parallelize log retrieval.

    Workload sizing options:
       --containers_per_pod=N
                        Number of containers per pod
       --deployments=N  Number of deployments or pods per namespace
       --processes=N    Number of processes per pod
       --replicas=N     Number of replicas per deployment
       --secrets=N      Number of secrets

    Generic workload rate options:
       --bytestransfer=N[,M]
                        Number of bytes for workloads operating on
                        fixed amounts of data.
       --targetdatarate=N
                        Target data rate for workloads operating at fixed
                        data rates.  May have suffixes of K, Ki,
                        M, Mi, G, Gi, T, or Ti.
       --workloadruntime=N
                        Time to run the workload where applicable
                        Two comma-separated numbers may be used to
                        specify maximum time.

    Workload placement options:
       --pin_node=[class1,class2...]=<node>
                        Force pod(s) of the specified class(es) onto the
                        specified node.  Multiple comma-separated classes
                        may be specified.  The following classes are
                        defined for general workloads:
                        - sync   (sync pods)
                        - client (worker/client pods)
                        Workloads may define other classes.
                        If no class is specified, pin node applies to all
                        pods.
       --sync-in-first-namespace=<0|1>
                        Place the sync pod in the first worker namespace.
                        Default ${sync_in_first_namespace}.
       --affinity       Force affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides affinity.
       --anti-affinity  Force anti-affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides anti-affinity.
       --anti-affinity  Force anti-affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides anti-affinity.
       --sync-affinity  Force affinity between sync and all worker pods.
       --sync-anti-affinity
                        Force anti-affinity between sync and all worker pods.
       --drop_cache     Drop the buffer cache in all pin nodes; if no
                        pin nodes are defined, drop all workers' caches.
       --drop_all_cache Drop the buffer cache on all workers.

    Generic workload storage options:
       --emptydir=dir   Mount an emptydir volume at the specified mount point.
       --volumes=N      Mount the specified number of emptydir volumes.
                        If this is not provided, file-based workloads will
                        use a default location, generally /tmp.
       --volume=name:type:name_type:mount_name:mount_path
                        Mount a specified persistent volume
                        name is the name of the volume (required).
                        type is the type of volume (required).
                        mount_path is the path on which to mount the volume
                            (required).
                        type_key is the key for the volume (e. g.
                            claimName for persistentVolumeClaim)
                        scoped_name is the volume's name as recognized
                            by the description.  All occurrences of %N
                            are replaced by the namespace of the pod
                            mounting the volume; all instances of %i
                            are replaced by the instance of the pod
                            within the namespace.
       --workdir=<dir>  Use the specified working directory for file I/O

    Pod Options:
       --container_image=<image>
                        Image to use (default $container_image).
                        Does not apply to "classic" or "pause" workloads.
       --deployment_type=<pod,deployment,vm>
                        Deploy via individual pods, deployments or vm (default $deployment_type)
       --external_sync=host:port
                        Sync to external host rather than internally
       --request=<resource=value>
                        Resource requests
       --limit=<resource=value>
                        Resource limits
       --runtimeclass=[class1,class2...]=class
                        Run the pods in the designated runtimeclass.
       --kata           Synonym for --runtimeclass=${kata_runtime_class}
       --tolerate=<key:operator:effect>
                        Apply the specified tolerations to created pods.
       --image_pull_policy=<policy>
                        Image pull policy (system default)
       --node_selector=selector
                        Annotate pods with the specified node selector
                        Default $node_selector
                        Specify empty value to not provide a node selector.
       --pod_annotation=[:class:]annotation
                        Apply the specified annotation to all pods of the
                        optionally specified class (same meaning as for
                        --pin_node as above).  This may be specified
                        multiple times.
       --headless-services=[0,1]
                        Use headless services for service creation.
                        Default ${headless_services}
       --liveness-probe=<interval>
                        Execute a simple liveness probe every <interval>
                        seconds.
       --liveness-probe-sleep=<seconds>
                        Arrange for the liveness probe to sleep for specified
                        time.
       --privileged-pods=[0,1]
                        Create pods as privileged (default $create_pods_privileged)

    Kata Virtualization Tuning:
       --virtiofsd-writeback=[0,1]
                        Use writeback caching for virtiofsd (default $virtiofsd_writeback).
       --virtiofsd-direct=[0,1]
                        Allow use of direct I/O for virtiofsd (default $virtiofsd_direct).
       --virtiofsd-threadpoolsize=n
                        Use the specified thread pool size for virtiofsd (default 1).

   OpenShift Virtualiation Options:
       --vmthreads=<value>
                        Specify the number of threads on each core (default $vm_threads).
       --vmcores=<value>
                        Specify the number of cores on each socket (default $vm_cores).
       --vmsockets=<value>
                        Specify the number of sockets (default $vm_sockets).
       --vmgraceperiod=<value>
                        Specify the period between when a vm is signaled to
                        shutdown and the point when KubeVirt will force off
                        the vm (default $vm_grace_period).
       --vmimage=<image_url>
                        Container vm disk image to use.
                        Default $vm_image
       --vm-migrate=[0,1]
                        Allow VMs to migrate when evicted rather than be deleted.
                        Default $vm_evict_migrate.

    Tuning object creation (short equivalents):
       --scale-ns=[0,1] Scale up the number of namespaces vs.
                        create new ones (default 0).
       --scale-deployments=[0,1]
                        Scale up the number of deployments vs.
                        create new ones (default 1)
       --first_deployment=N
                        Specify the index of the first deployment.
                        Default is 0 (but see --scale_deployments)
       --first_secret=N
                        Index number of first secret to be created
       --first_namespace=N
                        Index number of first namespace to be created
       --pod-prefix=prefix
                        Prefix all created pods with this prefix.
       --sleep=N        Number of seconds between object creations
                        Below options default to sleeptime
       --sleep_between_secrets=N
       --sleep_between_namespaces=N
       --sleep_between_deployments=N

       --objs_per_call=N
                        Number of objects per CLI call.  Only objects
                        within a namespace can be created this way;
                        to improve creation performance with multiple
                        namespaces, use --parallel.
                        Below options all default to objs_per_call
       --objs_per_call_secrets=N
       --objs_per_call_namespaces=N
       --objs_per_call_deployments=N

       --parallel=N     Number of operations in parallel.  Only
                        operations across namespaces can be
                        parallelized; to improve performance
                        within one namespace, use --objs_per_call.
                        Below options all default to parallel
       --parallel_secrets=N
       --parallel_namespaces=N
       --parallel_deployments=N
       --wait_secrets   Wait for secrets to be created (default 1)

Workload-specific options:
$(_help_options_workloads)

Advanced options (generally not required):
       --baseoffset=N   Add specified offset to base time
                        for calculation of start time offset
                        to correct for clock skew.  May be float.
                        This normally should not be needed, as
                        ClusterBuster can correct for clock skew
                        itself.
       --podsleep=N     Time for pod to sleep before exit
       --debug=<opt>
                        For testing purposes, print debugging information.
                        Options documented only in code.
       --inject_error=<opt>
                        For testing purposes, inject the specified error
                        condition (documented only in code).
       --force-abort    Abort the run on any error.

Here is a brief description of all available workloads:
$(_document_workloads)

EOF
}

function help() {
    _help_extended "$@" | "${PAGER:-more}" 1>&2
    exit 1
}

function help_extended() {
    _help_extended "$@" | "${PAGER:-more}" 1>&2
    exit 1
}

################################################################
# Option processing
################################################################

function set_workload_bytes() {
    local sizespec=$1
    local -i scale=${2:-1}
    if [[ $sizespec = *','* ]] ; then
	bytes_transfer=$(parse_size "${sizespec#*,}")
	bytes_transfer_max=$(parse_size "${sizespec%%,*}")
	if (( bytes_transfer > bytes_transfer_max )) ; then
	    local -i tmp=$bytes_transfer
	    bytes_transfer=$bytes_transfer_max
	    bytes_transfer_max=$tmp
	fi
    else
	bytes_transfer=$((sizespec * scale))
	bytes_transfer_max=$((sizespec * scale))
    fi
}

function set_runtime() {
    local timespec=$1
    if [[ $timespec = *','* ]] ; then
	workload_run_time=${timespec#*,}
	workload_run_time_max=${timespec%%,*}
	if (( workload_run_time > workload_run_time_max )) ; then
	    local -i tmp=$workload_run_time
	    workload_run_time=$workload_run_time_max
	    workload_run_time_max=$tmp
	fi
    else
	workload_run_time=$timespec
	workload_run_time_max=$timespec
    fi
}

function parse_volume_spec() {
    local volspec=$1
    local vname=
    local vtype=
    local vmount_path=
    IFS=':' read -r vname vtype vmount_path vtype_key vscoped_name <<< "$volspec"
    if [[ -z "$vname" || -z "$vtype" || -z "$vmount_path" ]] ; then
	echo "name, type, type_key, scoped name, and mount path must be provided"
	echo "for volumes"
	exit 1
    fi
    volumes+=("$vname")
    volume_mount_paths["$vname"]=$vmount_path
    volume_types["$vname"]=$vtype
    volume_type_keys["$vname"]=$vtype_key
    volume_scoped_names["$vname"]=$vscoped_name
}

function process_pin_node() {
    local nodespec=$1
    nodespec=${nodespec// /}
    [[ -n "$nodespec" ]] || return
    if [[ $nodespec = *'='* ]] ; then
	local node=${nodespec#*=}
	local class=${nodespec%%=*}
	class=${class//,/ }
	# shellcheck disable=SC2206
	local -a classes=($class)
	for class in "${classes[@]}" ; do
	    pin_nodes[$class]="$node"
	done
    else
	pin_nodes[default]="$nodespec"
    fi
}

function process_runtimeclass() {
    local runtimespec=$1
    runtimespec=${runtimespec// /}
    runtime_class=$runtimespec
    [[ -n "$runtimespec" ]] || return
    if [[ $runtimespec = *'='* ]] ; then
	local runtime=${runtimespec#*=}
	local class=${runtimespec%%=*}
	class=${class//,/ }
	# shellcheck disable=SC2206
	local -a classes=($class)
	for class in "${classes[@]}" ; do
	    runtime_classes[$class]="$runtime"
	done
    else
	runtime_classes[default]="$runtimespec"
    fi
}

function set_metrics_file() {
    metrics_file=${1:-}
    case "$metrics_file" in
	default|1|"$default_metrics_file") metrics_file="$__libdir__/$default_metrics_file" ;;
	''|0|none) metrics_file=							    ;;
	*)										    ;;
    esac
}

function inject_error() {
    local error="$1"
    local condition
    local options
    IFS='=' read -r condition options <<< "$error"
    injected_errors["$condition"]=${options:-SET}
    warn "*** Registering error injection '$condition' = '${injected_errors[$condition]}'"
}

function process_option() {
    local noptname
    local noptname1
    local optvalue
    read -r noptname1 noptname optvalue <<< "$(parse_option "$1")"
    # shellcheck disable=SC2206
    # shellcheck disable=SC2119
    processed_options+=("--$1")
    # shellcheck disable=SC2034
    case "$noptname1" in
	# Help, verbosity
	helpall*)		    help_extended				;;
	helpeverything*)	    help_extended				;;
	help*)			    help					;;
	verbose)		    verbose=$(bool "$optvalue")			;;
	doit)			    doit=$(bool "$optvalue")			;;
	quiet)			    verbose=$((! $(bool "$optvalue")))		;;
	forceabort*)		    set -e					;;
	# Reporting
	artifactdir)		    artifactdir="$optvalue"			;;
	metrics|metricsfile)	    set_metrics_file "$optvalue"		;;
	metricsepoch)		    metrics_epoch=$optvalue			;;
	reportformat)		    report_format=$optvalue			;;
	jsonreport)		    report_format=json				;;
	rawreport)		    report_format=raw				;;
	report)		    	    report_format=${optvalue:-summary}		;;
	verbosereport)		    report_format=verbose			;;
	reportobjectcreation)	    report_object_creation=$(bool "$optvalue")	;;
	prometheussnapshot)	    take_prometheus_snapshot=$(bool "$optvalue");;
	predelay)		    predelay=$optvalue				;;
	postdelay)		    postdelay=$optvalue				;;
	timeout)		    timeout=$optvalue				;;
	failurestatus)		    failure_status=$optvalue			;;
	parallellog*)		    parallel_log_retrieval=$optvalue		;;
	retrievesuc*)		    retrieve_successful_logs=$(bool "$optvalue");;
	logsuc*)		    retrieve_successful_logs=$(bool "$optvalue");;
	# Basic options
	jobname)		    job_name=$optvalue				;;
	workload)		    requested_workload=$optvalue		;;
	basename)		    basename=$optvalue				;;
	arch)			    arch=$optvalue				;;
	# Object definition
	workdir)		    common_workdir=$optvalue			;;
	configmapfile)		    configmap_files+=("$optvalue")		;;
	containerimage)		    container_image=$optvalue			;;
	containers)		    containers_per_pod=$optvalue		;;
	containersperpod)	    containers_per_pod=$optvalue		;;
	deploymenttype)		    deployment_type=$optvalue			;;
	deployments|depspername*)   deps_per_namespace=$optvalue		;;
	emptydir)		    emptydirs+=("$optvalue")			;;
	volumes)		    emptydir_volumes=$optvalue			;;
	exitatend)		    exit_at_end=$(bool "$optvalue")		;;
	imagepullpolicy)	    image_pull_policy=$optvalue			;;
	namespaces)		    namespaces=$optvalue			;;
	nodeselector)		    node_selector=$optvalue			;;
	volume)			    parse_volume_spec "$optvalue"		;;
	processes|processesperpod)  processes_per_pod=$optvalue			;;
	jobfile)		    process_job_file "$optvalue"		;;
	pinnode)		    process_pin_node "$optvalue"		;;
	replicas)		    replicas=$optvalue				;;
	limit|limits)		    resource_limits+=("$optvalue")		;;
	request|requests)	    resource_requests+=("$optvalue")		;;
	kata)			    process_runtimeclass "kata"			;;
	podannotation)		    pod_annotations+=("$optvalue")		;;
	runtimeclass)		    process_runtimeclass "$optvalue"		;;
	uuid)			    uuid=$optvalue				;;
	secrets)		    secrets=$optvalue				;;
	workloadruntime)	    set_runtime "$optvalue"			;;
	workload_size)		    set_workload_bytes "$optvalue"		;;
	targetdatarate)		    target_data_rate=$(parse_size "$optvalue")	;;
	tolerate|toleration)	    tolerations+=("$optvalue")			;;
	dropcache)                  drop_node_cache=$(bool "$optvalue")         ;;
	dropallcache)		    drop_all_node_cache=$(bool "$optvalue")     ;;
	headlessservices)	    headless_services=$(bool "$optvalue")	;;
	virtiofsdwriteback)	    virtiofsd_writeback=$(bool "$optvalue")	;;
	virtiofsddirect)	    virtiofsd_direct=$(bool "$optvalue")	;;
	virtiofsdthread*)	    virtiofsd_threadpoolsize=$optvalue		;;
	livenessprobeint*)	    liveness_probe_frequency=$optvalue		;;
	livenessprobesleep*)	    liveness_probe_sleep_time=$optvalue		;;
	privilege*)		    create_pods_privileged=$(bool "$optvalue")	;;
	syncinfirst*)		    sync_in_first_namespace=$(bool "$optvalue") ;;
	affinity)
	    case "$optvalue" in
		1|'') affinity=1      ;;
		2|anti) affinity=2    ;;
		*) affinity=0         ;;
	    esac
	    ;;
	antiaffinity)
	    case "$optvalue" in
		1|'') affinity=2      ;;
		*) affinity=0         ;;
	    esac
	    ;;
	syncaffinity)
	    case "$optvalue" in
		1|'') syncaffinity=1  ;;
		2|anti) syncaffinity=2;;
		*) syncaffinity=0     ;;
	    esac
	    ;;
	syncantiaffinity)
	    case "$optvalue" in
		1|'') syncaffinity=2  ;;
		*) syncaffinity=0     ;;
	    esac
	    ;;
        # vm specs
        vmcores)                    vm_cores=$optvalue                          ;;
        vmsockets)                  vm_sockets=$optvalue                        ;;
        vmthreads)                  vm_threads=$optvalue                        ;;
        vmgraceperiod)              vm_grace_period=$optvalue                   ;;
        vmimage)                    vm_image=$optvalue                          ;;
	# Object creation
	obj*spercall)		    objs_per_call=$optvalue			;;
	parallel)		    parallel=$optvalue				;;
	sleep)			    sleeptime=$optvalue				;;
	podprefix)		    pod_prefix=${optvalue:+$optvalue-}		;;
	firstdeployment)	    first_deployment=$optvalue			;;
	parallelconfigmaps)	    parallel_configmaps=$optvalue		;;
	parallelsecrets)	    parallel_secrets=$optvalue			;;
	parallelnamespaces)	    parallel_namespaces=$optvalue		;;
	paralleldeployments)	    parallel_deployments=$optvalue		;;
	obj*spercallconfigmaps)	    objs_per_call_configmaps=$optvalue		;;
	obj*spercallsecrets)	    objs_per_call_secrets=$optvalue		;;
	obj*spercallnamespaces)	    objs_per_call_namespaces=$optvalue		;;
	obj*spercalldeployments)    objs_per_call_deployments=$optvalue		;;
	sleepbetweenconfigmaps)	    sleep_between_configmaps=$optvalue		;;
	sleepbetweensecrets)	    sleep_between_secrets=$optvalue		;;
	sleepbetweennamespaces)	    sleep_between_namespaces=$optvalue		;;
	sleepbetweendeployments)    sleep_between_deployments=$optvalue		;;
	waitsecrets)		    wait_for_secrets=$(bool "$optvalue")	;;
	scalens)	   	    scale_ns=$(bool "$optvalue")		;;
	scaledeployments)   	    scale_deployments=$(bool "$optvalue")	;;
	precleanup)		    precleanup=$(bool "$optvalue")		;;
	cleanup)		    cleanup=$(bool "$optvalue")			;;
	cleanupalways)		    cleanup_always=$(bool "$optvalue")		;;
	removenamespace*)	    remove_namespaces=$(bool "$optvalue")	;;
	baseoffset)		    baseoffset=$optvalue			;;
	# Synchronization
	sync|syncstart)		    sync_start=$((1-sync_start))		;;
	waitforever)                wait_forever=$(bool "$optvalue")		;;
	forcenometrics)		    metrics_support=$(($(bool "$optvalue")-1))  ;;
	podstart[ti]*)		    pod_start_timeout=$optvalue			;;
	externalsync)
	    if [[ $optvalue =~ ^(-|([[:alnum:]][-_[:alnum:]]*[[:alnum:]]\.)*([[:alnum:]][-_[:alnum:]]*[[:alnum:]])):([1-9][[:digit:]]{0,4})$ ]] ; then
		sync_host=${BASH_REMATCH[1]}
		sync_port=${BASH_REMATCH[4]}
		if (( sync_port > 65535 )) ; then
		    echo "Illegal external sync port (must be 1 <= port <= 65535)"
		    help
		fi
		sync_start=1
	    else
		echo "Undecipherable external sync host:port $optvalue"
		help
	    fi
	    ;;
	# Testing
	injecterror)		    inject_error "$optvalue"			;;
	debug*)			    register_debug_condition "$optvalue"	;;
	# Force delete everything after oc delete times out.
	# This is dangerous and hence not documented.
	# See https://access.redhat.com/solutions/4165791
	forcecleanupiknowthisisdangerous)
	    force_cleanup_timeout=${optvalue:-600}
	    ;;
	# Unknown options
	*)
	    unknown_opts+=("$1")
	    unknown_opt_names+=("$noptname ($noptname1)") ;;
    esac
}

function process_job_file() {
    local jobfile="$1"
    if [[ ! -f $jobfile || ! -r $jobfile ]] ; then
	fatal "Job file $jobfile cannot be read"
    fi
    while IFS= read -r line ; do
	line=${line%%#*}
	line=${line## }
	line=${line##	}
	if [[ -z "$line" ]] ; then continue; fi
	process_option "$line"
    done < "$jobfile"
}

function validate_resource() {
    local rtype=$1
    local token
    local status=0
    shift
    for token in "$@" ; do
	if [[ $token != *'='* ]] ; then
	    warn "Invalid $rtype specification $token (must be <resource>=<quantity>)"
	    status=1
	fi
    done
    return $status
}

################################################################
# Workload API management
################################################################

function print_workloads_supporting_reporting() {
    local prefix="${1:-}"
    local workloads
    while read -r workload ; do
	echo "$prefix$workload"
    done <<< "$(workloads_supporting_api supports_reporting)"
}

function list_report_formats() {
    local prefix=${1:-}
    while read -r format ; do
	echo "$prefix$format"
    done <<< "$("${pathdir:-.}/clusterbuster-report" --list_formats)"
}

function json_encode_settings() {
    call_api -w "$requested_workload" json_encode_settings
}

function create_deployment() {
    call_api -w "$requested_workload" create_deployment "$@"
}

function calculate_logs_required() {
    if supports_api -w "$requested_workload" supports_reporting ; then
	call_api -w "$requested_workload" calculate_logs_required "$@"
    else
	echo 1
    fi
}

function list_configmaps() {
    cat <<EOF
${__podfile_dir__}/cb_util.py
${__podfile_dir__}/clusterbuster_pod_client.py
${__podfile_dir__}/sync.py
${__podfile_dir__}/drop_cache.py
EOF
    call_api -w "$requested_workload" -s list_configmaps
}

function list_user_configmaps() {
    call_api -w "$requested_workload" -s list_user_configmaps
}

function generate_workload_metadata() {
    call_api -w "$requested_workload" -s generate_metadata
}

function generate_environment() {
    call_api -w "$requested_workload" -s generate_environment
}

function requires_drop_cache() {
    call_api -w "$requested_workload" requires_drop_cache
}

function namespace_policy() {
    local policy
    if [[ $(type -t "deployment_type_${deployment_type,,}_policy") = function ]] ; then
        policy=$("deployment_type_${deployment_type,,}_policy")
    elif ((create_pods_privileged)) || requires_drop_cache ; then
	policy=privileged
    else
        policy=$(call_api -w "$requested_workload" -s namespace_policy)
    fi
    echo "${policy:-${default_namespace_policy}}"
}

function deployment_type_vm_policy() {
    echo privileged
}
################################################################
# Helpers
################################################################

function __OC() {
    if ((doit)) ; then
	debug kubectl "$OC" "$@"
    fi
    if ((! doit)) ; then
	echo "$OC" "${@@Q}" 1>&2
    elif [[ $1 = exec || $1 = rsh ]] ; then
	# Capture error output
	(set -o pipefail; "$OC" "$@" 2>&1 1>&3 |sed -e "s/^/${*//\//\\/}: /" |timestamp 1>&2) 3>&1
    elif [[ $1 = describe || $1 = get || $1 = status || $1 = logs ]] ; then
	"$OC" "$@"
    elif ((report_object_creation)) ; then
	"$OC" "$@" 2>&1
    else
	"$OC" "$@" 2>&1 |grep -v -E '(^No resources found|deleted|created|labeled|condition met)$'
    fi
    return "${PIPESTATUS[0]}"
}

function _OC() {
    debug kubectl "$OC" "$@"
    if ! __OC "$@" ; then
	fatal "__KUBEFAIL__ $OC $* failed!"
    fi
}

function ___OC() {
    debug kubectl "$OC" "$@"
    if ! __OC "$@" ; then
	fatal "$OC $* failed!"
    fi
}

function ____OC() {
    debug kubectl "$OC" "$@"
    "$OC" "$@"
}

function namespace_exists() {
    local ns=$1
    ____OC get ns "$ns" >/dev/null 2>&1
}

function object_exists() {
    ____OC get "$@" >/dev/null 2>&1
}

function delete_object_safe() {
    object_exists "$@" && _OC delete "$@"
}

################################################################
# Synchronization between worker pods
################################################################

function get_sync() {
    if (( sync_start )) ; then
	if [[ ${1:-} != -q ]] ; then
	    echo "${global_sync_service}:$sync_port:$sync_ns_port"
	fi
	return 0
    else
	return 1
    fi
}

function create_sync_service() {
    local namespace=$1
    local sync_clients=${2:-0}
    local initial_sync_clients=${3:-$sync_clients}
    if get_sync -q ; then
	if [[ $namespace = "${namespaces_to_create[0]:-}" ]] ; then
	    create_service -h "$sync_namespace" "${sync_namespace}-sync" "$sync_port" "$sync_ns_port"
	    create_sync_deployment "$sync_namespace" "$((sync_clients * ${#namespaces_to_create[@]}))" "$((initial_sync_clients * ${#namespaces_to_create[@]}))"
	fi
	create_external_service "$namespace" "${namespace}-sync" "$global_sync_service" "$sync_port" "$sync_ns_port"
    fi
}

################################################################
# Logging
################################################################

function supports_metrics() {
    if ((metrics_support < 0)) ; then
	metrics_support=1
	____OC get pod -n openshift-monitoring prometheus-k8s-0 >/dev/null 2>&1 || metrics_support=0
    fi
    ((metrics_support > 0))
}

function get_pod_timestamp() {
    local format=${1:-%s.%N}
    shift
    ___OC exec "$@" -- /bin/sh -c "date '+$format'" || killthemall "Unable to retrieve timestamp from pod $*"
}

function get_prometheus_pod_timestamp() {
    local format=${1:-%s.%N}
    if supports_metrics ; then
	get_pod_timestamp "$format" -n openshift-monitoring prometheus-k8s-0 -c prometheus
    else
	date "+$format"
	return 1
    fi
}

function readable_timestamp() {
    local rawtime=${1:-}
    date -u '+%Y_%m_%dT%H_%M_%S%z' ${rawtime:+"--date=@$rawtime"}
}

function set_start_timestamps() {
    local -i retries=5
    if supports_metrics ; then
	while ((retries-- > 0)) ; do
	    first_start_timestamp=$(date +%s.%N)
	    if prometheus_exact_starting_timestamp=$(get_prometheus_pod_timestamp '%s.%N') ; then
		prometheus_starting_timestamp=${prometheus_exact_starting_timestamp%%.*}
		second_start_timestamp=$(date +%s.%N)
		return 0
	    fi
	    if ((retries > 0)) ; then
		warn "Fetch timestamp failed! $retries attempt(s) left"
		sleep 5
	    else
		warn "Unable to retrieve Prometheus timestamp (hard error)"
	    fi
	done
	return 1
    else
	first_start_timestamp=$(date +%s.%N)
	prometheus_start_timestamp=$first_start_timestamp
	prometheus_exact_starting_timestamp=$first_start_timestamp
	second_start_timestamp=$first_start_timestamp
    fi
}

function get_pod_and_local_timestamps() {
    local -i sync_prestart
    sync_prestart=$(date +%s)
    until __OC exec "$@" -- /bin/sh -c "date +%s.%N" </dev/null >/dev/null 2>/dev/null ; do
	local status
	status=$(__OC get pod "$@" -o jsonpath="{.status.phase}" 2>/dev/null)
	case "$status" in
	    Error|Failed)
		echo "Sync pod failed:" 1>&2
		__OC logs "$@" | tail -10 1>&2
		killthemall "Sync pod failed"
		;;
	    *)  ;;
	esac
	if (($(date +%s) - sync_prestart > pod_start_timeout)) ; then
	    echo "Sync pod did not start!" 1>&2
	    if [[ -n "$artifactdir" ]] ; then
		__OC describe pod "$@" > "${artifactdir}/sync.desc"
	    fi
	    killthemall "Sync pod did not start in $pod_start_timeout seconds"
	fi
	sleep 2
    done
    local first_local_ts
    local remote_ts
    local second_local_ts
    first_local_ts=$(date +%s.%N)
    remote_ts=$(__OC exec "$@" -- /bin/sh -c "date +%s.%N" </dev/null) || killthemall "Unable to retrieve sync pod timestamp"
    second_local_ts=$(date +%s.%N)
    _OC exec --stdin=true "$@" -- /bin/sh -c "cat > '${controller_timestamp_file}.tmp' && mv '${controller_timestamp_file}.tmp' '${controller_timestamp_file}'" <<EOF
{
  "first_controller_ts": $first_local_ts,
  "sync_ts": $remote_ts,
  "second_controller_ts": $second_local_ts
}
EOF
    # shellcheck disable=SC2181
    if (( $? != 0 )) ; then
	killthemall "Unable to write timing data to sync pod"
    fi
    echo "$first_local_ts" "$remote_ts" "$second_local_ts"
}

function start_prometheus_snapshot() {
    if ((!doit)) || ! supports_metrics ; then return 0; fi
    echo "Starting Prometheus snapshot" | echo_if_desired 1>&2
    _OC delete pod -n openshift-monitoring prometheus-k8s-0
    local -i retry=12
    until __OC get pod -n openshift-monitoring prometheus-k8s-0 >/dev/null 2>&1 ; do
	echo "Promtheus pod did not start, $retry attempt(s) left" | echo_if_desired 1>&2
	if ((retry <= 0)) ; then
	    killthemall "Prometheus pod did not restart!"
	fi
	retry=$((retry-1))
	sleep 5
    done
    __OC wait --for=condition=Ready -n openshift-monitoring pod/prometheus-k8s-0 || killthemall "Prometheus pod did not become ready"
    set_start_timestamps || return 1
    # Wait for prometheus pod to fully initialize
    sleep 60
    echo "Prometheus snapshot started" | echo_if_desired 1>&2
}

function retrieve_prometheus_snapshot() {
    if ((!doit)) || ! supports_metrics ; then return 0; fi
    echo "Retrieving Prometheus snapshot" | echo_if_desired 1>&2
    local dir=${1:-$artifactdir}
    sleep 60
    prometheus_ending_timestamp=$(get_prometheus_pod_timestamp)
    local promdb_name
    promdb_name="promdb_$(readable_timestamp "$prometheus_starting_timestamp")_$(readable_timestamp "$prometheus_ending_timestamp")"
    local promdb_path="${dir:+${dir}/}${promdb_name}.tar"
    if __OC exec -n openshift-monitoring prometheus-k8s-0 -c prometheus -- /bin/sh -c "tar cf - . -C /prometheus --transform 's,^[.],./${promdb_name},' .; true" > "$promdb_path" ; then
	echo "Prometheus snapshot retrieved" | echo_if_desired 1>&2
    else
	echo "Unable to retrieve Prometheus snapshot"
    fi
}

function ts() {
    local dt
    dt=$(date '+%s.%N')
    local sec=${dt%.*}
    local ns=${dt#*.}
    echo "${sec}.${ns:0:6}"
}

function run_status_monitor() {
    function run_status_monitor_1() {
	____OC get pod "$@" -w -o jsonpath='{.metadata.namespace} {.metadata.name} {.status.phase}{"\n"}'
    }
    (while : ; do sleep 1; printf '++Mark++ ++Mark+ %(%s)T%s' -1 $'\n'; done &)
    if [[ -n "$artifactdir" ]] ; then
	mkdir -p "$artifactdir"
	run_status_monitor_1 "$@" | tee >(grep -v '\+\+Mark\+\+' | timestamp > "${artifactdir}/monitor.log")
    else
	run_status_monitor_1 "$@"
    fi
}

function _monitor_pods() {
    if ((timeout > 0)) ; then
	timeout=$(($(date +%s) + timeout))
    fi
    local pods_pending=''
    local -i pod_progress_timeout=0
    if [[ -n "${injected_errors[timeout]:-}" ]] ; then
	warn "*** Injecting forced timeout error"
    fi
    # shellcheck disable=2034
    local -i running_pod_count=0
    local -i finished_pod_count=0
    local -i other_pod_count=0
    local -A pod_status=()
    local -A running_pods=()
    local -A starting_pods=()
    local pod
    local name
    local namespace
    local status
    local lstatus
    if [[ -n "${injected_errors[pending]:-}" ]] ; then
	warn "*** Injecting pending error"
	starting_pods["TEST/TEST"]=1
    fi
    local -i message_printed=0
    while read -r namespace name status ; do
	if [[ -n "${cb_tempdir:-}" && -f "$cb_tempdir/___run_complete" ]] ; then
	    if [[ -z "${injected_errors[timeout]:-}" ]] ; then
		return 0
	    else
		sleep infinity
	    fi
	fi
	if [[ $namespace != ++Mark++ ]] ; then
	    # This definitely shouldn't happen, but being defensive doesn't hurt.
	    if [[ -z "$status" || -z "$namespace" || -z "$name" ]] ; then continue; fi
	    pod="${namespace,,}/${name,,}"
	    lstatus="${status,,}"
	    # No guarantee that we won't get a repeated status
	    if [[ $lstatus = "${pod_status[$name]:-}" ]] ; then continue; fi
	    debug monitor "$pod" "${pod_status[$pod]:-}" '=>' "$lstatus"
	    pod_status["$pod"]=$lstatus
	    unset "starting_pods[$pod]"
	    unset "running_pods[$pod]"
	    case "$lstatus" in
		error|failed)
		    echo "Pod -n $namespace $name $status!" 1>&2
		    echo "Tail end of logs:" 1>&2
		    # TODO: Need to get logs from each container
		    ____OC logs -n "$namespace" "$name" |tail -20 1>&2
		    killthemall "Pod -n $namespace $name failed"
		    return 1
		    ;;
		pending|containercreating)
		    starting_pods["$namespace/$name"]=1
		    ;;
		running)
		    other_pod_count=$((other_pod_count-1))
		    running_pods["$namespace/$name"]=1
		    ;;
		completed|terminat*|succeeded)
		    unset "pod_status[$pod]"
		    other_pod_count=$((other_pod_count-1))
		    finished_pod_count=$((finished_pod_count+1))
		    ;;
		*)
		    other_pod_count=$((other_pod_count+1))
		    ;;
	    esac
	else
	    # Timestamp
	    local timestamp=$status
	    if [[ -z "${injected_errors[timeout]:-}" ]] && ! supports_api -w "$requested_workload" supports_reporting ; then
		if (( ${#running_pods[@]} + finished_pod_count > 0 && other_pod_count == 0 )) ; then
		    sleep "$workload_run_time"
		    return
		fi
	    fi

	    if [[ -n "${starting_pods[*]}" ]] ; then
		pods_now_pending=$(IFS=$'\n'; echo "${!starting_pods[*]}" | sort)
		if [[ "$pods_now_pending" != "$pods_pending" ]] ; then
		    pod_progress_timeout=$((timestamp+pod_start_timeout))
		    pods_pending="$pods_now_pending"
		    if [[ -z "$pods_pending" ]] ; then
			echo "All pods are running                                 " | echo_if_desired 1>&2
		    else
			# If we've reported pods pending, and then more pods are running,
			# we want to overprint the old message so it doesn't look like things
			# are stuck.  However, to avoid log clutter, we don't do this unless
			# a pending message was previously printed.
			if ((message_printed)) ; then
			    echo -ne "                                                  \r" | echo_if_desired 1>&2
			    message_printed=0
			fi
		    fi
		else
		    if ((timestamp > pod_progress_timeout)) ; then
			killthemall "No progress with pods after $pod_start_timeout seconds, ${#starting_pods[@]} pods still pending."
		    fi
		    if ((pod_progress_timeout-timestamp <= 30)) ; then
			local p_seconds=seconds
			local p_pods=pods
			if ((pod_progress_timeout-timestamp == 1)) ; then p_seconds=second; fi
			if ((${#starting_pods[@]} == 1)) ; then p_pods=pod; fi
			echo -ne "${#starting_pods[@]} $p_pods pending (will retry $((pod_progress_timeout-timestamp)) $p_seconds)\r" | echo_if_desired 1>&2
			message_printed=1
		    fi
		fi
	    elif ((message_printed)) ; then
		echo -ne "                                                  \r" | echo_if_desired 1>&2
		message_printed=0
	    fi
	    if ((timeout > 0 && timestamp > timeout)) ; then
		killthemall "Monitor pods timeout!"
	    fi
	fi
    done
}

function monitor_pods() {
    exec 0</dev/null 1>/dev/null
    run_status_monitor "$@" | _monitor_pods "$timeout"
}

function run_logger() {
    local -a pids_to_kill=()
    local OPTIND=0
    while getopts "p:" arg "$@" ; do
	case "$arg" in
	    p) pids_to_kill+=("$OPTARG") ;;
	    *)                           ;;
	esac
    done
    shift $((OPTIND-1))
    "$@" </dev/null
    # shellcheck disable=SC2046
    if [[ -n "${pids_to_kill[*]}" ]] ; then
	exec 2>/dev/null
	kill -USR2 "${pids_to_kill[@]}"
	[[ -n "$(jobs -p)" ]] && kill $(jobs -p)
    fi
}

# Use a separate flag file for errors to simplify the logic.
# This way we can send correct output to stdout, but handle
# errors in a separate subprocess.
function _fail_helper()  {
    trap exit TERM
    local faildata
    while : ; do
	local podstatus
	podstatus=$(____OC get pod -ojsonpath='{.status.phase}' "$@" 2>/dev/null)
	case "$podstatus" in
	    Succeeded|Terminated)
		return
		;;
	    Running)
		# Need to produce periodic output to stderr to prevent oc exec connection from prematurely terminating
		faildata="$(____OC exec --stdin=false "$@" -- sh -c "while [[ ! -f '$sync_error_file' ]] ; do sleep 5; echo KEEPALIVE 1>&2; done; cat '$sync_error_file'" 2>/dev/null)"
		if [[ -n "$faildata" ]] ; then
		    ____OC exec --stdin=false "$@" -- sh -c "rm -f '$sync_error_file'"
		    echo "Run failed:" 1>&2
		    echo "${faildata:-}" 1>&2
		    killthemall "Run failed, exiting!"
		fi
		;;
	    *)
		sleep 1;
		;;
	esac
    done
}

function _log_helper() {
    local OPTIND=0
    local OPTARG
    local fail_helper=
    while getopts 'e:' opt "$@" ; do
	case "$opt" in
	    e) fail_helper=$OPTARG ;;
	    *)			   ;;
	esac
    done
    shift $((OPTIND-1))
    if [[ -n "${fail_helper:-}" ]] ; then
	"$fail_helper" "$@" &
    fi
    if [[ -n "${injected_errors[timeout]:-}" ]] ; then
	echo "*** Injecting forced timeout error"
	sleep infinity
    fi
    until [[ $(____OC get pod -ojsonpath='{.status.phase}' "$@" 2>/dev/null) != 'Running' ]] ; do
	# Need to produce periodic output to stderr to prevent oc exec connection from prematurely terminating
	____OC exec --stdin=false "$@" -- sh -c "while [[ ! -f '$sync_flag_file' ]] ; do sleep 5; echo KEEPALIVE 1>&2; done; cat '$sync_flag_file'; sleep 5; echo DONE 1>&2" 2>/dev/null && break
	sleep 5
    done
    # Tell the pod monitor to stop.
    if [[ -n "${cb_tempdir:-}" ]] ; then
	touch "$cb_tempdir/___run_complete"
    fi
    ____OC exec --stdin=false "$@" -- sh -c "rm -f '$sync_flag_file'" || killthemall "Can't terminate sync"
}

function _get_logs_poll() {
    local pod
    local status
    local counter=$#
    for pod in "$@" ; do
	pod_status_found=0
	while : ; do
	    # shellcheck disable=SC2086
	    status=$(__OC get pod $pod -o jsonpath="{.status.phase}" 2>/dev/null)
	    case "$status" in
		Running)
		    break
		    ;;
		Pending)
		    pod_status_found=1
		    sleep 1
		    ;;
		Error|Failed)
		    pod_status_found=1
		    echo "Status of pod $pod failed!" 1>&2
		    return 1
		    ;;
		''|Succeeded)	# Might be an old pod not yet deleted
		    if ((pod_status_found)) ; then
			echo "Unexpected status '$status' of pod $pod!" 1>&2
			return 1
		    else
			sleep 1
		    fi
		    ;;
		*)
		    pod_status_found=1
		    echo "Unexpected status '$status' of pod $pod!" 1>&2
		    return 1
		    ;;
	    esac
	done
	# shellcheck disable=SC2086
	local mypid=$BASHPID
	monitor_pods -A -l "${basename}-x-worker=$xuuid" &
	# shellcheck disable=SC2086
	if supports_api -w "$requested_workload" supports_reporting ; then
	    # Ensure that if there's a lot of output that we don't remove the sync file before it has all been flushed out
	    run_logger -p "$mypid" -- _log_helper -e _fail_helper -- $pod &
	fi
	trap 'kill $(jobs -p) 2>/dev/null 1>&2 && wait' USR2
	wait
	if ((--counter > 0)) ; then
	    echo ','
	fi
    done
}

function get_logs_poll() {
    local tfile
    tfile=$(mktemp ${cb_tempdir:+-p "$cb_tempdir"} -t '_clusterbusterlogs.XXXXXXXXXX') || fatal "Cannot create temporary file"
    trap 'if [[ -n "${tfile:-}" && -f "$tfile" ]] ; then rm -f "$tfile"; unset tfile; fi; echo "Status: Fail"; return 1' INT TERM HUP USR1
    _get_logs_poll "$*" > "$tfile"
    local status=$?
    if ((status == 0)) ; then
	echo -n '"Results":'; cat < "$tfile"
    else
	echo "Collecting error report" 1>&2
	echo -n '"Results": {}'
    fi
    if [[ -n "${tfile:-}" && -f "$tfile" ]] ; then
	rm -f "$tfile"
    fi
    unset tfile
    return $status
}

function __report_one_volume() {
    local volname=$1
	cat <<EOF
  {
     "name": "$volname",
     "mount_path": "${volume_mount_paths[$volname]}",
     "type": "${volume_types[$vname]}",
     "type_key": "${volume_types[$vname]}",
     "scoped_name": "${volume_scoped_names[$vname]}"
  }
EOF
}

function _report_volumes() {
    echo '"volumes": ['
    local -a vols=()
    for volname in "${volumes[@]}" ; do
	vols+=("$(__report_one_volume "$volname")")
    done
    (IFS=$',\n'; echo "${vols[*]}")
    echo '],'
}

function _report_emptydirs() {
    if ((${#emptydirs[@]})) ; then
	echo "emptydirVolumes: ["
	local -a ed=("${emptydirs[@]}")
	ed=("${ed[@]/#/}")
	ed=("${ed[@]/%/}")
	(IFS=$',\n'; echo "${ed[*]}")
	echo "],"
    fi
}

function quote_list() {
    local -a a=("$@")
    a=("${a[@]//\"/\\\"}")
    a=("${a[@]//\$'\n'/ }")
    a=("${a[@]/#/\"}")
    a=("${a[@]/%/\"}")
    (IFS=$',\n '; echo "${a[*]}")
}

function _report_pin_nodes() {
    local -a nodes
    local class
    for class in "${!pin_nodes[@]}" ; do
	nodes+=("\"$class\": \"${pin_nodes[$class]}\"")
    done
    (IFS=$',\n '; echo "${nodes[*]}")
}

function _report_runtime_classes() {
    local -a runtimeclasses
    local class
    for class in "${!runtime_classes[@]}" ; do
	runtimeclasses+=("\"$class\": \"${runtime_classes[$class]}\"")
    done
    (IFS=$',\n '; echo "${runtimeclasses[*]}")
}

function _extract_metrics() {
    if supports_metrics && [[ -r "$metrics_file" ]] ; then
	cat <<EOF
"metrics": $("${__topdir__}/prom-extract" --define "namespace_re=${basename}-.*" -m "$metrics_file" --metrics-only --start_time="$prometheus_starting_timestamp" --epoch="$metrics_epoch" --post-settling-time=0),
EOF
    fi
}

function _workload_reporting_class() {
    if supports_api -w "$requested_workload" workload_reporting_class ; then
	call_api -w "$requested_workload" workload_reporting_class
    else
	echo "$requested_workload"
    fi
}

function _report_liveness_probe() {
    cat <<EOF
"liveness_probe_frequency": $liveness_probe_frequency,
"liveness_probe_sleep_time": $liveness_probe_sleep_time,
EOF
}

function _get_csv_version() {
    local product
    local namespace=
    local field='.spec.version'
    local OPTARG
    local OPTIND=0
    local opt
    while getopts 'n:f:' opt "$@" ; do
	case "$opt" in
	    n) namespace=$OPTARG ;;
	    f) field=$OPTARG	 ;;
	    *)			 ;;
	esac
    done
    shift $((OPTIND-1))
    local product=$1
    namespace=${namespace:-openshift-${product}}
    if namespace_exists "$namespace" ; then
	local version
	version=$(____OC get csv -n "$namespace" -o jsonpath="{.items[0]${field}}" 2>/dev/null)
	if [[ -n "$version" ]] ; then
	    cat <<EOF
"${product}_version": "${version:-}",
EOF
	fi
    fi
}

function _report_metadata_and_objects() {
    function readable_timestamp_report() {
	local rawtime=${1:-}
	date -u '+%Y-%m-%dT%T%:z' ${rawtime:+"--date=@$rawtime"}
    }
    local status=$1
    local enddate=
    enddate=$(date +%s.%N)
    cat <<EOF
"Status": "$status",
"metadata": {
  "kind": "clusterbusterResults",
  "controller_first_start_timestamp": $first_start_timestamp,
  "prometheus_start_timestamp": $prometheus_exact_starting_timestamp,
  "controller_second_start_timestamp": $second_start_timestamp,
  "controller_end_timestamp": $enddate,
  "controller_elapsed_time": $(bc <<< "$enddate - ${second_start_timestamp:-0}"),
  "cluster_start_time": "$(readable_timestamp "${prometheus_starting_timestamp}")",
  "job_name": "$job_name",
  "uuid": "$uuid",
  "workload": "$requested_workload",
  "workload_reporting_class": "$(_workload_reporting_class)",
  "kubernetes_version": $(indent_hang 2 __OC version -ojson),
  "command_line": [$(quote_list "${saved_argv[@]}")],
  "expanded_command_line": [$(quote_list "${processed_options[@]}")],
  "runHost": "$(hostname -f)",
  "workload_metadata": {$(indent_hang 2 generate_workload_metadata)},
  "controller_presync_timestamp": $first_local_ts,
  "sync_timestamp": $remote_ts,
  "controller_postsync_timestamp": $second_local_ts,
  "artifact_directory": "$artifactdir",
$(indent 2 _get_csv_version -n openshift-sandboxed-containers-operator kata)
$(indent 2 _get_csv_version cnv)
$(indent 2 _get_csv_version -f .spec.labels.full_version -n openshift-storage odf)
  "options": {
    "basename": "$basename",
    "containers_per_pod": $containers_per_pod,
    "deployments_per_namespace": $deps_per_namespace,
    "namespaces": $namespaces,
    "bytes_transfer": $bytes_transfer,
    "bytes_transfer_max": $bytes_transfer_max,
    "workload_run_time": $workload_run_time,
    "workload_run_time_max": $workload_run_time_max,
    "headless_services": $headless_services,
    "drop_cache": $drop_node_cache,
    "always_drop_cache": $drop_all_node_cache,
    "pin_nodes": {$(_report_pin_nodes)},
$(indent 4 _report_volumes)
$(indent 4 _report_liveness_probe)
    "secrets": $secrets,
    "replicas": $replicas,
    "container_image": "$container_image",
    "node_selector": "$node_selector",
$(indent 4 container_resources_json)
    "runtime_classes": {$(_report_runtime_classes)},
$(indent 4 _report_emptydirs)
    "target_data_rate": $target_data_rate,
    "workloadOptions": {
$(indent 8 call_api -w "$requested_workload" "report_options")
    }
  }
},
$(indent 2 _extract_metrics)
"nodes": $(__OC get nodes -ojson),
"api_objects": $(__OC get all -A -l "${basename}-id=$uuid" -ojson |jq -r .items?),
"csvs": $(__OC get csv -A -ojson | jq -r '[foreach .items[]? as $item ([[],[]];0; {name: $item.metadata.name, namespace: $item.metadata.namespace, version: $item.spec.version})]')
EOF
}

function _report_failure() {
    cat <<EOF
{
  "Results": {},
$(indent 2 _report_metadata_and_objects "$failure_status")
}
EOF
}

function _get_logs() {
    local first_local_ts
    local remote_ts
    local second_local_ts
    local status=$failure_status
    local do_retrieve_artifacts=1
    read -r first_local_ts remote_ts second_local_ts <<< "$(get_pod_and_local_timestamps "${@:2}")"
    if [[ -z "$remote_ts" ]] ; then
	killthemall "Unable to retrieve sync timestamp"
    fi
    sync_ts_error_interval=$(bc <<< "$second_local_ts - $first_local_ts")
    sync_ts_delta=$(bc <<< "$second_local_ts - $first_local_ts")
    local -i reported=0
    local tfile1=
    local tfile2=
    trap 'if [[ -n "$tfile2" && -f "$tfile2" ]] ; then rm -f "$tfile1" "$tfile2" ; fi; if ((! reported)) ; _report_failure; reported=1; fi; _retrieve_artifacts; return 1' TERM
    tfile1=$(mktemp ${cb_tempdir:+-p "$cb_tempdir"} -t '_clusterbusterlogs1.XXXXXXXXXX') || fatal "Cannot create temporary file"
    tfile2=$(mktemp ${cb_tempdir:+-p "$cb_tempdir"} -t '_clusterbusterlogs2.XXXXXXXXXX') || fatal "Cannot create temporary file"
    "$@" > "$tfile1" && {
	status=Success
	do_retrieve_artifacts=
    }
    cat > "$tfile2" <<EOF
{
$(indent 2 < "$tfile1"),
$(indent 2 _report_metadata_and_objects "$status")
}
EOF
    ((reported)) || cat "$tfile2"
    reported=1
    rm -f "$tfile1" "$tfile2"
    # shellcheck disable=SC2086
    _retrieve_artifacts $do_retrieve_artifacts
}

function _retrieve_artifacts() {
    local always_retrieve_artifacts=${1:-}
    if [[ -n "$artifactdir" ]] ; then
	mkdir -p "$artifactdir/Logs"
	mkdir -p "$artifactdir/Describe"
	local -i failcount=0
	local -A jobs_pending
	local job
	local -A pods_described=()
	while read -r namespace pod container status; do
	    if [[ ($namespace != "$sync_namespace" || $sync_in_first_namespace) && -z $always_retrieve_artifacts &&
		      $retrieve_successful_logs -eq 0 && ($status = Running || $status = Completed) ]] ; then
		continue
	    fi
	    local podname="${namespace}:${pod}"
	    local name="${podname}:${container}"
	    if [[ -n "${pods_described[$podname]:-}" ]] ; then
		__OC logs -n "$namespace" -c "$container" "$pod" 2>/dev/null > "$artifactdir/Logs/$name" &
	    else
		(__OC logs -n "$namespace" -c "$container" "$pod" 2>/dev/null > "$artifactdir/Logs/$name"; __OC describe pod -n "$namespace" "$pod" 2>/dev/null > "$artifactdir/Describe/$podname") &
		pods_described[$podname]=1
	    fi
	    jobs_pending[$!]=$name
	    if (( ${#jobs_pending[@]} > parallel_log_retrieval )) ; then
		for job in "${!jobs_pending[@]}" ; do
		    wait "$job" || {
			rm -f "$artifactdir/Logs/$name"
			failcount=$((failcount+1))
			if ((failcount <= 5)) ; then
			    echo "Unable to retrieve logs for pod $name" 1>&2
			fi
		    }
		    unset "jobs_pending[$job]"
		done
	    fi
	done <<< "$(__OC get pod -A -l "${basename}-id=$uuid" -ojson | jq -r '[foreach .items[]? as $item ([[],[]];0; (if ($item.kind == "Pod") then ([foreach $item.spec.containers[]? as $container ([[],[]];0; $item.metadata.namespace + " " + $item.metadata.name + " " + $container.name + " " + $item.status.phase)]) else null end))] | flatten | map (select (. != null))[]')"
	for job in "${!jobs_pending[@]}" ; do
	    wait "$job" || {
		rm -f "$artifactdir/Logs/$name"
		failcount=$((failcount+1))
		if ((failcount <= 5)) ; then
		    echo "Unable to retrieve logs for pod $name" 1>&2
		fi
	    }
	    unset "jobs_pending[$job]"
	done
	if ((failcount > 5)) ; then
	    echo "+ $((failcount - 5)) more" 1>&2
	fi
    fi
}

function print_report() {
    if [[ $report_format = raw ]] ; then
	cat
    else
	"${pathdir:-.}/clusterbuster-report" ${report_format:+-o "$report_format"}
    fi
}

function get_logs() {
    trap - TERM
    local extra_arg=
    local log_cmd
    if supports_api -w "$requested_workload" supports_reporting ; then
	log_cmd=get_logs_poll
    else
	log_cmd=true
    fi
    local report_cmd=cat
    if supports_api -w "$requested_workload" supports_reporting && [[ -n "$*" ]]; then
	if [[ -n "$artifactdir" ]] ; then
	    # Even if reporting fails, make sure we capture the JSON
	    _get_logs "$log_cmd" "$@" > "$artifactdir/clusterbuster-report.tmp.json" &&
		mv "$artifactdir/clusterbuster-report.tmp.json" "$artifactdir/clusterbuster-report.json" &&
		print_report < "$artifactdir/clusterbuster-report.json" &&
		return 0
	    return 1
	else
	    _get_logs "$log_cmd" "$@" | print_report
	    return $((PIPESTATUS[0] || PIPESTATUS[1]))
	fi
    else
	echo "Workload $requested_workload does not support reporting" 1>&2
    fi
}

################################################################
# YAML fragment generation
################################################################

function privileged_security_context_content() {
    cat <<'EOF'
privileged: true
runAsUser: 0
EOF
}

function restricted_security_context_content() {
    cat <<'EOF'
allowPrivilegeEscalation: False
capabilities:
  drop:
  - ALL
runAsNonRoot: True
seccompProfile:
  type: RuntimeDefault
EOF
}

function default_security_context_content() {
    if ((create_pods_privileged)) ; then
	privileged_security_context_content
    else
	restricted_security_context_content
    fi
}

function default_security_context() {
    cat <<EOF
securityContext:
$(indent 2 default_security_context_content)
EOF
}

function privileged_security_context() {
    cat <<EOF
securityContext:
$(indent 2 privileged_security_context_content)
EOF
}

function restricted_security_context() {
    cat <<EOF
securityContext:
$(indent 2 restricted_security_context_content)
EOF
}

function indent() {
    local -i column="$1"
    local -a cmd
    shift
    local str
    str=$(printf "%${column}s" '')
    if [[ -n "${hang_indent:-}" ]] ; then
	# shellcheck disable=SC2016
	cmd=(awk '/./ {if (a == 0) {print} else {print "'"$str"'", $0}; a = 1}')
    else
	cmd=(sed -e '/^$/d' -e "s/^/${str}/")
    fi
    if [[ -n "$*" ]] ; then
	"$@" | "${cmd[@]}"
    else
	"${cmd[@]}"
    fi
}

function indent_hang() {
    hang_indent=1 indent "$@"
}

function mkpodname() {
    local podname
    podname="${pod_prefix}$(IFS=-; echo "$*")"
    echo "${podname//_/-}"
}

function get_pin_node() {
    if [[ -n "${1:-}" ]] ; then
	if [[ -n "${pin_nodes[$1]:-}" ]] ; then
	    echo "${pin_nodes[$1]}"
	elif [[ -n "${pin_nodes[default]:-}" ]] ; then
	    echo "${pin_nodes[default]}"
	fi
    fi
}

function tolerations_yaml() {
    local toleration
    (( ${#tolerations[@]} )) || return
    echo "tolerations:"
    for toleration in "${tolerations[@]}" ; do
	# shellcheck disable=SC2206
	local -a tol=(${toleration//:/ })
	cat <<EOF
- key: "${tol[0]:-}"
  operator: "${tol[1]:-}"
  effect: "${tol[2]:-}"
EOF
    done
}

function container_resources_yaml() {
    function resources() {
	local token
	for token in "$@" ; do
	    local resource=${token%%=*}
	    local value=${token#*=}
	    echo "$resource: $value"
	done
    }
    if (( ${#resource_limits[@]} + ${#resource_requests[@]} )) ; then
	echo "resources:"
	if (( ${#resource_limits[@]} )) ; then
	    echo "  limits:"
	    indent 4 resources "${resource_limits[@]}"
	fi
	if (( ${#resource_requests[@]} )) ; then
	    echo "  requests:"
	    indent 4 resources "${resource_requests[@]}"
	fi
    fi
}

function liveness_probe_yaml() {
    if ((liveness_probe_frequency > 0)) ; then
	cat <<EOF
livenessProbe:
  exec:
    command:
    - /bin/sleep "$liveness_probe_sleep_time"
  initialDelaySeconds: 10
  periodSeconds: $liveness_probe_frequency
EOF
    fi
}

function mk_yaml_args() {
    function yamlize() {
	local arg=$1
	if [[ $arg =~ $'\n' ]] ; then
	    echo "- |"$'\n'"  ${arg//$'\n'/$'\n  '}"
	else
	    echo "- \"$arg\""
	fi
    }
    local OPTARG
    local OPTIND=0
    local arg
    while getopts 's' opt "$@" ; do
	case "$opt" in
	    s)
		while read -r arg ; do
		    yamlize "$arg"
		done
		;;
	    *)
		;;
	esac
    done
    shift $((OPTIND-1))
    for arg in "$@" ; do
	yamlize "$arg"
    done
}

function create_standard_containers() {
    local OPTARG
    local OPTIND=0
    local opt
    local image="$container_image"
    while getopts 'i:' opt "$@" ; do
	case "$opt" in
	    i) image="$OPTARG"  ;;
	    *)		        ;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    local instance=$2
    local secret_count=$3
    local replicas=$4
    local containers=$5
    local -i container
    local sync_service=
    local sync_port_num=
    local sync_ns_port_num=
    IFS=: read -r sync_service sync_port_num sync_ns_port_num <<< "$(get_sync)"
    for container in $(seq 0 $((containers - 1))) ; do
	cat <<EOF
- name: "c${container}"
  imagePullPolicy: $image_pull_policy
  image: "$container_image"
$(indent 2 container_standard_auxiliary_yaml)
$(indent 2 standard_environment)
$(indent 2 generate_environment)
  command:
$(if [[ -n "${arglist_function}" ]] ; then
     "${arglist_function}" "${system_configmap_mount_dir}/" "$@" "$container" -- "$namespace" "c$container" \
     "$basetime" "$baseoffset" "$(ts)" "$exit_at_end" "$sync_service" "$sync_port_num" "$sync_ns_port_num" | indent 2 ; fi)
$(indent 2 volume_mounts_yaml "$namespace" "${instance}" "$secret_count")
$(indent 2 <<< "${security_context:-}")
EOF
    done
}

function container_standard_auxiliary_yaml() {
    container_resources_yaml
    liveness_probe_yaml
}

function resources() {
    local token
    for token in "$@" ; do
	local resource=${token%%=*}
	local value=${token#*=}
	if [[ ! $value =~ ^-?[[:digit:]]+(\.[[:digit:]]+)?$ ]] ; then
	    value="\"$value\""
	fi
	echo "\"$resource\": $value"
    done
}

function resource_json() {
    local request_type=$1
    shift
    echo "    \"$request_type\": {"
    local -a resources=()
    local resource
    for resource in "$@" ; do
	resources+=("$(resources "$resource")")
    done
    (IFS=$',\n        '; echo "${resources[*]}")
    echo "    }"
}

function container_resources_json() {
    local -a res_data=()
    if (( ${#resource_limits[@]} + ${#resource_requests[@]} )) ; then
	echo '"resources": {'
	if (( ${#resource_limits[@]} )) ; then
	    res_data+=("$(resource_json limits "${resource_limits[@]}")")
	fi
	if (( ${#resource_requests[@]} )) ; then
	    res_data+=("$(resource_json requests "${resource_requests[@]}")")
	fi
	(IFS=$',\n    '; echo "${res_data[*]}")
	echo "},"
    fi
}

function standard_labels_yaml() {
    local objtype=worker
    local OPTIND=0
    local logger=
    local suffix=
    local -i worker_label=1
    while getopts 'St:ls:' opt "$@" ; do
	case "$opt" in
	    S) worker_label=0	 ;;
	    t) objtype="$OPTARG" ;;
	    l) logger=true	 ;;
	    s) suffix="$OPTARG"  ;;
	    *)			 ;;
	esac
    done
    shift $((OPTIND-1))
    local workload=${1:-}
    local namespace=${2:-}
    local instance=${3:-}
    local replica=${4:-}
    # Different versions of bash expand quotes inside the ${logger:+...} differently.
    # true and "true" are interpreted differently.
    local true='"true"'
    cat <<EOF
labels:
  ${basename}-xuuid: "$xuuid"
  ${basename}-uuid: "$uuid"
  ${basename}-id: "$uuid"
  ${basename}: "true"
  clusterbusterbase: "true"
  ${basename}base: "true"
${objtype:+  ${basename}-${objtype}: "$uuid"}
${objtype:+  ${basename}-x-${objtype}: "$xuuid"}
${logger:+  ${basename}-logger: $true}
EOF
    if ((worker_label)) ; then
	cat <<EOF
  ${basename}-worker: "true"
  clusterbuster-worker: "true"
EOF
    fi
    if [[ -n "$workload" ]] ; then
	cat <<EOF
  ${basename}-${workload}: "$uuid"
  name: "${namespace}${workload:+-$workload}${instance:+-$instance}${suffix:+-$suffix}"
  app: "${namespace}${workload:+-$workload}${suffix:+-$suffix}"
  k8s-app: "${namespace}${workload:+-$workload}${suffix:+-$suffix}"
  instance: "${namespace}${workload:+-$workload}${instance:+-$instance}${suffix:+-$suffix}"
  replica: "${namespace}${workload:+-$workload}${instance:+-$instance}${replica:+-$replica}${suffix:+-$suffix}"
  ${workload}: "true"
EOF
    fi
}

function standard_environment() {
    cat <<EOF
env:
- name: VERBOSE
  value: "$verbose"
- name: PODFILE_DIR
  value: "$system_configmap_mount_dir"
- name: PYTHONPATH
  value: "$system_configmap_mount_dir"
EOF
}

function volume_mounts_yaml() {
    local namespace=$1
    local deployment=${2:-1}
    local secrets=${3:-1}
    (( secrets + ${#emptydirs[@]} + ${#configmap_files[@]} + ${#volumes[@]} + has_system_configmap )) || return;
    local -i i
    echo volumeMounts:
    for i in $(seq 0 $((secrets - 1))) ; do
	local name="secret-${namespace}-${deployment}-$i"
	cat <<EOF
- name: $name
  mountPath: /etc/$name
  readOnly: true
EOF
    done
    if [[ -n "${emptydirs[*]:-}" ]] ; then
	local vol
	for vol in "${emptydirs[@]}" ; do
	    cat <<EOF
- name: ${vol##*/}
  mountPath: "$vol"
EOF
	done
    fi
    if [[ $namespace != "$sync_namespace" && -n "${configmap_files[*]:-}" ]] ; then
	cat <<EOF
- name: "configmap-${namespace}"
  mountPath: "${configmap_mount_dir}"
  readOnly: true
EOF
    fi
    if [[ -n "${volumes[*]:-}" ]] ; then
	local vol
	for vol in "${volumes[@]:-}" ; do
	    cat <<EOF
- name: "$vol"
  mountPath: "${volume_mount_paths[$vol]}"
EOF
	done
    fi
    if (( has_system_configmap )) ; then
	cat <<EOF
- name: "systemconfigmap-${namespace}"
  mountPath: "${system_configmap_mount_dir}"
  readOnly: true
EOF
    fi
}

function namespace_yaml() {
    local namespace=$1
    (( use_namespaces )) && echo "namespace: \"$namespace\""
}

function annotations_yaml() {
    local desired_class=${2:-client}
    local annotation
    echo "annotations:"
    for annotation in "${pod_annotations[@]}" ; do
	if [[ $annotation =~ ^:([^:]*):(.*)$ ]] ; then
	    # shellcheck disable=SC2206
	    local -a classes=(${BASH_REMATCH[1]//,/ })
	    annotation=${BASH_REMATCH[2]:-}
	    local class
	    local -i class_found=0
	    for class in "${classes[@]}" ; do
		if [[ $desired_class = "$class" ]] ; then
		    class_found=1
		    break
		fi
	    done
	    if (( ! class_found )) ; then
		continue
	    fi
	fi
	echo "  $annotation"
    done
}

function standard_deployment_metadata_yaml() {
    local namespace=${1:-}
    local class=${2:-client}
    namespace_yaml "$namespace"
}

function standard_pod_metadata_yaml() {
    local namespace=${1:-}
    local class=${2:-client}
    annotations_yaml "$class"
}

function expand_volume() {
    local scoped_name=$1
    local namespace=${2:+-$2}
    local instance=${3:+-$3}
    scoped_name=${scoped_name//%N/$namespace}
    scoped_name=${scoped_name//%i/$instance}
    echo "$scoped_name"
}

function volumes_yaml() {
    local namespace=$1
    local instance=${2:-1}
    local secrets=${3:-1}
    (( secrets + ${#emptydirs[@]} + ${#configmap_files[@]} + ${#volumes[@]} + has_system_configmap )) || return;
    local -i i
    echo "volumes:"
    for i in $(seq 0 $((secrets - 1))) ; do
	local name="secret-${namespace}-${instance}-$i"
	cat<<EOF
- name: $name
  secret:
    secretName: $name
EOF
    done

    if [[ -n "${emptydirs[*]:-}" ]] ; then
	local vol
	for vol in "${emptydirs[@]}" ; do
	    cat <<EOF
- name: ${vol##*/}
  emptydDir: {}
EOF
	done
    fi
    if [[ $namespace != "$sync_namespace" && -n "${configmap_files[*]:-}" ]] ; then
	cat <<EOF
- name: "configmap-${namespace}"
  configMap:
    name: "configmap-${namespace}"
EOF
    fi
    if [[ -n "${volumes[*]:-}" ]] ; then
	local vol
	for vol in "${volumes[@]:-}" ; do
	    cat <<EOF
- name: "$vol"
EOF
	    if [[ -n "${volume_type_keys[$vol]}" ]] ; then
		cat <<EOF
  ${volume_types[$vol]}:
    ${volume_type_keys[$vol]}: "$(expand_volume "${volume_scoped_names[$vol]}" "$namespace" "$instance")"
EOF
	    else
		cat <<EOF
  ${volume_types[$vol]}: {}
EOF
	    fi
	done
    fi
    if (( has_system_configmap )) ; then
	cat <<EOF
- name: "systemconfigmap-${namespace}"
  configMap:
    name: "systemconfigmap-${namespace}"
EOF
    fi
}

function create_affinity_yaml() {
    local OPTIND=0
    local OPTARG
    local _affinity="$affinity"
    local topo_key="kubernetes.io/hostname"
    local key="instance"
    local operator="In"
    local opt
    while getopts "a:k:o:t:" opt "$@" ; do
	case "$opt" in
	    a) _affinity="$OPTARG" ;;
	    k) key="$OPTARG"       ;;
	    o) operator="$OPTARG"  ;;
	    t) topo_key="$OPTARG"  ;;
	    *)                     ;;
	esac
    done
    case "${_affinity,,}" in
	0)	     return		    ;;
	1|af*|podaf) _affinity=Affinity	    ;;
	*)	     _affinity=AntiAffinity ;;
    esac
    shift $((OPTIND-1))
    if [[ -n "${1:-}" ]] ; then
	cat <<EOF
affinity:
  pod$_affinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: $key
          operator: $operator
          values:
          - "$1"
      topologyKey: $topo_key
EOF
    fi
}

################################################################
# Object YAML creation
################################################################

function _really_create_objects() {
    if [[ $doit -ne 0 && -n $accumulateddata ]] ; then
	__OC --validate=false apply -f - <<< "$accumulateddata" || {
	    echo "Create objects failed:" 1>&2
	    echo "$accumulateddata" 1>&2
	    killthemall "Failed to create objects"
	}
    fi
}

function really_create_objects() {
    _really_create_objects | timestamp 1>&2
    accumulateddata=
    objs_item_count=0
    return "${PIPESTATUS[0]}"
}

function create_object() {
    local OPTIND=0
    local namespace=
    local objtype=${deployment_type^}
    local objname=
    local force=0
    while getopts 'n:t:f' opt "$@" ; do
	case "$opt" in
	    n) namespace=$OPTARG ;;
	    t) objtype=$OPTARG   ;;
	    f) force=1           ;;
	    *)                   ;;
	esac
    done
    shift $((OPTIND-1))
    objname=${1:-UNKNOWN}
    local data=
    local line=
    local -i data_found=0
    while IFS='' read -r 'line' ; do
	line=${line%% }
	if [[ $line =~ [^[:space:]] ]] ; then
	    data_found=1
	    [[ -n $data ]] || data="---$nl"
	    data+="$line$nl"
	fi
    done
    ((! data_found)) && return
    data+="$nl"
    if (( doit )) ; then
	if [[ -n "$artifactdir" ]] ; then
	    mkdir -p "$artifactdir/$objtype"
	    echo "$data" > "$artifactdir/${objtype}/${namespace}:$objname"
	fi
	accumulateddata+="$data"
	if (( force || (++objs_item_count >= objs_per_call) )) ; then
	    really_create_objects
	    (( !sleeptime )) || sleep "$sleeptime"
	fi
    else
	IFS= echo "$data"
    fi
}

function echo_if_desired() {
    if (( report_object_creation )) ; then
	cat
    else
	cat >/dev/null
    fi
}

function _create_object_from_file() {
    function _ns() {
	((use_namespaces)) && echo -n "-n $namespace"
    }
    local objtype=$1; shift
    local namespace=$1; shift
    local objname=$1; shift
    local ns=${use_namespaces:+-n "$namespace"}
    local ns1=${use_namespaces:+"$namespace"}
    local file
    for file in "$@" ; do
	[[ -r "$file" ]] || fatal "Can't read file $file!"
    done
    # shellcheck disable=SC2046
    delete_object_safe "$objtype" $(_ns) "$objname"
    # shellcheck disable=SC2046
    _OC create "$objtype" $(_ns) "$objname" "${@/#/--from-file=}" || killthemall "Unable to create object $objtype/$namespace:$objname"
    # shellcheck disable=SC2046
    _OC label "$objtype" $(_ns) "$objname" "${basename}base=true" 'clusterbusterbase=true' "${basename}-uuid=${uuid}" "${basename}-id=$uuid" "${basename}=true" || killthemall "Unable to label $objtype/$namespace:$objname"
    if (( doit )) ; then
	if [[ -n "$artifactdir" ]] ; then
	    mkdir -p "$artifactdir/$objtype"
	    local outfile="$artifactdir/${objtype}/${ns1}:${objname}"
	    rm -f "$outfile"
	    for file in "$@" ; do
		echo '---' >> "$outfile"
		cat "$file" >> "$outfile"
	    done
	fi
	total_objects_created=$((total_objects_created+1))
	if [[ -z ${objects_created[$objtype]:-} ]] ; then
	    objects_created[$objtype]=1
	else
	    objects_created[$objtype]=$((${objects_created[$objtype]}+1))
	fi
	(( !sleeptime )) || sleep "$sleeptime"
    fi
}

function create_object_from_file() {
    _create_object_from_file "$@" |echo_if_desired |timestamp 1>&2
}

function create_namespace() {
    local OPTIND=0
    local force=
    local opt=
    local policy
    local worker_label=
    policy=$(namespace_policy)
    while getopts 'Sfp:' opt "$@" ; do
	case "$opt" in
	    S) worker_label=-S	;;
	    f) force=-f		;;
	    p) policy="$OPTARG" ;;
	    *)			;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    if ((use_namespaces)) ; then
	namespace_exists "$namespace" || create_object $force -t namespace "$namespace" <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: "${namespace}"
$(indent 2 standard_labels_yaml $worker_label -t namespace)
EOF
	_OC label namespace --overwrite "$namespace" "pod-security.kubernetes.io/enforce=$policy"
	_OC label namespace --overwrite "$namespace" "pod-security.kubernetes.io/audit=$policy"
	_OC label namespace --overwrite "$namespace" "pod-security.kubernetes.io/warn=$policy"
	if [[ $policy = privileged ]] ; then
	    if ! __OC get serviceaccount -n "$namespace" "$namespace" >/dev/null 2>&1 ; then
		_OC create serviceaccount -n "$namespace" "$namespace"
	    fi
	    _OC label serviceaccount -n "$namespace" "$namespace" "${basename}=true"
	    _OC adm policy add-scc-to-user -n "$namespace" privileged -z "$namespace"
	fi
    fi
}

function create_secrets() {
    local namespace=$1
    local deps_per_namespace=${2:-1}
    local secrets=${3:-1}
    local -i i
    local -i j
    (( secrets )) || return;
    for i in $(seq "$first_deployment" $((deps_per_namespace + first_deployment - 1))) ; do
	for j in $(seq 0 $((secrets - 1))) ; do
	    local secname="secret${namespace:+-$namespace}-${i}-${j}"
	    create_object -t secret -n "$namespace" "$secname" <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: "$secname"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml -t service)
data:
  key1: "$(base64 <<< "${namespace}X${i}Y${j}Z1")"
  key2: "$(base64 <<< "${namespace}X${i}Y${j}Z2")"
type: Opaque
EOF
	done
    done
}

function check_configmaps() {
    local errors=0
    for mapfile in "$@" ; do
	if [[ ! -r "$mapfile" ]] ; then
	    echo "Can't find configmap file $mapfile"
	    errors=$((errors+1))
	fi
    done
    if (( errors )) ; then
	exit 1
    fi
}

function create_configmaps() {
    local namespace=$1; shift
    local mapname=$1; shift
    (( $# )) || return;
    create_object_from_file configmap "$namespace" "${mapname}-${namespace}" "$@"
}

function _create_service_ports() {
    local basename=$1
    shift
    local i
    local p
    for i in "$@" ; do
	for p in TCP UDP ; do
	    cat <<EOF
- name: ${basename}-${i}-${p,,}
  protocol: $p
  port: $i
  targetPort: $i
EOF
	done
    done
}

function create_service() {
    local OPTIND=0
    local OPTARG
    local -i force_headless_services=-1
    local headless=
    local key=name
    local prefix='svc-'
    while getopts 'eEhHk:' opt ; do
	case "$opt" in
	    e) prefix=			 ;;
	    E) prefix='svc-'		 ;;
	    h) force_headless_services=1 ;;
	    H) force_headless_services=0 ;;
	    k) key="$OPTARG"		 ;;
	    *) ;;
	esac
    done
    shift $((OPTIND-1))
    ((force_headless_services < 0)) && force_headless_services=$headless_services
    ((force_headless_services)) && headless='clusterIP: "None"'
    local namespace=$1
    local deployment=$2
    shift 2
    create_object -t Service -n "$namespace" "svc-${deployment}" <<EOF
apiVersion: v1
kind: Service
metadata:
  name: "${prefix}${deployment}"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml -t service)
spec:
$(indent 2 <<< "$headless")
  ports:
$(indent 4 _create_service_ports "svc-${deployment}" "$@")
  selector:
    ${key}: "${deployment}"
EOF
}

function create_external_service() {
    local namespace=$1
    local deployment=$2
    local external_name=$3
    shift 3
    function list_ports() {
	local portnum
	for portnum in "$@" ; do
	    cat <<EOF
- port: $portnum
  name: "svc-${deployment}-${portnum}"
EOF
	done
    }
    create_object -t Service -n "$namespace" "svc-${deployment}" <<EOF
apiVersion: v1
kind: Service
metadata:
  name: "svc-${deployment}"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml -t service)
spec:
  type: ExternalName
  externalName: $external_name
  ports:
$(indent 2 list_ports "$@")
EOF
}

function create_spec() {
    local affinity_yaml=
    local node_class=client
    local OPTIND=0
    local OPTARG
    local ns_yaml=${node_selector:+"nodeSelector:${nl}  $node_selector: \"\""}
    local runtime_class
    local opt
    local pin_node
    while getopts "A:c:r:Pp" opt "$@" ; do
	case "$opt" in
	    A) affinity_yaml=$OPTARG ;;
	    c) node_class=$OPTARG    ;;
	    r) runtime_class=$OPTARG ;;
	    *)                       ;;
	esac
    done
    shift $((OPTIND-1))
    (( $# >= 3 )) || fatal "Usage: create_spec namespace instance secret_count args..."
    # Node specifiers override affinity
    pin_node=$(get_pin_node "$node_class")
    if [[ -n "${pin_node:-}" ]] ; then
	affinity_yaml="nodeSelector:${nl}  kubernetes.io/hostname: \"$pin_node\""
    fi
    if [[ -z "${runtime_class+x}" ]] ; then
	if [[ -n "$node_class" && -n "${runtime_classes[$node_class]:-}" ]] ; then
	    runtime_class=${runtime_classes[$node_class]}
	elif [[ -n "${runtime_classes[default]:-}" ]] ; then
	    runtime_class=${runtime_classes[default]}
	fi
    fi
    [[ -n "$(type -t "$create_container_function")" ]] || fatal "Cannot run $create_container_function: command not found"
    local namespace=$1
    local instance=$2
    local secret_count=$3
    local runtime_class=${runtime_class:-}
    if [[ $runtime_class = runc ]] ; then
	runtime_class=
    fi
    cat <<EOF
spec:
  terminationGracePeriodSeconds: 1
$(indent 2 <<< "$affinity_yaml")
$(indent 2 tolerations_yaml ${tolerations+"${tolerations[@]}"})
${runtime_class:+$(indent 2 <<< "runtimeClassName: \"$runtime_class\"")}
  containers:
$(indent 2 "$create_container_function" "$@")
$(indent 2 volumes_yaml "$namespace" "$instance" "$secret_count")
EOF
}

function create_pod_deployment() {
    local namespace=$1
    local instance=$2
    local -i replicas=$4
    local -i replica=0
    local depname="${namespace}-${workload}-${instance}"
    while (( replica++ < replicas )) ; do
	local name="${depname}-${replica}"
	create_object -n "$namespace" "$name" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: $(mkpodname "$name")
$(indent 2 standard_deployment_metadata_yaml "$namespace" client)
$(indent 2 standard_pod_metadata_yaml "$namespace" client)
  selector:
    matchLabels:
      app: $name
$(indent 2 standard_labels_yaml -l "$workload" "$namespace" "${instance}" "${replica}")
$(create_spec -A "$affinity_yaml" -c "$node_class" "$@" "$replica")
  restartPolicy: Never
EOF
    done
}

function create_vm_deployment() {
    local namespace=$1
    local instance=$2
    local -i replicas=$4
    local -i replica=0
    local depname="${namespace}-${workload}-${instance}"
    while (( replica++ < replicas )) ; do
        local name="${depname}-${replica}"
        local vmname
	vmname=$(mkpodname "$name")
        create_object -n "$namespace" "$name" <<EOF
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
$(indent 2 standard_labels_yaml)
  name: "$vmname"
$(indent 2 standard_deployment_metadata_yaml "$namespace" client)
$(create_vm_spec -A "$affinity_yaml" -n "$vmname")
EOF
    done
}

function create_vm_spec() {
    function vm_evict_strategy() {
	if ((vm_evict_migrate)) ; then
	    echo "evictionStrategy: LiveMigrate"
	fi
    }
    local vmname="$basename"
    while getopts 'A:n:' opt; do
        case "$opt" in
	    A) affinity_yaml=$OPTARG ;;
            n) vmname=$OPTARG ;;
	    *)		      ;;
        esac
    done
    [[ -n "$vmname" ]] || fatal "vmname is required"
    pin_node=$(get_pin_node "$node_class")
    if [[ -n "${pin_node:-}" ]] ; then
	affinity_yaml="nodeSelector:${nl}  kubernetes.io/hostname: \"$pin_node\""
    fi
    cat <<EOF
spec:
  running: true
  template:
    metadata:
$(indent 6 standard_labels_yaml)
        kubevirt-vm: "$vmname"
$(indent 6 standard_deployment_metadata_yaml "$namespace" client)
$(indent 6 standard_pod_metadata_yaml "$namespace" client)
    spec:
$(indent 6 vm_evict_strategy)
      domain:
        cpu:
          cores: $vm_cores
          sockets: $vm_sockets
          threads: $vm_threads
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
$(indent 8 container_resources_yaml)
      terminationGracePeriodSeconds: $vm_grace_period
$(indent 6 <<< "$affinity_yaml")
      volumes:
        - name: containerdisk
          containerDisk:
            image: $vm_image
EOF
}

function create_replication_deployment() {
    local namespace=$1
    local instance=$2
    local -i replicas=$4
    local name="${namespace}-${workload}-${instance}"
    create_object -n "$namespace" "$name" <<EOF
apiVersion: apps/v1
kind: $deployment_type
metadata:
  name: $(mkpodname "$name")
$(indent 2 standard_deployment_metadata_yaml "$namespace" client)
$(indent 2 standard_labels_yaml -l "$workload" "$namespace" "$instance")
spec:
  replicas: $replicas
  selector:
    matchLabels:
      instance: ${namespace}-${workload}-${instance}
  strategy:
    type: RollingUpdate
  template:
    metadata:
$(indent 6 standard_labels_yaml -l "$workload" "$namespace" "$instance")
$(indent 6 standard_pod_metadata_yaml "$namespace" client)
$(indent 4 create_spec -A "$affinity_yaml" -c "$node_class" "$@")
EOF
}

function create_standard_deployment() {
    local OPTIND=0
    local OPTARG=
    local affinity_yaml=
    local workload="$requested_workload"
    local opt
    local security_context
    local allow_replica_controller=1
    local node_class=client
    local deployment_type=$deployment_type
    local arglist_function=
    local create_container_function=create_standard_containers
    local pod_annotations=("${pod_annotations[@]}")
    security_context="$(default_security_context)"
    while getopts 'A:w:S:pc:d:e:C:a:N:' opt "$@" ; do
	case "$opt" in
	    A) affinity_yaml="$OPTARG"		   ;;
	    a) arglist_function="$OPTARG"	   ;;
	    C) create_container_function="$OPTARG" ;;
	    c) node_class="$OPTARG"		   ;;
	    d) deployment_type="$OPTARG"	   ;;
	    N) pod_annotations+=("$OPTARG")	   ;;
	    p) allow_replica_controller=0	   ;;
	    S) security_context="$OPTARG"	   ;;
	    w) workload="$OPTARG"	  	   ;;
	    *)					   ;;
	esac
    done
    [[ -n "$(type -t "$create_container_function")" ]] || fatal "Cannot run create container function $create_container_function: command not found"
    [[ -z "$arglist_function" || -n "$(type -t "$arglist_function")" ]] || fatal "Cannot run arglist function $arglist_function: command not found"
    shift $((OPTIND-1))
    if (($# != 5)) ; then
	fatal "Usage: create_standard_deployment <args> namespace instance secret_count replicas containers"
    fi
    requires_drop_cache && create_drop_cache_deployment "$1" "$requested_workload" "$2" "$4"

    case "${deployment_type,,}" in
	pod)
	    create_pod_deployment "$@"
	    ;;
	replicaset|deployment)
	    if ((allow_replica_controller)) ; then
		create_replication_deployment "$@"
	    else
		create_pod_deployment "$@"
	    fi
	    ;;
        vm)
            create_vm_deployment "$@"
            ;;
	*)
	    fatal "Unknown deployment type $deployment_type"
	    ;;
    esac
}

function create_container_sync() {
    local namespace=$1
    local expected_clients=$4
    local initial_expected_clients=$5
    cat <<EOF
- name: ${namespace}
  image_pull_policy: $image_pull_policy
  image: "$container_image"
  ports:
  - containerPort: $port
$(indent 2 standard_environment)
  command:
  - "python3"
  - "$system_configmap_mount_dir/sync.py"
  - "$sync_flag_file"
  - "$sync_error_file"
  - "$controller_timestamp_file"
  - "$predelay"
  - "$postdelay"
  - "$sync_port"
  - "$sync_ns_port"
  - "$expected_clients"
  - "$initial_expected_clients"
$(indent 2 volume_mounts_yaml "$namespace" 0 0)
$(indent 2 restricted_security_context)
EOF
}

function create_sync_deployment() {
    local namespace=$1; shift
    local -a tolerations=('node-role.kubernetes.io/infra:Equal:NoSchedule' "${tolerations[@]}")
    local affinity_yaml="$(affinity=$sync_affinity create_affinity_yaml -k ${basename}-worker "true")"
    create_object -t Pod -n "$namespace" "${namespace}-sync" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: $(mkpodname "${basename}-sync")
$(indent 2 standard_deployment_metadata_yaml "$namespace" sync)
$(indent 2 standard_labels_yaml -S 'sync' "$basename")
    ${basename}-sync: "true"
  selector:
    matchLabels:
      app: ${namespace}
$(create_container_function=create_container_sync create_spec -P -c sync -r '' "$namespace" 0 0 "$@")
$(indent 2 <<< "$affinity_yaml")
  restartPolicy: Never
EOF
}

function create_container_drop_cache() {
    local namespace=$1
    local sync_service=
    local sync_port_num=
    local sync_ns_port_num=
    IFS=: read -r sync_service sync_port_num sync_ns_port_num <<< "$(get_sync)"
    cat <<EOF
- name: ${namespace}-dc
  image_pull_policy: $image_pull_policy
  image: "$container_image"
  securityContext:
    privileged: true
    runAsUser: 0
  ports:
  - containerPort: $drop_cache_port
$(indent 2 standard_environment)
  command:
  - "python3"
  - "${system_configmap_mount_dir}/drop_cache.py"
  - "$namespace"
  - "c0"
  - "$basetime"
  - "$baseoffset"
  - "$(ts)"
  - "0"
  - "$sync_service"
  - "$sync_port_num"
  - "$sync_ns_port"
  - "$drop_cache_port"
$(indent 2 volume_mounts_yaml "$namespace" 0 0)
  - mountPath: "/proc-sys-vm"
    name: "proc-sys-vm"
EOF
}

function create_drop_cache_deployment() {
    local namespace=$1
    local workload=$2
    local instance=$3
    local -i replicas=$4
    local -i replica=0
    while ((replica++ < replicas)) ; do
	local name="${namespace}-${workload}-${instance}-${replica}"
	local dcname="${name}-dc"
	create_service -H -k replica "$namespace" "${dcname}" "$drop_cache_port"
	create_object -t Pod -n "$namespace" "${namespace}-${workload}-${instance}-${replica}-dc" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: $(mkpodname "$dcname")
$(indent 2 standard_pod_metadata_yaml "$namespace" drop_cache)
$(indent 2 standard_deployment_metadata_yaml "$namespace" drop_cache)
$(indent 2 standard_labels_yaml -s dc "$workload" "$namespace" "$instance" "$replica")
  selector:
    matchLabels:
      replica: ${namespace}-${workload}-${instance}-${replica}
$(create_container_function=create_container_drop_cache create_spec -P -c drop-cache -r '' "$namespace" "$instance" 0)
  - name: "proc-sys-vm"
    hostPath:
      path: "/proc/sys/vm"
$(indent 2 create_affinity_yaml -a affinity -k replica "$name")
  restartPolicy: Never
EOF
    done
}

################################################################
# Generic Object creation
################################################################

function create_objects_n() {
    local objtype=$1; shift
    local parallel=$1; shift
    local objs_per_call=$1; shift
    local rotor=$1; shift
    local sleeptime=$1; shift
    while (( rotor < namespaces )) ; do
	"create_${objtype}" "${namespaces_to_create[$rotor]}" "$@"
	if (( sleeptime )) ; then sleep "$sleeptime"; fi
	rotor=$((rotor + parallel))
    done
    really_create_objects
}

function allocate_namespaces() {
    local -i ns_count=0
    local -i ns_idx=0
    if (( scale_ns )) ; then
        while read -r ns ; do
	    if [[ -n "$ns" ]] ; then
		namespaces_in_use[${ns#namespace/}]=1
	    fi
        done < <(__OC get ns -l "$basename" --no-headers -o name 2>/dev/null)
    fi
    while (( ns_count++ < namespaces )) ; do
        while [[ -n "${namespaces_in_use[${basename}-$ns_idx]:-}" ]] ; do
            ns_idx=$((ns_idx+1))
        done
        namespaces_to_create+=("${basename}-$((ns_idx++))")
    done
    if ((sync_start)) ; then
	if ((sync_in_first_namespace)) ; then
	    sync_namespace=${namespaces_to_create[0]}
	else
	    sync_namespace="${basename}-sync"
	fi
    fi
}

function find_first_deployment() {
    if (( scale_deployments )) ; then
	local ns
	local deployment
	local stuff
	# shellcheck disable=SC2034
        while read -r ns deployment stuff ; do
	    if [[ -n "$deployment" ]] ; then
		deployment=${deployment#"${ns}"-}
		deployment=${deployment%-*}
		echo __OC get deployments -l "$basename" -A --no-headers 1>&2
		if (( deployment + 1 > first_deployment )) ; then
		    first_deployment=$((deployment + 1))
		fi
	    fi
        done <<< "$(__OC get deployments -l "$basename" -A --no-headers 2>/dev/null)"
    fi
}

function create_all_namespaces() {
    local i
    local -a pids=()
    for i in $(seq 0 "$((parallel_namespaces - 1))") ; do
	create_objects_n namespace "$parallel_namespaces" "$objs_per_call_namespaces" "$i" "$sleep_between_namespaces" &
	pids+=("$!")
    done
    wait "${pids[@]}" || killthemall "Unable to create namespaces"
}

function create_all_secrets() {
    local i
    local -a pids=()
    for i in $(seq 0 "$((parallel_secrets - 1))") ; do
	create_objects_n secrets "$parallel_secrets" "$objs_per_call_secrets" "$i" "$sleep_between_secrets" "$deps_per_namespace" "$secrets" &
	pids+=("$!")
    done
    wait "${pids[@]}" || killthemall "Unable to create secrets"
}

function create_system_configmap() {
    if supports_api -w "$requested_workload" list_configmaps ; then
	local -a systemfiles
	readarray -t systemfiles < <(list_configmaps)
	if [[ $sync_start -ne 0 && -n "$sync_namespace" && $sync_in_first_namespace -ne 0 ]] ; then
	    create_configmaps "$sync_namespace" "systemconfigmap" "${systemfiles[@]}"
	fi
	for i in $(seq 0 "$((parallel_configmaps - 1))") ; do
	    create_objects_n configmaps "$parallel_configmaps" "$objs_per_call_configmaps" "$i" "$sleep_between_configmaps" "systemconfigmap" "${systemfiles[@]}"&
	    pids+=("$!")
	done
	wait "${pids[@]}" || killthemall "Unable to create system configmap"
    fi
}

function create_all_configmaps() {
    create_system_configmap
    (( ${#configmap_files[@]} )) || return;
    local i
    local -a pids=()
    for i in $(seq 0 "$((parallel_configmaps - 1))") ; do
	create_objects_n configmaps "$parallel_configmaps" "$objs_per_call_configmaps" "$i" "$sleep_between_configmaps" "configmap" "${configmap_files[@]}"&
	pids+=("$!")
    done
    wait "${pids[@]}" || killthemall "Unable to create configmaps"
}

function drop_host_caches() {
    ((doit && (drop_node_cache || drop_all_node_cache) )) || return 0
    local -a nodes_to_drop
    if [[ -n "${pin_nodes[*]}" && $drop_all_node_cache -eq 0 ]] ; then
	local n
	local -A tmp_nodes_to_drop
	for n in "${pin_nodes[@]}" ; do
	    tmp_nodes_to_drop[$n]=1
	done
	nodes_to_drop=("${!tmp_nodes_to_drop[@]}")
    else
	readarray -t nodes_to_drop <<< "$(____OC get -l node-role.kubernetes.io/worker node -ojsonpath="{range .items[*]}{.metadata.name}{'\n'}{end}")"
    fi
    local -a pids=()
    for n in "${nodes_to_drop[@]}" ; do
	((report_object_creation)) && echo "Dropping buffer cache: $n"
	_OC debug --no-tty=true "node/$n" -- chroot /host sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches' 2>/dev/null &
	pids+=("$!")
    done
    wait "${pids[@]}"
}

function create_all_deployments() {
    local i
    local -a pids=()
    for i in $(seq 0 "$((parallel_deployments - 1))") ; do
	create_objects_n deployment "$parallel_deployments" "$objs_per_call_deployments" "$i" "$sleep_between_deployments" "$deps_per_namespace" "$secrets" "$replicas" "$containers_per_pod" &
	pids+=("$!")
    done
    wait "${pids[@]}" || killthemall "Unable to create deployments"
}

function create_all_objects() {
    local found_objtype
    local objtype=$1
    shift
    while IFS= read -r 'line' ; do
	if [[ $line = *'__KUBEFAIL__ '* ]] ; then
	    echo "${line#__KUBEFAIL__ }" 1>&2
	    return 1
	elif [[ $line =~ ^[[:digit:]]{4}(-[[:digit:]]{2}){2}T([[:digit:]]{2}:){2}[[:digit:]]{2}\.[[:digit:]]{6}\ +(([-a-z0-9]+)(\.[-a-z0-9]*)?)/([-a-z0-9]+)\ +created$ ]] ; then
	    if (( report_object_creation )) ; then
		echo "$line" 1>&2
	    fi
	    total_objects_created=$((total_objects_created+1))
	    found_objtype=${BASH_REMATCH[4]}
	    if [[ -z ${objects_created[$found_objtype]:-} ]] ; then
		objects_created[$found_objtype]=1
	    else
		objects_created[$found_objtype]=$((${objects_created[$found_objtype]:-}+1))
	    fi
	elif [[ -n "$line" ]] ; then
	    if ((doit)) ; then
		(IFS=''; echo "$line" 1>&2)
	    else
		(IFS=''; echo "$line")
	    fi
	fi
    done < <("create_all_${objtype}" "$@" 2>&1)
}

function _do_cleanup_1() {
    local ltimeout="${force_cleanup_timeout:+--timeout=${timeout}s}"
    local force=0
    local OPTIND=0
    local opt
    while getopts 'f' opt "$@" ; do
	case "$opt" in
	    f) force=1 ;;
	    *)         ;;
	esac
    done
    shift $((OPTIND-1))
    local objects_to_clean=${1:-}
    local objects_to_clean_sync=${2:-$objects_to_clean}
    if [[ -n "$objects_to_clean" ]] ; then
	objects_to_clean="-A $objects_to_clean"
    else
	objects_to_clean=ns
    fi
    if [[ -n "$objects_to_clean_sync" ]] ; then
	objects_to_clean_sync="-A $objects_to_clean_sync"
    else
	objects_to_clean_sync=ns
    fi
    if ((force)) ; then
	shift
	# It appears that allowing processes to write to stderr inside
	# a trap results in the subprocess not doing everything
	# (in this case, oc delete doesn't complete).
	exec >/dev/null
	exec 2>/dev/null
	# shellcheck disable=SC2086
	____OC delete $objects_to_clean_sync -l "${basename}-sync" $ltimeout
	# shellcheck disable=SC2086
	____OC delete $objects_to_clean -l "$basename" $ltimeout
    elif ((report_object_creation)) ; then
	# shellcheck disable=SC2086
	__OC delete $objects_to_clean_sync -l "${basename}-sync" $ltimeout 1>&2
	# shellcheck disable=SC2086
	__OC delete $objects_to_clean -l "${basename}-sync" $ltimeout 1>&2
    else
	# shellcheck disable=SC2086
	__OC delete $objects_to_clean_sync -l "$basename" $ltimeout 2>&1 |grep 'DEBUG kubectl:' 1>&2
	# shellcheck disable=SC2086
	__OC delete $objects_to_clean -l "$basename" $ltimeout 2>&1 |grep 'DEBUG kubectl:' 1>&2
    fi
}

function _do_cleanup() {
    if ! _do_cleanup_1 "$@" ; then
	if [[ -n "$force_cleanup_timeout" ]] ; then
	    warn "*** Cleanup failed, doing a force delete!"
	    if [[ $deployment_type = vm ]] ; then
		__OC delete pod,configmap,secret,service,serviceaccout -A -l "${basename}-sync" --force --grace-period=0 1>&2
		__OC delete vm,configmap,secret,service,serviceaccount -A  -l "$basename" --force --grace-period=0 1>&2
	    else
		__OC delete deployment,replicaset,pod,configmap,secret,service,serviceaccount -A  -l "$basename" --force --grace-period=0 1>&2
	    fi
	    if ((use_namespaces && remove_namespaces)) ; then
		__OC delete namespace -l "$basename" --force --grace-period=0 1>&2
	    fi
	fi
    fi
}

# shellcheck disable=SC2120
function do_cleanup() {
    local force=
    local OPTIND=0
    local -i precleanup=0
    local opt
    while getopts 'fp' opt "$@" ; do
	case "$opt" in
	    f) force=-f      ;;
	    p) precleanup=1 ;;
	    *)              ;;
	esac
    done
    shift "$OPTIND"
    local objects_to_clean=
    local objects_to_clean_sync=
    local objtype
    (( doit )) || return 0
    if ((use_namespaces && remove_namespaces)) ; then
	objects_to_clean=namespace
    elif [[ $deployment_type = vm ]] ; then
	objects_to_clean_sync="configmap,secret,service,pod"
	objects_to_clean="configmap,secret,service,vm"
    else
	objects_to_clean="deployment,replicaset,configmap,secret,service,pod"
    fi
    if [[ -n "${injected_errors[precleanup]:-}" && $precleanup -gt 0 ]] ; then
	warn "*** Injecting precleanup error ${inject_errors[precleanup]:-}"
	sleep "${injected_errors[precleanup]:-}"
	killthemall "Cleanup timed out!"
    fi
    if [[ -n "${injected_errors[cleanup]:-}" && $precleanup -eq 0 ]] ; then
	warn "*** Injecting cleanup error ${inject_errors[cleanup]:-}"
	sleep "${injected_errors[cleanup]:-}"
	killthemall "Cleanup timed out!"
    fi
    # shellcheck disable=SC2086
    _do_cleanup $force "$objects_to_clean" "$objects_to_clean_sync" || killthemall "Cleanup timed out!"
}

################################################################
# Main work loop
################################################################

function create_secrets_top_level() {
    # If previous secrets weren't all created, this will yield
    # the wrong result.
    if (( doit && wait_for_secrets )) ; then
	local -i i
	local -i j
	local -i k
	local ns
	local nsd
	for i in $(seq 0 $((namespaces - 1))) ; do
	    ns="secret/secret-${basename}-${namespaces_to_create[$i]##*-}"
	    for j in $(seq "$first_deployment" $((deps_per_namespace + first_deployment - 1))) ; do
		nsd="${ns}-$j"
		for k in $(seq 0 $((secrets - 1))) ; do
		    expected_secrets["${nsd}-$k"]=1
		done
	    done
	done
    fi
    create_all_objects secrets
    if (( doit )) ; then
	local secname=""
	while (( ${#expected_secrets[@]} )) ; do
	    echo "Expect ${#expected_secrets[@]}" 1>&2
	    while read -r secname ; do
		[[ -n "${secname:-}" ]] && unset "expected_secrets[$secname]"
	    done < <(__OC get secret -oname --no-headers -l "$basename" -A)
	    if (( ${#expected_secrets[@]} )) ; then
		echo "Still waiting for ${#expected_secrets[@]} secrets to be created."
		sleep 10
	    else
		break
	    fi
	done
    fi
}

function do_logging() {
    ((wait_forever)) && sleep infinity
    ((report)) || return 0
    local -i logs_expected=0
    logs_expected="$(calculate_logs_required "$namespaces" "$deps_per_namespace" "$replicas" "$containers_per_pod")"
    ((logs_expected > 0)) || return 0
    local -i get_logs_pid=0
    trap 'if ((get_logs_pid > 1)) ; then kill -TERM "$get_logs_pid"; wait "$get_logs_pid" 2>/dev/null; fi; return 1' TERM

    local -a sync_pods=()
    local namespace
    local pod
    local ignore
    local namespace_retrieved=
    # shellcheck disable=SC2034
    while read -r namespace pod ignore ; do
	if [[ -n "$namespace" && -n "$pod" ]] ; then
	    if [[ -z "$namespace_retrieved" ]] ; then
		namespace_retrieved=$namespace
		sync_pods+=("-n" "$namespace")
	    elif [[ "$namespace" != "$namespace_retrieved" ]] ; then
		killthemall "Namespace mismatch: $namespace vs $namespace_retrieved"
	    fi
	    sync_pods+=("$pod")
	fi
    done <<< "$(____OC get pod -A --no-headers -l "${basename}-sync=${uuid}")"
    get_logs "${sync_pods[@]}" &
    get_logs_pid=$!
    if ((timeout > 0)) ; then
	local -i itimeout=$timeout
	local -i finis=0
	while (( timeout < 0 || timeout-- )) ; do
	    # CreateContainerError is not necessarily fatal
	    if __OC get pods -A -l "$basename" |grep -q -E -e '([^r]Error|Evicted|Crash)' ; then
		echo "Run failed:" 1>&2
		__OC get pods -A -l "$basename" |grep -E -e '([^r]Error|Evicted|Crash)' | {
		    local line
		    local ns
		    local pod
		    local rest
		    while read -r line ; do
			read -r ns pod rest <<< "$line"
			echo "$line" 1>&2
			____OC logs -n "$ns" "$pod" 1>&2
			echo 1>&2
		    done
		}
		finis=-1
		break
	    fi
	    # Commands in a pipeline are each run in subshells.  Background jobs are child
	    # of the parent, not any children, so running "jobs -l" inside the pipeline
	    # will not reap any exited subjobs.  The child still has the job table, so
	    # it knows what subjobs exist, but the "jobs -l" must be run at top level
	    # to actually reap them.
	    jobs -l >/dev/null
	    if jobs -l |grep -q . ; then
		sleep 1
	    else
		finis=1
		break
	    fi
	done
	if (( finis <= 0 )) ; then
	    if ((finis < 0)) ; then
		killthemall "Run failed"
	    else
		killthemall "Run did not terminate in $itimeout seconds"
	    fi
	fi
    fi
    if (( get_logs_pid > 1 )) ; then
	wait "$get_logs_pid" 2>/dev/null
    fi
}

function report_object_creation() {
    (( report_object_creation )) || return 0
    local objtype
    while read -r objtype ; do
	[[ -n "${objtype:-}" ]] && printf "Created %${#total_objects_created}d %s%s\n" "${objects_created[$objtype]}" "$objtype" "${plurals[$((${objects_created[$objtype]} == 1))]}" 1>&2
    done < <( (IFS=$'\n'; echo "${!objects_created[*]}") | sort)
    printf "Created %d object%s total\n" "$total_objects_created" "${plurals[$((total_objects_created == 1))]}" 1>&2
}

function expand_string() {
    local string=$1
    shift
    local -A _____dict_____=()
    local -a arg
    for arg in "$@" ; do
	local noptname1 optname optvalue
	read -r noptname optname optvalue <<< "$(parse_option "$arg")"
	_____dict_____["$optname"]="$optvalue"
    done
    while [[ $string =~ (.*)(%\{([[:alnum:]_]+(\[[[:alnum:]]+\])?)(:-([^\}]*))?\})(.*) ]] ; do
	local prefix=${BASH_REMATCH[1]:-}
	local var=${BASH_REMATCH[3]:-}
	local replacement=${BASH_REMATCH[6]:-UNKNOWN$var}
	local suffix=${BASH_REMATCH[7]:-}
	if [[ -n "${_____dict_____[$var]+isset}" ]] ; then
	    string="${prefix}${_____dict_____[$var]}${suffix}"
	else
	    string="${prefix}${!var:-$replacement}${suffix}"
	fi
    done
    echo "$string"
}

function run_clusterbuster_2() {
    local -A expected_secrets=()
    local -i status=0
    if (( precleanup && doit )) ; then
	do_cleanup -p 1>&2 || return 1
    fi
    if ((cleanup_always)) ; then
	trap 'echo "Cleaning up" 1>&2; doit=1; do_cleanup -f; killthemall ""; wait; remove_tmpdir; doit=1; do_cleanup -f; exit 1' TERM INT HUP
    else
	trap 'killthemall "Not cleaning up"; wait; remove_tmpdir; exit 1' TERM HUP INT
    fi
    if [[ $doit -gt 0 ]] ; then
	if [[ -n "${artifactdir:-}" && $take_prometheus_snapshot -gt 0 ]] ; then
	    start_prometheus_snapshot || return 1
	else
	    set_start_timestamps || return 1
	fi
	if [[ -n "${artifactdir:-}" ]] ; then
	    artifactdir=${artifactdir//%s/$(readable_timestamp "$prometheus_starting_timestamp")}
	    artifactdir=${artifactdir//%w/$requested_workload}
	    artifactdir=${artifactdir//%n/$job_name}
	    artifactdir="$(expand_string "$artifactdir")"
	    mkdir -p "$artifactdir" || fatal "Can't create log directory $artifactdir"
	    echo "${saved_argv[@]@Q}" > "$artifactdir/commandline"
	    exec 2> >(tee >(tr "\r" "\n" > "$artifactdir/stderr.log") >&2)
	fi
    fi
    allocate_namespaces || return 1
    find_first_deployment || return 1
    if [[ $sync_start -ne 0 && -n "$sync_namespace" && $sync_in_first_namespace -eq 0 ]] ; then
	create_namespace -S -f "$sync_namespace" || return 1
    fi
    create_all_objects namespaces || return 1
    if get_sync -q ; then
	if (( use_namespaces )) ; then
	    global_sync_service="svc-${sync_namespace}-sync.${sync_namespace}.svc.cluster.local"
	else
	    global_sync_service="svc-${basename}-sync-0-sync"
	fi
    fi

    (( ${#configmap_files[@]} )) && check_configmaps "${configmap_files[@]}"
    create_all_objects configmaps || return 1
    supports_api -w "$requested_workload" list_configmaps && has_system_configmap=1

    create_secrets_top_level || return 1

    basetime=$(date +%s.%N)
    (drop_host_caches)
    create_all_objects deployments || return 1
    (( doit )) || return 0

    do_logging || return 1
    if ((! status)) ; then
	report_object_creation
    fi
    return 0
}

function run_clusterbuster_1() {
    local -i status=0
    run_clusterbuster_2
    status=$?
    if (( (cleanup_always || (cleanup && status == 0)) && doit)) ; then
	do_cleanup || {
	    if ((status == 0)) ; then
		status=1
	    fi
	}
    fi

    if ((status)) ; then
	fatal "Clusterbuster failed!"
    else
	if [[ $take_prometheus_snapshot -gt 0 && -n "$artifactdir" ]] ; then
	    retrieve_prometheus_snapshot "$artifactdir"
	fi
    fi
    return $status
}

################################################################
# Top level
################################################################

function remove_tmpdir() {
    [[ -n "${cb_tempdir:-}" && -d "$cb_tempdir" && $cb_tempdir = "/tmp/cbtmp_"* ]] && rm -rf "$cb_tempdir" && unset cb_tempdir
}

trap "remove_tmpdir; exit" EXIT
trap "remove_tmpdir; exit 1" TERM INT HUP
cb_tempdir=$(umask 77; mktemp -d -p /tmp -t cbtmp_XXXXXXXX) || fatal "Can't create temporary directory"

load_workloads "$__workloaddir__"

set_metrics_file default

while getopts ":B:Eef:Hhno:P:Qqv-:" opt ; do
    case "$opt" in
	B) process_option "basename=$OPTARG"	 ;;
	E) process_option "exit_at_end=0"	 ;;
	e) process_option "exit_at_end=1"	 ;;
	f) process_job_file "$OPTARG"		 ;;
	h) help					 ;;
	H) help_extended			 ;;
	n) process_option "doit=0"		 ;;
	o) process_option "reportformat=$OPTARG" ;;
	P) process_option "workload=$OPTARG"     ;;
	Q) process_option "reportobjectcreate=0" ;;
	q) process_option "verbose=0"		 ;;
	v) process_option "verbose=1"		 ;;
	-) process_option "$OPTARG"		 ;;
	*) help "$OPTARG"			 ;;
    esac
done

if [[ -z "$requested_workload" ]] ; then
    help "Workload must be specified with -P"
fi

iworkload=$requested_workload
requested_workload=$(get_workload "$requested_workload") || help_extended "(Unknown workload '$iworkload')"
arch=${arch:-$(uname -m)}
container_image=${container_image:-quay.io/rkrawitz/clusterbuster:${arch}-latest}

call_api -w "$requested_workload" -s "process_options" "${unknown_opts[@]}" || help "${unknown_opt_names[@]}"

if supports_api -w "$requested_workload" list_user_configmaps ; then
    readarray -t userfiles < <(list_user_configmaps)
    configmap_files+=("${userfiles[@]}")
fi

if (( namespaces <= 0 )) ; then
    namespaces=1
    use_namespaces=0
fi
(( parallel_configmaps )) || parallel_configmaps=$parallel
(( parallel_secrets )) || parallel_secrets=$parallel
(( parallel_namespaces )) || parallel_namespaces=$parallel
(( parallel_deployments )) || parallel_deployments=$parallel

(( !objs_per_call_configmaps )) && objs_per_call_configmaps=$objs_per_call
(( !objs_per_call_secrets )) && objs_per_call_secrets=$objs_per_call
(( !objs_per_call_namespaces )) && objs_per_call_namespaces=$objs_per_call
(( !objs_per_call_deployments )) && objs_per_call_deployments=$objs_per_call

[[ -n "$report_format" ]] && report=1

if [[ -n "$metrics_file" && ! -r "$metrics_file" ]] ; then
    fatal "Cannot read metrics file $metrics_file"
fi

case "${deployment_type,,}" in
    # If we're using deployments or replicasets, we do not want
    # to exit when workers complete, or the controller will
    # immediately try to re-create them.
    replicaset|rs)         deployment_type=ReplicaSet; exit_at_end=0 ;;
    deployment|dep|deploy) deployment_type=Deployment; exit_at_end=0 ;;
    pod) deployment_type=pod                                         ;;
    vm) deployment_type=vm                                           ;;
    *)
	echo "--deployment_type must be pod, vm, deployment, or replicaset"
	help
esac

if [[ -z $OC && $doit -gt 0 ]] ; then
    fatal "Cannot find oc or kubectl command, exiting!"
fi

resource_validation_failed=0
validate_resource resource_request "${resource_requests[@]}" || resource_validation_failed=1
validate_resource resource_limit "${resource_limits[@]}" || resource_validation_failed=1
if ((resource_validation_failed)) ; then
    exit 1
fi

for i in $(seq 0 $((emptydir_volumes-1)) ) ; do
    emptydirs+=("/mnt/volume-$i")
done

job_name=${job_name:-${requested_workload:-}}

((virtiofsd_direct)) && virtiofsd_args+=('"-o"' '"allow_direct_io"')
((virtiofsd_writeback)) && virtiofsd_args+=('"-o"' '"writeback"')
((virtiofsd_threadpoolsize)) && virtiofsd_args+=("\"--thread-pool-size=$virtiofsd_threadpoolsize\"")
if [[ -n "${virtiofsd_args[*]:-}" ]] ; then
    pod_annotations+=("io.katacontainers.config.hypervisor.virtio_fs_extra_args: '[$(IFS=,; echo "${virtiofsd_args[*]}")]'")
fi

shift $((OPTIND - 1))
if [[ -n ${1:-} ]] ; then
    basename="$1"
    shift
fi

[[ -z "$*" ]] || warn "Warning: extraneous arguments $* after basename will be ignored!"

(run_clusterbuster_1) 4>&2
