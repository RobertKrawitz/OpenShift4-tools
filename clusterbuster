#!/bin/bash

# Copyright 2019-2022 Robert Krawitz/Red Hat
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Unfortunate that there's no way to tell shellcheck to always source
# specific files.
# shellcheck disable=SC2034

# See the bash change log, differences between 4.4-beta2 and 4.4-rc2:
# a.  Using ${a[@]} or ${a[*]} with an array without any assigned elements when
#     the nounset option is enabled no longer throws an unbound variable error.
if (( BASH_VERSINFO[0] >= 5 || (BASH_VERSINFO[0] == 4 && BASH_VERSINFO[1] >= 4) )) ; then
    set -u
else
    cat 1>&2 <<EOF
Warning: bash version at least 4.4 is recommended for using ${0##*/}.
Actual version is ${BASH_VERSION}
EOF
fi

declare ___arg
for ___arg in "$@" ; do
    if [[ "${___arg:-}" = '--force-abort'* ]] ; then
	echo "*** Warning: will abort on any shell error!" 1>&2
	set -e
	set -o errtrace
	#trap 'killthemall "Caught error, aborting!"' ERR
	break
    fi
done

declare -i namespaces=1
declare -i use_namespaces=1
declare -i deps_per_namespace=1
declare -i secrets=0
declare -i replicas=1
declare -i parallel=1
declare -i first_deployment=0
declare -i sleep_between_secrets=0
declare -i sleep_between_configmaps=0
declare -i sleep_between_namespaces=0
declare -i sleep_between_deployments=0
declare -i parallel_secrets=0
declare -i parallel_configmaps=0
declare -i parallel_namespaces=0
declare -i parallel_deployments=0
declare -i objs_per_call=1
declare -i objs_per_call_secrets=0
declare -i objs_per_call_configmaps=0
declare -i objs_per_call_namespaces=0
declare -i objs_per_call_deployments=0
declare -i containers_per_pod=1
declare -i sleeptime=0
declare -i doit=1
declare -i objs_item_count=0
declare -i port=7777
declare -i sync_port=7778
declare -i drop_cache_port=7779
declare log_host=
declare -i log_port=0
declare sync_host=
declare -i affinity=0
declare -A pin_nodes=()
declare -A runtime_classes=()
declare runtime_class
declare -i verbose=0
declare -i wait_for_secrets=1
declare -i bytes_transfer=0
declare -i bytes_transfer_max=0
declare -i default_bytes_transfer=1000000000
declare -i workload_run_time=0
declare -i workload_run_time_max=0
declare -i exit_at_end=1
declare -i report_object_creation=1
declare -A objects_created=()
declare baseoffset=0
declare -i metrics_epoch=0
declare requested_workload=
declare basename=${CLUSTERBUSTER_DEFAULT_BASENAME:-clusterbuster}
declare deployment_type=pod
declare basetime
declare opt
declare -r nl=$'\n'
declare -a resource_requests=()
declare -a resource_limits=()
declare -A namespaces_in_use=()
declare -a namespaces_to_create=()
declare sync_namespace=
declare -i scale_ns=0
declare -i scale_deployments=1
declare -i sync_start=1
declare -a emptydirs=()
declare -a volumes=()
declare -A volume_mount_paths=()
declare -A volume_types=()
declare -A volume_type_keys=()
declare -A volume_scoped_names=()
declare common_workdir=
declare -i report=0
declare report_format=summary
declare log_strategy=poll
declare -i precleanup=1
declare -i cleanup=0
declare -i timeout=0
declare pathdir=${0%/*}
declare -a unknown_opts=()
declare -a unknown_opt_names=()
declare -i total_objects_created=0
# <name, filename with contents>
declare -a configmap_files=()
declare -a tolerations=()
declare configmap_mount_dir=/etc/map
declare system_configmap_mount_dir=/etc/bootstrap
declare -i has_system_configmap=0
declare node_selector='node-role.kubernetes.io/worker'
declare -i pbench_controller_port=0
declare pbench_controller_address=
declare -i processes_per_pod=1
declare -i emptydir_volumes=0
declare -i take_prometheus_snapshot=0
declare first_start_timestamp=
declare prometheus_starting_timestamp=
declare prometheus_exact_starting_timestamp=
declare second_start_timestamp=
declare prometheus_ending_timestamp=
declare -i target_data_rate=0
declare metrics_file=
declare job_name=
# shellcheck disable=2155
declare -i job_start_time=$(date +%s)
declare debug=
declare global_sync_service=
declare -i predelay=0
declare -i postdelay=0
declare -r default_metrics_file=metrics-default.yaml
declare -i drop_node_cache=0
declare -i drop_all_node_cache=0
declare -i headless_services=1
declare -i virtiofsd_writeback=0
declare -i virtiofsd_direct=1
declare -i virtiofsd_threadpoolsize=0
declare -a virtiofsd_args=()
declare -i liveness_probe_frequency=0
declare -i liveness_probe_sleep_time=0
declare pod_prefix=

declare -r sync_flag_file="/tmp/syncfile";
declare -r controller_timestamp_file="/tmp/timing.json";
# shellcheck disable=2155
declare uuid=$(uuidgen -r)

declare kata_runtime_class=kata
declare image_pull_policy=
declare container_image=quay.io/rkrawitz/bench-army-knife

declare accumulateddata=
declare -a plurals=('s' '')
declare artifactdir=
declare -a saved_argv=("$0" "$@")
declare -a processed_options=("$0")
declare -a pod_annotations=()
declare -A injected_errors=()
declare cb_tempdir=
declare force_cleanup_timeout=

declare OC=${OC:-${KUBECTL:-}}
OC=${OC:-$(type -p oc)}
OC=${OC:-$(type -p kubectl)}	# kubectl might not work, though...

function fatal() {
    echo "$*" 1>&2
    exit 1
}

function warn() {
    echo "$*" 1>&2
}

if [[ -z "$OC" ]] ; then
    fatal "Can't find kubectl or oc"
fi

declare __me__
__me__=$(realpath -e "$0")

if [[ -z "$__me__" ]] ; then
    echo "Can't find my path!" 1>&2
    exit 1
fi

declare __topdir__=${__me__%/*}
declare __libdir__=${__topdir__}/lib/clusterbuster
declare __podfile_dir__=${__libdir__}/pod_files
declare __workloaddir__=${__libdir__}/workloads

[[ -d "$__libdir__" ]] || fatal "Can't find my library dir!"
[[ -d "$__workloaddir__" ]] || fatal "Can't find my workload directory $__workloaddir__!"

. "${__libdir__}"/libclusterbuster.sh

function _helpmsg() {
    local opt
    for opt in "$@" ; do
	echo "Unknown option $opt"
    done
cat <<EOF
Clusterbuster is a tool to permit you to load a configurable workload
onto an OpenShift cluster.  ClusterBuster focuses primarily on workload
scalability, including synchronization of multi-instance workloads.

Usage: $0 [options] [name]

    Help:
       -h              Print basic help information.
       -H              Print extended help.

    Options:
       -B basename     Base name of pods.  Default is
                       \$CLUSTERBUSTER_DEFAULT_BASENAME if defined or
                       otherwise 'clusterbuster'.
                       All objects are labeled with this name.
       -E              Don't exit after all operations are complete.
       -e              Exit after all operations are complete (default).
       -f jobfile      Job file containing settings
                       A number of examples are provided in the
                       examples/clusterbuster directory.
       -n              Print what would be done without doing it
       -o              Specify report format, as --report-format
       -P workload     workload (mandatory), one of:
$(print_workloads '                       - ')
       -q              Do not print verbose log messages (default)
       -Q              Don't report creation of individual objects (default
                       report them)
       -v              Print verbose log messages.
       --opt[=val]     Set the specified option.
                       Use ${0##*/} -H to list the available options.
EOF
}

function _help_extended() {
    if [[ -z "$*" ]] ; then
	_helpmsg "$@"
	cat <<'EOF'

Extended Options:
EOF
    fi
    cat <<EOF
    General Options (short equivalents):
       --doit=<1,0>     Run the command or not (default 1) (inverse of -n)
       --jobname=name   Name of the job, for logging purposes.
                        Defaults to the workload name
       --workload=type  Specify the workload (-P) (mandatory)
       --basename=name  Specify the base name for any namespaces (-B)
       --namespaces=N   Number of namespaces
       --jobfile=jobfile
                        Process job file (-f)
       --sync           Synchronize start of workload instances (default yes)
       --precleanup     Clean up any prior objects
       --cleanup        Clean up generated objects
       --predelay=N     Delay for the specified time after workload
                        starts before it starts running.
       --postdelay=N    Delay for the specified time after workload
                        completes.
       --timeout=N      Time out reporting after N seconds
       --report_object_creation=<1,0>
                        Report creation of individual objects (default 1)
                        (inverse of -Q)
       --uuid=<uuid>    Use the specified UUID for the run.  Default is to
                        generate a random-based UUID.
       --exit_at_end    Exit upon completion of workload (-e/-E)
       --verbose        Print verbose log messages (-v)

    Reporting Options:
       --report=<format>
                        Print report in specified format.  Meaning of
                        report types is by type.  Default is summary
                        if not specified; if that is not reported,
                        raw format will be used.
                        - none
$(list_report_formats '                        - ')
                        The following workloads support reporting:
$(print_workloads_supporting_reporting '                        - ')
       --artifactdir=<dir>
                        Save artifacts to <dir>.  <dir> can have embedded
                        format codes:
                        %n              Job name
                        %s              Timestamp of run
                        %w              Workload
                        %{var}          Variable's value is substituted
                        %{var[item]}    to reference an array variable
                        %{var:-default} to use a default value if not set
       --prometheus-snapshot
                        Take a Prometheus snapshot and save to the
                        artifacts directory
       --metrics[=<file>]
                        benchmark-runner compatible metrics file
                        for metrics extraction if promextract
                        reporting is in use.  With no argument, uses
                        default in $default_metrics_file.
       --metrics-epoch=<seconds>
                        Number of seconds to look back for metrics prior
                        to start of run (default $metrics_epoch)

    Workload sizing options:
       --containers_per_pod=N
                        Number of containers per pod
       --deployments=N  Number of deployments or pods per namespace
       --processes=N    Number of processes per pod
       --replicas=N     Number of replicas per deployment
       --secrets=N      Number of secrets

    Generic workload rate options:
       --bytestransfer=N[,M]
                        Number of bytes for workloads operating on
                        fixed amounts of data.
       --targetdatarate=N
                        Target data rate for workloads operating at fixed
                        data rates.  May have suffixes of K, Ki,
                        M, Mi, G, Gi, T, or Ti.
       --workloadruntime=N
                        Time to run the workload where applicable
                        Two comma-separated numbers may be used to
                        specify maximum time.

    Workload placement options:
       --pin_node=[class1,class2...]=<node>
                        Force pod(s) of the specified class(es) onto the
                        specified node.  Multiple comma-separated classes
                        may be specified.  The following classes are
                        defined for general workloads:
                        - sync   (sync pods)
                        - client (worker/client pods)
                        Workloads may define other classes.
                        If no class is specified, pin node applies to all
                        pods.
       --affinity       Force affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides affinity.
       --anti-affinity  Force anti-affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides anti-affinity.
       --drop_cache     Drop the buffer cache in all pin nodes; if no
                        pin nodes are defined, drop all workers' caches.
       --drop_all_cache Drop the buffer cache on all workers.

    Generic workload storage options:
       --emptydir=dir   Mount an emptydir volume at the specified mount point.
       --volumes=N      Mount the specified number of emptydir volumes.
                        If this is not provided, file-based workloads will
                        use a default location, generally /tmp.
       --volume=name:type:name_type:mount_name:mount_path
                        Mount a specified persistent volume
                        name is the name of the volume (required).
                        type is the type of volume (required).
                        mount_path is the path on which to mount the volume
                            (required).
                        type_key is the key for the volume (e. g.
                            claimName for persistentVolumeClaim)
                        scoped_name is the volume's name as recognized
                            by the description.  All occurrences of %N
                            are replaced by the namespace of the pod
                            mounting the volume; all instances of %i
                            are replaced by the instance of the pod
                            within the namespace.
       --workdir=<dir>  Use the specified working directory for file I/O

    Pod Options:
       --container_image=<image>
                        Image to use (default $container_image).
                        Does not apply to "classic" or "pause" workloads.
       --deployment_type=<pod,deployment>
                        Deploy via individual pods or deployments (default $deployment_type)
       --external_sync=host:port
                        Sync to external host rather than internally
       --request=<resource=value>
                        Resource requests
       --limit=<resource=value>
                        Resource limits
       --runtimeclass=[class1,class2...]=class
                        Run the pods in the designated runtimeclass.
       --kata           Synonym for --runtimeclass=${kata_runtime_class}
       --tolerate=<key:operator:effect>
                        Apply the specified tolerations to created pods.
       --image_pull_policy=<policy>
                        Image pull policy (system default)
       --node_selector=selector
                        Annotate pods with the specified node selector
                        Default $node_selector
                        Specify empty value to not provide a node selector.
       --pod_annotation=[:class:]annotation
                        Apply the specified annotation to all pods of the
                        optionally specified class (same meaning as for
                        --pin_node as above).  This may be specified
                        multiple times.
       --headless-services=[0,1]
                        Use headless services for service creation.
                        Default ${headless_services}
       --liveness-probe=<interval>
                        Execute a simple liveness probe every <interval>
                        seconds.
       --liveness-probe-sleep=<seconds>
                        Arrange for the liveness probe to sleep for specified
                        time.

    Tuning object creation (short equivalents):
       --scale-ns=[0,1] Scale up the number of namespaces vs.
                        create new ones (default 0).
       --scale-deployments=[0,1]
                        Scale up the number of deployments vs.
                        create new ones (default 1)
       --first_deployment=N
                        Specify the index of the first deployment.
                        Default is 0 (but see --scale_deployments)
       --first_secret=N
                        Index number of first secret to be created
       --first_namespace=N
                        Index number of first namespace to be created
       --first_deployment=N
                        Index number of first pod to be created
       --pod-prefix=prefix
                        Prefix all created pods with this prefix.
       --sleep=N        Number of seconds between object creations
                        Below options default to sleeptime
       --sleep_between_secrets=N
       --sleep_between_namespaces=N
       --sleep_between_deployments=N

       --objs_per_call=N
                        Number of objects per CLI call.  Only objects
                        within a namespace can be created this way;
                        to improve creation performance with multiple
                        namespaces, use --parallel.
                        Below options all default to objs_per_call
       --objs_per_call_secrets=N
       --objs_per_call_namespaces=N
       --objs_per_call_deployments=N

       --parallel=N     Number of operations in parallel.  Only
                        operations across namespaces can be
                        parallelized; to improve performance
                        within one namespace, use --objs_per_call.
                        Below options all default to parallel
       --parallel_secrets=N
       --parallel_namespaces=N
       --parallel_deployments=N
       --wait_secrets   Wait for secrets to be created (default 1)

    Using PBench:
       If pbench is to be used, both arguments below must be provided.
       --pbench_controller_address=address
                        Use the specified address to connect to the controller.
       --pbench_controller_port=port
                        Use the specified port to connect to the controller.

Workload-specific options:
$(_help_options_workloads)

Advanced options (generally not required):
       --baseoffset=N   Add specified offset to base time
                        for calculation of start time offset
                        to correct for clock skew.  May be float.
                        This normally should not be needed, as
                        ClusterBuster can correct for clock skew
                        itself.
       --log-strategy={poll,listen}
                        Use the desired logging strategy to retrieve logs.
                        - Poll indicates that sync pods should be polled
                          for logs.
                        - Listen indicates that pods should report log
                          data directly to this tool, which will listen
                          for the data.
                        The default strategy is polling, which should work
                        in all cases.  The older listen strategy will not
                        work if pods cannot connect to the host from which
                        this tool is run.  Polling should be used unless
                        there is reason to believe it is not functioning.
       --podsleep=N     Time for pod to sleep before exit
       --inject_error=<opt>
                        For testing purposes, inject the specified error
                        condition (documented only in code).
       --force-abort    Abort the run on any error.

Here is a brief description of all available workloads:
$(_document_workloads)

EOF
}

function help() {
    _help_extended "$@" | "${PAGER:-more}" 1>&2
    exit 1
}

function help_extended() {
    _help_extended "$@" | "${PAGER:-more}" 1>&2
    exit 1
}

################################################################
# Option processing
################################################################

function set_workload_bytes() {
    local sizespec=$1
    local -i scale=${2:-1}
    if [[ $sizespec = *','* ]] ; then
	bytes_transfer=$(parse_size "${sizespec#*,}")
	bytes_transfer_max=$(parse_size "${sizespec%%,*}")
	if (( bytes_transfer > bytes_transfer_max )) ; then
	    local -i tmp=$bytes_transfer
	    bytes_transfer=$bytes_transfer_max
	    bytes_transfer_max=$tmp
	fi
    else
	bytes_transfer=$((sizespec * scale))
	bytes_transfer_max=$((sizespec * scale))
    fi
}

function set_runtime() {
    local timespec=$1
    if [[ $timespec = *','* ]] ; then
	workload_run_time=${timespec#*,}
	workload_run_time_max=${timespec%%,*}
	if (( workload_run_time > workload_run_time_max )) ; then
	    local -i tmp=$workload_run_time
	    workload_run_time=$workload_run_time_max
	    workload_run_time_max=$tmp
	fi
    else
	workload_run_time=$timespec
	workload_run_time_max=$timespec
    fi
}

function parse_volume_spec() {
    local volspec=$1
    local vname=
    local vtype=
    local vmount_path=
    IFS=':' read -r vname vtype vmount_path vtype_key vscoped_name <<< "$volspec"
    if [[ -z "$vname" || -z "$vtype" || -z "$vmount_path" ]] ; then
	echo "name, type, type_key, scoped name, and mount path must be provided"
	echo "for volumes"
	exit 1
    fi
    volumes+=("$vname")
    volume_mount_paths["$vname"]=$vmount_path
    volume_types["$vname"]=$vtype
    volume_type_keys["$vname"]=$vtype_key
    volume_scoped_names["$vname"]=$vscoped_name
}

function process_pin_node() {
    local nodespec=$1
    nodespec=${nodespec// /}
    [[ -n "$nodespec" ]] || return
    if [[ $nodespec = *'='* ]] ; then
	local node=${nodespec#*=}
	local class=${nodespec%%=*}
	class=${class//,/ }
	# shellcheck disable=SC2206
	local -a classes=($class)
	for class in "${classes[@]}" ; do
	    pin_nodes[$class]="$node"
	done
    else
	pin_nodes[default]="$nodespec"
    fi
}

function process_runtimeclass() {
    local runtimespec=$1
    runtimespec=${runtimespec// /}
    runtime_class=$runtimespec
    [[ -n "$runtimespec" ]] || return
    if [[ $runtimespec = *'='* ]] ; then
	local runtime=${runtimespec#*=}
	local class=${runtimespec%%=*}
	class=${class//,/ }
	# shellcheck disable=SC2206
	local -a classes=($class)
	for class in "${classes[@]}" ; do
	    runtime_classes[$class]="$runtime"
	done
    else
	runtime_classes[default]="$runtimespec"
    fi
}

function set_metrics_file() {
    metrics_file=${1:-}
    if [[ -z "$metrics_file" || $metrics_file == 1 ]] ; then
	metrics_file="$__libdir__/$default_metrics_file"
    fi
}

function inject_error() {
    local error="$1"
    local condition
    local options
    IFS='=' read -r condition options <<< "$error"
    injected_errors["$condition"]=${options:-SET}
    warn "*** Registering error injection '$condition' = '${injected_errors[$condition]}'"
}

function process_option() {
    local noptname
    local noptname1
    local optvalue
    read -r noptname1 noptname optvalue <<< "$(parse_option "$1")"
    # shellcheck disable=SC2206
    # shellcheck disable=SC2119
    processed_options+=("--$1")
    case "$noptname1" in
	# Help, verbosity
	helpall*)		    help_extended				;;
	helpeverything*)	    help_extended				;;
	help*)			    help					;;
	verbose)		    verbose=$(bool "$optvalue")			;;
	doit)			    doit=$(bool "$optvalue")			;;
	quiet)			    verbose=$((! $(bool "$optvalue")))		;;
	forceabort*)		    set -e					;;
	# Reporting
	artifactdir)		    artifactdir="$optvalue"			;;
	logstrategy)		    log_strategy=$optvalue			;;
	metrics|metricsfile)	    set_metrics_file "$optvalue"		;;
	metricsepoch)		    metrics_epoch=$optvalue			;;
	reportformat)		    report_format=$optvalue			;;
	jsonreport)		    report_format=json				;;
	rawreport)		    report_format=raw				;;
	report)		    	    report_format=${optvalue:-summary}		;;
	verbosereport)		    report_format=verbose			;;
	reportobjectcreation)	    report_object_creation=$(bool "$optvalue")	;;
	prometheussnapshot)	    take_prometheus_snapshot=$(bool "$optvalue");;
	predelay)		    predelay=$optvalue				;;
	postdelay)		    postdelay=$optvalue				;;
	timeout)		    timeout=$optvalue				;;
	# Basic options
	jobname)		    job_name=$optvalue				;;
	workload)		    requested_workload=$optvalue		;;
	basename)		    basename=$optvalue				;;
	# Object definition
	workdir)		    common_workdir=$optvalue			;;
	configmapfile)		    configmap_files+=("$optvalue")		;;
	containerimage)		    container_image=$optvalue			;;
	containers)		    containers_per_pod=$optvalue		;;
	containersperpod)	    containers_per_pod=$optvalue		;;
	deploymenttype)		    deployment_type=$optvalue			;;
	deployments|depspername*)   deps_per_namespace=$optvalue		;;
	emptydir)		    emptydirs+=("$optvalue")			;;
	volumes)		    emptydir_volumes=$optvalue			;;
	exitatend)		    exit_at_end=$(bool "$optvalue")		;;
	imagepullpolicy)	    image_pull_policy=$optvalue			;;
	namespaces)		    namespaces=$optvalue			;;
	nodeselector)		    node_selector=$optvalue			;;
	volume)			    parse_volume_spec "$optvalue"		;;
	pbenchcontrolleraddress)    pbench_controller_address=$optvalue		;;
	pbenchcontrollerport)	    pbench_controller_port=$optvalue		;;
	processes|processesperpod)  processes_per_pod=$optvalue			;;
	jobfile)		    process_job_file "$optvalue"		;;
	pinnode)		    process_pin_node "$optvalue"		;;
	replicas)		    replicas=$optvalue				;;
	limit|limits)		    resource_limits+=("$optvalue")		;;
	request|requests)	    resource_requests+=("$optvalue")		;;
	kata)			    process_runtimeclass "kata"			;;
	podannotation)		    pod_annotations+=("$optvalue")		;;
	runtimeclass)		    process_runtimeclass "$optvalue"		;;
	uuid)			    uuid=$optvalue				;;
	secrets)		    secrets=$optvalue				;;
	workloadruntime)	    set_runtime "$optvalue"			;;
	workload_size)		    set_workload_bytes "$optvalue"		;;
	targetdatarate)		    target_data_rate=$(parse_size "$optvalue")	;;
	tolerate|toleration)	    tolerations+=("$optvalue")			;;
	dropcache)                  drop_node_cache=$(bool "$optvalue")         ;;
	dropallcache)		    drop_all_node_cache=$(bool "$optvalue")     ;;
	headlessservices)	    headless_services=$(bool "$optvalue")	;;
	virtiofsdwriteback)	    virtiofsd_writeback=$(bool "$optvalue")	;;
	virtiofsddirect)	    virtiofsd_direct=$(bool "$optvalue")	;;
	virtiofsdthread*)	    virtiofsd_threadpoolsize=$optvalue		;;
	livenessprobeint*)	    liveness_probe_frequency=$optvalue		;;
	livenessprobesleep*)	    liveness_probe_sleep_time=$optvalue		;;
	affinity)
	    case "$optvalue" in
		1|'') affinity=1      ;;
		2|anti) affinity=2    ;;
		*) affinity=0         ;;
	    esac
	    ;;
	antiaffinity)
	    case "$optvalue" in
		1|'') affinity=2      ;;
		*) affinity=0         ;;
	    esac
	    ;;
	# Object creation
	objspercall)		    objs_per_call=$optvalue			;;
	parallel)		    parallel=$optvalue				;;
	sleep)			    sleeptime=$optvalue				;;
	podprefix)		    pod_prefix=${optvalue:+$optvalue-}		;;
	firstdeployment)	    first_deployment=$optvalue			;;
	parallelconfigmaps)	    parallel_configmaps=$optvalue		;;
	parallelsecrets)	    parallel_secrets=$optvalue			;;
	parallelnamespaces)	    parallel_namespaces=$optvalue		;;
	paralleldeployments)	    parallel_deployments=$optvalue		;;
	objspercallconfigmaps)	    objs_per_call_configmaps=$optvalue		;;
	objspercallsecrets)	    objs_per_call_secrets=$optvalue		;;
	objspercallnamespaces)	    objs_per_call_namespaces=$optvalue		;;
	objspercalldeployments)	    objs_per_call_deployments=$optvalue		;;
	sleepbetweenconfigmaps)	    sleep_between_configmaps=$optvalue		;;
	sleepbetweensecrets)	    sleep_between_secrets=$optvalue		;;
	sleepbetweennamespaces)	    sleep_between_namespaces=$optvalue		;;
	sleepbetweendeployments)    sleep_between_deployments=$optvalue		;;
	waitsecrets)		    wait_for_secrets=$(bool "$optvalue")	;;
	scalens)	   	    scale_ns=$(bool "$optvalue")		;;
	scaledeployments)   	    scale_deployments=$(bool "$optvalue")	;;
	precleanup)		    precleanup=$(bool "$optvalue")		;;
	cleanup)		    cleanup=$(bool "$optvalue")			;;
	baseoffset)		    baseoffset=$optvalue			;;
	# Synchronization
	sync|syncstart)		    sync_start=$((1-sync_start))		;;
	externalsync)
	    if [[ $optvalue =~ ^(-|([[:alnum:]][-_[:alnum:]]*[[:alnum:]]\.)*([[:alnum:]][-_[:alnum:]]*[[:alnum:]])):([1-9][[:digit:]]{0,4})$ ]] ; then
		sync_host=${BASH_REMATCH[1]}
		sync_port=${BASH_REMATCH[4]}
		if (( sync_port > 65535 )) ; then
		    echo "Illegal external sync port (must be 1 <= port <= 65535)"
		    help
		fi
		sync_start=1
	    else
		echo "Undecipherable external sync host:port $optvalue"
		help
	    fi
	    ;;
	# Testing
	injecterror)		    inject_error "$optvalue"			;;
	# Force delete everything after oc delete times out.
	# This is dangerous and hence not documented.
	# See https://access.redhat.com/solutions/4165791
	forcecleanupiknowthisisdangerous)
	    force_cleanup_timeout=${optvalue:-600}
	    ;;
	# Unknown options
	*)
	    unknown_opts+=("$1")
	    unknown_opt_names+=("$noptname ($noptname1)") ;;
    esac
}

function process_job_file() {
    local jobfile="$1"
    if [[ ! -f $jobfile || ! -r $jobfile ]] ; then
	fatal "Job file $jobfile cannot be read"
    fi
    while IFS= read -r line ; do
	line=${line%%#*}
	line=${line## }
	line=${line##	}
	if [[ -z "$line" ]] ; then continue; fi
	process_option "$line"
    done < "$jobfile"
}

function validate_resource() {
    local rtype=$1
    local token
    local status=0
    shift
    for token in "$@" ; do
	if [[ $token != *'='* ]] ; then
	    warn "Invalid $rtype specification $token (must be <resource>=<quantity>)"
	    status=1
	fi
    done
    return $status
}

################################################################
# Workload API management
################################################################

function print_workloads_supporting_reporting() {
    local prefix="${1:-}"
    local workloads
    while read -r workload ; do
	echo "$prefix$workload"
    done <<< "$(workloads_supporting_api supports_reporting)"
}

function list_report_formats() {
    local prefix=${1:-}
    while read -r format ; do
	echo "$prefix$format"
    done <<< "$("${pathdir:-.}/clusterbuster-report" --list_formats)"
}

function json_encode_settings() {
    call_api -w "$requested_workload" json_encode_settings
}

function create_deployment() {
    call_api -w "$requested_workload" create_deployment "$@"
}

function calculate_logs_required() {
    if supports_api -w "$requested_workload" supports_reporting ; then
	call_api -w "$requested_workload" calculate_logs_required "$@"
    else
	echo 1
    fi
}

function list_configmaps() {
    cat <<EOF
${__podfile_dir__}/clientlib.pl
${__podfile_dir__}/sync.pl
EOF
    call_api -w "$requested_workload" list_configmaps
}

function list_user_configmaps() {
    call_api -w "$requested_workload" list_user_configmaps
}

function generate_workload_metadata() {
    call_api -w "$requested_workload" -s generate_metadata
}

################################################################
# Helpers
################################################################

function __OC() {
    if ((! doit)) ; then
	echo "$OC" "$@" 1>&2
    elif [[ $1 = exec || $1 = rsh ]] ; then
	# Capture error output
	(set -o pipefail; "$OC" "$@" 2>&1 1>&3 |sed -e "s/^/${*//\//\\/}: /" |timestamp 1>&2) 3>&1
    elif [[ $1 = describe || $1 = get || $1 = status || $1 = logs ]] ; then
	"$OC" "$@"
    elif ((report_object_creation)) ; then
	if [[ -n "$debug" ]] ; then
	    echo "$OC" "$@" 1>&2
	fi
	"$OC" "$@" 2>&1
    else
	"$OC" "$@" 2>&1 |grep -v -E '(^No resources found|deleted|created|labeled|condition met)$'
    fi
    return "${PIPESTATUS[0]}"
}

function _OC() {
    if ! __OC "$@" ; then
	fatal "__KUBEFAIL__ $OC $* failed!"
    fi
}

function ___OC() {
    if ! __OC "$@" ; then
	fatal "$OC $* failed!"
    fi
}

# The big red button if something goes wrong.

function childrenof() {
    IFS=' '
    function is_descendent() {
	local -i child=$1
	local ancestor=$2
	if ((child == ancestor)) ; then
	    true
	else
	    parent=${ps_parents[$child]:-1}
	    case "$parent" in
		1)           false ;;
		"$ancestor") true  ;;
		*)           is_descendent "$parent" "$ancestor" ;;
	    esac
	fi
    }
    local -A exclude=()
    local OPTIND=0
    while getopts 'e:' opt "$@" ; do
	case "$opt" in
	    e) exclude[$OPTARG]=1 ;;
	    *)                    ;;
	esac
    done
    shift $((OPTIND-1))
    local target=$1
    local -A ps_parents=()
    local -A ps_commands=()
    local ppid
    local pid
    local command
    local ignore
    while read -r pid ppid command ignore; do
	ps_parents["$pid"]=$ppid
	ps_commands["$pid"]=$command
    done <<< "$(ps -axo pid,ppid,command | tail -n +2)"
    for pid in "${!ps_parents[@]}" ; do
	# Don't kill loggers and the report generator; we still want
	# output even if there's a failure.
	if [[ -z "${!exclude[$pid]:-}" && "${ps_commands[$pid]}" != 'tee'* && "${ps_commands[$pid]}" != *'clusterbuster-report'* ]] ; then
	    if is_descendent "$pid" "$target" ; then
		echo "$pid"
	    fi
	fi
    done
}

function killthemall() {
    echo "${*:-Exiting!}" 1>&2
    # Selectively kill all processes.  We don't want to kill
    # processes between us and the root, or processes that
    # aren't actually clusterbuster (e. g. reporting)
    # Also, RHEL 8 doesn't support kill -<pgrp> syntax
    local -a pids_to_kill=()
    readarray -t pids_to_kill <<< "$(childrenof -e "$BASHPID" $$)"
    # killthemall can livelock under the wrong circumstances.
    # Make sure that that doesn't happen; we want to control
    # our own exit.
    trap : TERM
    if [[ -n "${pids_to_kill[*]}" ]] ; then
	/bin/kill -TERM "${pids_to_kill[@]}" >/dev/null 2>&1
	if (( $$ == BASHPID )) ; then
	    wait
	    local -i tstart
	    tstart=$(date +%s)
	    until [[ -z "$(ps -o pid "${pids_to_kill[@]}" | awk '{print $1}' | grep -v "^${$}$" | tail -n +2)" ]] ; do
		sleep 1
		if (( $(date +%s) - tstart > 60 )) ; then
		    warn "Unable to terminate all processes!"
		    ps "${pids_to_kill[@]}" 1>&2
		    break
		fi
	    done
	fi
    fi
    exit 1
}

################################################################
# Synchronization between worker pods
################################################################

function get_sync() {
    if (( sync_start )) ; then
	if [[ ${1:-} != -q ]] ; then
	    echo "${global_sync_service}:$sync_port"
	fi
	return 0
    else
	return 1
    fi
}

function create_sync_service_if_needed() {
    local namespace=$1
    local sync_count=${2:-0}
    local sync_clients=${3:-0}
    if ((sync_count <= 0)) ; then return; fi
    if get_sync -q ; then
	if [[ $namespace = "${namespaces_to_create[0]:-}" ]] ; then
	    create_service -h "$sync_namespace" "$sync_namespace" "$sync_port"
	    create_sync_deployment "$sync_namespace" "$sync_count" $((sync_clients * ${#namespaces_to_create[@]}))
	fi
	create_external_service "$namespace" "${namespace}-sync" "$sync_port" "$global_sync_service"
    fi
}

################################################################
# Logging
################################################################

function get_pod_timestamp() {
    local format=${1:-%s.%N}
    shift
    ___OC exec "$@" -- /bin/sh -c "date '+$format'" || killthemall "Unable to retrieve timestamp from pod $*"
}

function get_prometheus_pod_timestamp() {
    local format=${1:-%s.%N}
    get_pod_timestamp '%s.%N' -n openshift-monitoring prometheus-k8s-0 -c prometheus
}

function readable_timestamp() {
    local rawtime=${1:-}
    date -u '+%Y_%m_%dT%H_%M_%S%z' ${rawtime:+"--date=@$rawtime"}
}

function set_start_timestamps() {
    local -i retries=5
    while ((retries-- > 0)) ; do
	first_start_timestamp=$(date +%s.%N)
	if prometheus_exact_starting_timestamp=$(get_prometheus_pod_timestamp '%s.%N') ; then
	    prometheus_starting_timestamp=${prometheus_exact_starting_timestamp%%.*}
	    second_start_timestamp=$(date +%s.%N)
	    return 0
	fi
	if ((retries > 0)) ; then
	    warn "Fetch timestamp failed! $retries attempt(s) left"
	    sleep 5
	else
	    warn "Unable to retrieve Prometheus timestamp (hard error)"
	fi
    done
    return 1
}

function get_pod_and_local_timestamps() {
    until __OC exec "$@" -- /bin/sh -c "date +%s.%N" </dev/null >/dev/null 2>/dev/null ; do
	local status
	status=$(__OC get pod "$@" -o jsonpath="{.status.phase}" 2>/dev/null)
	case "$status" in
	    Error|Failed)
		echo "Sync pod failed to start:" 1>&2
		__OC logs "$@" | tail -10 1>&2
		exit 1
		;;
	    *)  ;;
	esac
	sleep 2
    done
    local first_local_ts
    local remote_ts
    local second_local_ts
    first_local_ts=$(date +%s.%N)
    remote_ts=$(__OC exec "$@" -- /bin/sh -c "date +%s.%N" </dev/null) || killthemall "Unable to retrieve sync pod timestamp"
    second_local_ts=$(date +%s.%N)
    _OC exec --stdin=true "$@" -- /bin/sh -c "cat > '$controller_timestamp_file'" <<EOF
{
  "first_controller_ts": $first_local_ts,
  "sync_ts": $remote_ts,
  "second_controller_ts": $second_local_ts
}
EOF
    # shellcheck disable=SC2181
    if (( $? != 0 )) ; then
	killthemall "Unable to write timing data to sync pod"
    fi
    echo "$first_local_ts" "$remote_ts" "$second_local_ts"
}

function start_prometheus_snapshot() {
    ((doit)) || return 0
    echo "Starting Prometheus snapshot" | echo_if_desired 1>&2
    _OC delete pod -n openshift-monitoring prometheus-k8s-0
    local -i retry=12
    until __OC get pod -n openshift-monitoring prometheus-k8s-0 >/dev/null 2>&1 ; do
	echo "Promtheus pod did not start, $retry attempt(s) left" | echo_if_desired 1>&2
	if ((retry <= 0)) ; then
	    killthemall "Prometheus pod did not restart!"
	fi
	retry=$((retry-1))
	sleep 5
    done
    __OC wait --for=condition=Ready -n openshift-monitoring pod/prometheus-k8s-0 || killthemall "Prometheus pod did not become ready"
    set_start_timestamps || return 1
    # Wait for prometheus pod to fully initialize
    sleep 60
    echo "Prometheus snapshot started" | echo_if_desired 1>&2
}

function retrieve_prometheus_snapshot() {
    ((doit)) || return 0
    echo "Retrieving Prometheus snapshot" | echo_if_desired 1>&2
    local dir=${1:-$artifactdir}
    sleep 60
    prometheus_ending_timestamp=$(get_prometheus_pod_timestamp)
    local promdb_name
    promdb_name="promdb_$(readable_timestamp "$prometheus_starting_timestamp")_$(readable_timestamp "$prometheus_ending_timestamp")"
    local promdb_path="${dir:+${dir}/}${promdb_name}.tar"
    if __OC exec -n openshift-monitoring prometheus-k8s-0 -c prometheus -- /bin/sh -c "tar cf - . -C /prometheus --transform 's,^[.],./${promdb_name},' .; true" > "$promdb_path" ; then
	echo "Prometheus snapshot retrieved" | echo_if_desired 1>&2
    else
	echo "Unable to retrieve Prometheus snapshot"
    fi
}

function ts() {
    local dt
    dt=$(date '+%s.%N')
    local sec=${dt%.*}
    local ns=${dt#*.}
    echo "${sec}.${ns:0:6}"
}

function timestamp() {
    while IFS= read -r 'LINE' ; do
	printf "%s %s\n" "$(TZ=GMT-0 date '+%Y-%m-%dT%T.%N' | cut -c1-26)" "$LINE"
    done
}

function monitor_pods() {
    exec 0</dev/null 1>/dev/null
    local arg
    local -i timeout=-1
    local -i ppid=0
    local OPTIND=0
    while getopts "t:" arg "$@" ; do
	case "$arg" in
	    t) timeout="$OPTARG" ;;
	    *)                   ;;
	esac
    done
    shift $((OPTIND - 1))
    if ((timeout > 0)) ; then
	timeout=$(($(date +%s) + timeout))
    fi
    local pods_pending=''
    local -i pods_pending_retry=12
    local -i pods_unchanged_countdown=$pods_pending_retry
    if [[ -n "${injected_errors[timeout]:-}" ]] ; then
	warn "*** Injecting forced timeout error"
    fi
    while : ; do
	# Stop work as soon as we know we're done.
	if [[ -z "${injected_errors[timeout]:-}" && -n "$cb_tempdir" && -f "$cb_tempdir/___run_complete" ]] ; then
	    return 0
	fi
	# shellcheck disable=2034
	local -a pending_pods=()
	local -i running_pod_count=0
	local -i finished_pod_count=0
	local -i other_pod_count=0
	while read -r namespace name rdy status rest ; do
	    if [[ -z "${injected_errors[timeout]:-}" && -n "$cb_tempdir" && -f "$cb_tempdir/___run_complete" ]] ; then
		# Don't exit out of this, which will leave the oc get pods process around
		continue
	    fi
	    case "${status,,}" in
		error|failed)
		    echo "Pod -n $namespace $name $status!" 1>&2
		    echo "Tail end of logs:" 1>&2
		    oc logs -n "$namespace" "$name" |tail -20 1>&2
		    killthemall
		    ;;
		pending|containercreating)
		    other_pod_count=$((other_pod_count+1))
		    pending_pods+=("$namespace/$name")
		    ;;
		running)
		    running_pod_count=$((running_pod_count+1))
		    ;;
		completed|terminated)
		    if supports_api -w "$requested_workload" supports_reporting ; then
			if ! __OC exec --stdin=false -n "${basename}-sync" "$(mkpodname "${basename}-sync")" -- test -f /tmp/clusterbuster-finished ; then
			    echo "Sync pod exited prematurely!" 1>&2
			    echo "Tail end of logs:" 1>&2
			    oc logs -n "${basename}-sync" "${basename}-sync" |tail -20 1>&2
			    killthemall "Unexpected termination of sync pod"
			fi
		    fi
		    finished_pod_count=$((finished_pod_count+1))
		    ;;
		*)
		    other_pod_count=$((other_pod_count+1))
		    ;;
	    esac
	done <<< "$(__OC get pods -A --no-headers "$@" 2>/dev/null)"
	if [[ -z "${injected_errors[timeout]:-}" && -n "$cb_tempdir" && -f "$cb_tempdir/___run_complete" ]] ; then
	    return 0
	fi
	if [[ -z "${injected_errors[timeout]:-}" ]] && ! supports_api -w "$requested_workload" supports_reporting ; then
	    if (( running_pod_count + finished_pod_count > 0 && other_pod_count == 0 )) ; then
		sleep "$workload_run_time"
		return
	    fi
	fi

	if [[ -n "${injected_errors[pending]:-}" ]] ; then
	    warn "*** Injecting pending error"
	    pending_pods+=("TEST/TEST")
	fi

	if [[ -n "${pending_pods[*]}" ]] ; then
	    pods_now_pending=$(IFS=$'\n'; echo "${pending_pods[*]}" | sort)
	    if [[ "$pods_now_pending" != "$pods_pending" ]] ; then
		pods_unchanged_countdown=$pods_pending_retry
		pods_pending="$pods_now_pending"
	    else
		if ((pods_unchanged_countdown <= 0)) ; then
		    killthemall "No progress with pods after 10 retries, ${#pending_pods[@]} pods still pending."
		fi
		local times=times
		if ((pods_unchanged_countdown == 1)) ; then times='time' ; fi
		echo "${#pending_pods[@]} pods pending (will retry $pods_unchanged_countdown $times)" | echo_if_desired 1>&2
		pods_unchanged_countdown=$((pods_unchanged_countdown-1))
	    fi
	fi
	if ((timeout > 0 && $(date +%s) > timeout)) ; then
	    killthemall "Monitor pods timeout!"
	fi
	local -i i
	for i in {1..5} ; do
	    if [[ -z "${injected_errors[timeout]:-}" && -n "$cb_tempdir" && -f "$cb_tempdir/___run_complete" ]] ; then
		return 0
	    fi
	    sleep 1
	done
    done
}

function run_logger() {
    local -a pids_to_kill=()
    local OPTIND=0
    while getopts "p:" arg "$@" ; do
	case "$arg" in
	    p) pids_to_kill+=("$OPTARG") ;;
	    *)                           ;;
	esac
    done
    shift $((OPTIND-1))
    "$@" </dev/null
    # shellcheck disable=SC2046
    if [[ -n "${pids_to_kill[*]}" ]] ; then
	exec 2>/dev/null
	kill -USR2 "${pids_to_kill[@]}"
	[[ -n "$(jobs -p)" ]] && kill $(jobs -p)
    fi
}

function _log_helper() {
    if [[ -n "${injected_errors[timeout]:-}" ]] ; then
	echo "*** Injecting forced timeout error"
	sleep infinity
    fi
    until [[ $("$OC" get pod -ojsonpath='{.status.phase}' "$@" 2>/dev/null) != 'Running' ]] ; do
	# Need to produce periodic output to stderr to prevent oc exec connection from prematurely terminating
	"$OC" exec --stdin=false "$@" -- sh -c "while [[ ! -f '$sync_flag_file' ]] ; do sleep 5; echo KEEPALIVE 1>&2; done; jq -rS . '$sync_flag_file'" 2>/dev/null && break
	sleep 5
    done
    # Tell the pod monitor to stop.
    if [[ -n "$cb_tempdir" ]] ; then
	touch "$cb_tempdir/___run_complete"
    fi
    "$OC" exec --stdin=false "$@" -- sh -c "rm -f '$sync_flag_file'" || killthemall "Can't terminate sync"
}

function _get_logs_poll() {
    local pod
    local status
    local counter=$#
    for pod in "$@" ; do
	pod_status_found=0
	while : ; do
	    # shellcheck disable=SC2086
	    status=$(__OC get pod $pod -o jsonpath="{.status.phase}" 2>/dev/null)
	    case "$status" in
		Running)
		    break
		    ;;
		Pending)
		    pod_status_found=1
		    sleep 1
		    ;;
		Error|Failed)
		    pod_status_found=1
		    echo "Status of pod $pod failed!" 1>&2
		    return 1
		    ;;
		''|Succeeded)	# Might be an old pod not yet deleted
		    if ((pod_status_found)) ; then
			echo "Unexpected status '$status' of pod $pod!" 1>&2
			return 1
		    else
			sleep 1
		    fi
		    ;;
		*)
		    pod_status_found=1
		    echo "Unexpected status '$status' of pod $pod!" 1>&2
		    return 1
		    ;;
	    esac
	done
	# shellcheck disable=SC2086
	local cleanup_command="; sleep 1; rm -f '$sync_flag_file'"
	local mypid=$BASHPID
	monitor_pods -- -l "${basename}-worker=$uuid" &
	# shellcheck disable=SC2086
	if supports_api -w "$requested_workload" supports_reporting ; then
	    # Ensure that if there's a lot of output that we don't remove the sync file before it has all been flushed out
	    run_logger -p "$mypid" -- _log_helper $pod &
	fi
	trap 'kill $(jobs -p) 2>/dev/null 1>&2 && wait' USR2
	wait
	if ((--counter > 0)) ; then
	    echo ','
	fi
    done
}

function get_logs_poll() {
    local tfile
    tfile=$(mktemp ${cb_tempdir:+-p "$cb_tempdir"} -t '_clusterbusterlogs.XXXXXXXXXX') || fatal "Cannot create temporary file"
    trap 'if [[ -n "${tfile:-}" && -f "$tfile" ]] ; then rm -f "$tfile"; unset tfile; fi; echo "Status: Fail"; return 1' INT TERM HUP USR1
    _get_logs_poll "$@" > "$tfile"
    local status=$?
    if ((status == 0)) ; then
	echo -n '"Results":'; cat < "$tfile"; echo ','
	echo '"Status": "Success"'
    else
	echo '"Status": "Fail"'
    fi
    if [[ -n "${tfile:-}" && -f "$tfile" ]] ; then
	rm -f "$tfile"
    fi
    unset tfile
    return $status
}

function get_logs_listen() {
    "${__libdir__}/get_logs" "$@"
}

function __report_one_volume() {
    local volname=$1
	cat <<EOF
  {
     "name": "$volname",
     "mount_path": "${volume_mount_paths[$volname]}",
     "type": "${volume_types[$vname]}",
     "type_key": "${volume_types[$vname]}",
     "scoped_name": "${volume_scoped_names[$vname]}"
  }
EOF
}

function _report_volumes() {
    echo '"volumes": ['
    local -a vols=()
    for volname in "${volumes[@]}" ; do
	vols+=("$(__report_one_volume "$volname")")
    done
    (IFS=$',\n'; echo "${vols[*]}")
    echo '],'
}

function _report_emptydirs() {
    if ((${#emptydirs[@]})) ; then
	echo "emptydirVolumes: ["
	local -a ed=("${emptydirs[@]}")
	ed=("${ed[@]/#/}")
	ed=("${ed[@]/%/}")
	(IFS=$',\n'; echo "${ed[*]}")
	echo "],"
    fi
}

function quote_list() {
    local -a a=("$@")
    a=("${a[@]//\"/\\\"}")
    a=("${a[@]//\$'\n'/ }")
    a=("${a[@]/#/\"}")
    a=("${a[@]/%/\"}")
    (IFS=$',\n '; echo "${a[*]}")
}

function _report_pin_nodes() {
    local -a nodes
    local class
    for class in "${!pin_nodes[@]}" ; do
	nodes+=("\"$class\": \"${pin_nodes[$class]}\"")
    done
    (IFS=$',\n '; echo "${nodes[*]}")
}

function _report_runtime_classes() {
    local -a runtimeclasses
    local class
    for class in "${!runtime_classes[@]}" ; do
	runtimeclasses+=("\"$class\": \"${runtime_classes[$class]}\"")
    done
    (IFS=$',\n '; echo "${runtimeclasses[*]}")
}

function _extract_metrics() {
    if [[ -r "$metrics_file" ]] ; then
	cat <<EOF
,
"metrics": $("${__topdir__}/prom-extract" --define "namespace_re=${basename}-.*" -m "$metrics_file" --metrics-only --start_time="$prometheus_starting_timestamp" --epoch="$metrics_epoch" --post-settling-time=0)
EOF
    fi
}

function _workload_reporting_class() {
    if supports_api -w "$requested_workload" workload_reporting_class ; then
	call_api -w "$requested_workload" workload_reporting_class
    else
	echo "$requested_workload"
    fi
}

function _report_liveness_probe() {
    cat <<EOF
"liveness_probe_frequency": $liveness_probe_frequency,
"liveness_probe_sleep_time": $liveness_probe_sleep_time,
EOF
}

function _get_kata_version() {
    local kata_version
    kata_version=$(oc get csv  -n openshift-sandboxed-containers-operator -o jsonpath="{.items[0].spec.version}" 2>/dev/null)
    if [[ -n "$kata_version" ]] ; then
	cat <<EOF
"kata_version": "${kata_version:-}",
EOF
    fi
}

function _report_objects() {
    local first_local_ts
    local remote_ts
    local second_local_ts
    # shellcheck disable=SC2206
    local -a podname=($*)
    read -r first_local_ts remote_ts second_local_ts <<< "$(get_pod_and_local_timestamps "$@")"
    if [[ -z "$remote_ts" ]] ; then
	killthemall "Unable to retrieve sync timestamp"
    fi
    sync_ts_error_interval=$(bc <<< "$second_local_ts - $first_local_ts")
    sync_ts_delta=$(bc <<< "$second_local_ts - $first_local_ts")
    cat <<EOF
"metadata": {
  "kind": "clusterbusterResults",
  "controller_first_start_timestamp": $first_start_timestamp,
  "prometheus_start_timestamp": $prometheus_exact_starting_timestamp,
  "controller_second_start_timestamp": $second_start_timestamp,
  "cluster_start_time": "$(readable_timestamp "${prometheus_starting_timestamp}")",
  "job_name": "$job_name",
  "uuid": "$uuid",
  "workload": "$requested_workload",
  "workload_reporting_class": "$(_workload_reporting_class)",
  "kubernetes_version": $(__OC version -ojson),
  "command_line": [$(quote_list "${saved_argv[@]}")],
  "expanded_command_line": [$(quote_list "${processed_options[@]}")],
  "runHost": "$(hostname -f)",
  "workload_metadata": {$(generate_workload_metadata)},
  "controller_presync_timestamp": $first_local_ts,
  "sync_timestamp": $remote_ts,
  "controller_postsync_timestamp": $second_local_ts,
  "artifact_directory": "$artifactdir",
  $(_get_kata_version)
  "options": {
    "basename": "$basename",
    "containers_per_pod": $containers_per_pod,
    "deployments_per_namespace": $deps_per_namespace,
    "namespaces": $namespaces,
    "bytes_transfer": $bytes_transfer,
    "bytes_transfer_max": $bytes_transfer_max,
    "workload_run_time": $workload_run_time,
    "workload_run_time_max": $workload_run_time_max,
    "headless_services": $headless_services,
    "drop_cache": $drop_node_cache,
    "always_drop_cache": $drop_all_node_cache,
    "pin_nodes": {$(_report_pin_nodes)},
    $(indent 4 _report_volumes)
    $(_report_liveness_probe)
    "secrets": $secrets,
    "replicas": $replicas,
    "container_image": "$container_image",
    "node_selector": "$node_selector",
    $(indent 4 container_resources_json)
    "runtime_classes": {$(_report_runtime_classes)},
    $(indent 4 _report_emptydirs)
    "target_data_rate": $target_data_rate,
    "workloadOptions": {
    $(indent 4 call_api -w "$requested_workload" "report_options")
    }
  }
},
"nodes": $(__OC get nodes -ojson),
"api_objects": $( __OC get all -A -l "${basename}-id=$uuid" -ojson |jq -r .items?),
"csvs": $(__OC get csv -A -ojson | jq -r '[foreach .items[]? as $item ([[],[]];0; {name: $item.metadata.name, namespace: $item.metadata.namespace, version: $item.spec.version})]'),
EOF
}

function _retrieve_artifacts() {
    if [[ -n "$artifactdir" ]] ; then
	mkdir -p "$artifactdir/Logs"
	local -i failcount=0
	while read -r namespace pod container ; do
	    if ! __OC logs -n "$namespace" -c "$container" "$pod" 2>/dev/null > "$artifactdir/Logs/${namespace}:${pod}:${container}" ; then
		rm -f "$artifactdir/Logs/${namespace}:${pod}:${container}"
		failcount=$((failcount+1))
		if ((failcount <= 5)) ; then
		    echo "Unable to retrieve logs for pod $namespace/$pod:$container" 1>&2
		fi
	    fi
	done <<< "$(__OC get pod -A -l "${basename}-id=$uuid" -ojson | jq -r '[foreach .items[]? as $item ([[],[]];0; (if ($item.kind == "Pod") then ([foreach $item.spec.containers[]? as $container ([[],[]];0; $item.metadata.namespace + " " + $item.metadata.name + " " + $container.name)]) else null end))] | flatten | map (select (. != null))[]')"
	if ((failcount > 5)) ; then
	    echo "+ $((failcount - 5)) more" 1>&2
	fi
    fi
}

function _get_logs() {
    echo '{'
    # shellcheck disable=SC2068
    _report_objects ${@:2}
    "$@"
    _extract_metrics
    echo '}'
    _retrieve_artifacts
}

function print_report() {
    if [[ $report_format == raw ]] ; then
	cat
    else
	"${pathdir:-.}/clusterbuster-report" ${report_format:+-o "$report_format"}
    fi
}

function get_logs() {
    trap - TERM
    local extra_arg=
    local log_cmd
    if supports_api -w "$requested_workload" supports_reporting ; then
	log_cmd=get_logs_poll
	if [[ ${log_strategy,,} = 'listen'* ]] ; then
	    log_cmd=get_logs_listen
	fi
    else
	log_cmd=true
    fi
    local report_cmd=cat
    if supports_api -w "$requested_workload" supports_reporting ; then
	if [[ -z "$*" ]] ; then
	    echo "Workload $requested_workload does not support reporting" 1>&2
	elif [[ -n "$artifactdir" ]] ; then
	    _get_logs "$log_cmd" "$@" > "$artifactdir/clusterbuster-report.tmp.json" && {
		mv "$artifactdir/clusterbuster-report.tmp.json" "$artifactdir/clusterbuster-report.json"
		print_report < "$artifactdir/clusterbuster-report.json"
	    }
	else
	    _get_logs "$log_cmd" "$@" | print_report
	fi
    else
	echo "Workload $requested_workload does not support reporting" 1>&2
    fi
    return $((PIPESTATUS[0] || PIPESTATUS[1]))
}

################################################################
# YAML fragment generation
################################################################

function _indent() {
    local -i column="$1"
    local line
    while IFS='' read -r 'line' ; do
	[[ -z "$line" ]] || printf "%${column}s%s\n" ' ' "$line"
    done
}

function indent() {
    local -i column="$1"
    shift
    if [[ -n "$*" ]] ; then
	# "$@" | _indent "$column" strips whitespace with bash 4.2
	_indent "$column" < <("$@")
    else
	_indent "$column"
    fi
}

function mkpodname() {
    local podname="${pod_prefix}$(IFS=-; echo "$*")"
    echo "${podname//_/-}"
}

function tolerations_yaml() {
    local toleration
    (( ${#tolerations[@]} )) || return
    echo "tolerations:"
    for toleration in "${tolerations[@]}" ; do
	# shellcheck disable=SC2206
	local -a tol=(${toleration//:/ })
	cat <<EOF
- key: "${tol[0]:-}"
  operator: "${tol[1]:-}"
  effect: "${tol[2]:-}"
EOF
    done
}

function container_resources_yaml() {
    function resources() {
	local token
	for token in "$@" ; do
	    local resource=${token%%=*}
	    local value=${token#*=}
	    echo "$resource: $value"
	done
    }
    if (( ${#resource_limits[@]} + ${#resource_requests[@]} )) ; then
	echo "resources:"
	if (( ${#resource_limits[@]} )) ; then
	    echo "  limits:"
	    indent 4 resources "${resource_limits[@]}"
	fi
	if (( ${#resource_requests[@]} )) ; then
	    echo "  requests:"
	    indent 4 resources "${resource_requests[@]}"
	fi
    fi
}

function liveness_probe_yaml() {
    if ((liveness_probe_frequency > 0)) ; then
	cat <<EOF
livenessProbe:
  exec:
    command:
    - /bin/sleep "$liveness_probe_sleep_time"
  initialDelaySeconds: 10
  periodSeconds: $liveness_probe_frequency
EOF
    fi
}

function mk_container_args() {
    local arg
    while read -r arg ; do
	echo "- \"$arg\""
    done
}

function standard_container_args() {
    local namespace=$1
    local container=$2
    mk_container_args <<EOF
$namespace
c$container
$basetime
$baseoffset
$(ts)
$exit_at_end
$sync_service
$sync_port_num
$log_host
$log_port
EOF
}

function make_standard_containers() {
    local OPTARG
    local OPTIND=0
    local opt
    local command="${requested_workload}.pl"
    local image="$container_image"
    while getopts 'c:i:' opt "$@" ; do
	case "$opt" in
	    c) command="$OPTARG";;
	    i) image="$OPTARG"  ;;
	    *)		        ;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    local instance=$2
    local secret_count=$3
    local containers_per_pod=$5
    local -i container
    local sync_service=
    local sync_port_num=
    local container_args="$(mk_container_args)"
    IFS=: read -r sync_service sync_port_num <<< "$(get_sync)"
    for container in $(seq 0 $((containers_per_pod - 1))) ; do
	cat <<EOF
- name: "c${container}"
  imagePullPolicy: $image_pull_policy
  image: "$container_image"
$(indent 2 container_standard_auxiliary_yaml)
  env:
  - name: VERBOSE
    value: "$verbose"
$(indent 2 bootstrap_command_yaml "$command")
$(indent 2 standard_container_args "$namespace" "$container")
$(indent 2 <<< "$container_args")
$(indent 2 volume_mounts_yaml "$namespace" "${instance}" "$secret_count")
EOF
    done
}

function container_standard_auxiliary_yaml() {
    container_resources_yaml
    liveness_probe_yaml
}

function resources() {
    local token
    for token in "$@" ; do
	local resource=${token%%=*}
	local value=${token#*=}
	if [[ ! $value =~ ^-?[[:digit:]]+(\.[[:digit:]]+)?$ ]] ; then
	    value="\"$value\""
	fi
	echo "\"$resource\": $value"
    done
}

function resource_json() {
    local request_type=$1
    shift
    echo "    \"$request_type\": {"
    local -a resources=()
    local resource
    for resource in "$@" ; do
	resources+=("$(resources "$resource")")
    done
    (IFS=$',\n        '; echo "${resources[*]}")
    echo "    }"
}

function container_resources_json() {
    local -a res_data=()
    if (( ${#resource_limits[@]} + ${#resource_requests[@]} )) ; then
	echo '"resources": {'
	if (( ${#resource_limits[@]} )) ; then
	    res_data+=("$(resource_json limits "${resource_limits[@]}")")
	fi
	if (( ${#resource_requests[@]} )) ; then
	    res_data+=("$(resource_json requests "${resource_requests[@]}")")
	fi
	(IFS=$',\n    '; echo "${res_data[*]}")
	echo "},"
    fi
}

function standard_labels_yaml() {
    local objtype=worker
    local OPTIND=0
    while getopts 't:' opt "$@" ; do
	case "$opt" in
	    t) objtype="$OPTARG" ;;
	    *)			 ;;
	esac
    done
    shift $((OPTIND-1))
    local workload=${1:-}
    local namespace=${2:-}
    local instance=${3:-}
    local logger=${4:+logger}
    # Different versions of bash expand quotes inside the ${logger:+...} differently.
    local true='"true"'
    cat <<EOF
labels:
  ${basename}-uuid: "$uuid"
  ${basename}-id: "$uuid"
  ${basename}: "true"
  clusterbusterbase: "true"
  ${basename}base: "true"
  ${objtype:+${basename}-${objtype}: "$uuid"}
EOF
    if [[ -n "$workload" ]] ; then
	cat <<EOF
  name: "${namespace}${workload:+-$workload}${instance:+-$instance}"
  app: "${namespace}${workload:+-$workload}${instance:+-$instance}"
  ${basename}-${workload}: "true"
${logger:+  ${basename}-logger: $true}
EOF
    fi
}

function bootstrap_command_yaml() {
    local command=$1
    cat <<EOF
command:
- bootstrap.sh
args:
- "$system_configmap_mount_dir/$command"
EOF
}

function volume_mounts_yaml() {
    local namespace=$1
    local deployment=${2:-1}
    local secrets=${3:-1}
    (( secrets + ${#emptydirs[@]} + ${#configmap_files[@]} + ${#volumes[@]} + has_system_configmap )) || return;
    local -i i
    echo volumeMounts:
    for i in $(seq 0 $((secrets - 1))) ; do
	local name="secret-${namespace}-${deployment}-$i"
	cat <<EOF
- name: $name
  mountPath: /etc/$name
  readOnly: true
EOF
    done
    if [[ -n "${emptydirs[*]:-}" ]] ; then
	local vol
	for vol in "${emptydirs[@]}" ; do
	    cat <<EOF
- name: ${vol##*/}
  mountPath: "$vol"
EOF
	done
    fi
    if [[ $namespace != "$sync_namespace" && -n "${configmap_files[*]:-}" ]] ; then
	cat <<EOF
- name: "configmap-${namespace}"
  mountPath: "${configmap_mount_dir}"
  readOnly: true
EOF
    fi
    if [[ -n "${volumes[*]:-}" ]] ; then
	local vol
	for vol in "${volumes[@]:-}" ; do
	    cat <<EOF
- name: "$vol"
  mountPath: "${volume_mount_paths[$vol]}"
EOF
	done
    fi
    if (( has_system_configmap )) ; then
	cat <<EOF
- name: "systemconfigmap-${namespace}"
  mountPath: "${system_configmap_mount_dir}"
  readOnly: true
EOF
    fi
}

function pbench_agent_yaml() {
    if (( pbench_controller_port > 0 )) ; then
	# This isn't ideal, as there still can be collisions.
	# Without a port server, this is certainly the most
	# expeditious.
	local ssh_port=0
	while (( ssh_port < 2023 )) ; do
	    ssh_port=$RANDOM
	done
	cat <<EOF
- name: "pbench-agent"
  image: "$container_image"
  ${image_pull_policy:+imagePullPolicy: $image_pull_policy}
  securityContext:
    privileged: false
    capabilities:
      add:
      - AUDIT_WRITE
  env:
  - name: VERBOSE
    value: "0"
  command:
  - "/usr/local/bin/run-pbench-agent-container"
  args:
  - "$pbench_controller_port"
  - "$pbench_controller_address"
#hostPID: true
serviceAccount: pbench
EOF
    fi
}

function privilege_yaml() {
    if (( pbench_controller_port > 0)) ; then
	cat <<EOF
openshift.io/scc: privileged
EOF
    fi
}

function namespace_yaml() {
    local namespace=$1
    (( use_namespaces )) && echo "namespace: \"$namespace\""
}

function annotations_yaml() {
    local desired_class=${2:-client}
    local annotation
    echo "annotations:"
    for annotation in "${pod_annotations[@]}" ; do
	if [[ $annotation =~ ^:([^:]*):(.*)$ ]] ; then
	    # shellcheck disable=SC2206
	    local -a classes=(${BASH_REMATCH[1]//,/ })
	    annotation=${BASH_REMATCH[2]:-}
	    local class
	    local -i class_found=0
	    for class in "${classes[@]}" ; do
		if [[ $desired_class = "$class" ]] ; then
		    class_found=1
		    break
		fi
	    done
	    if (( ! class_found )) ; then
		continue
	    fi
	fi
	echo "  $annotation"
    done
}

function standard_deployment_metadata_yaml() {
    local namespace=${1:-}
    local class=${2:-client}
    namespace_yaml "$namespace"
    privilege_yaml
}

function standard_pod_metadata_yaml() {
    local namespace=${1:-}
    local class=${2:-client}
    annotations_yaml "$class"
}

function standard_all_metadata_yaml() {
    standard_deployment_metadata_yaml "$@"
    standard_pod_metadata_yaml "$@"
}

function expand_volume() {
    local scoped_name=$1
    local namespace=${2:+-$2}
    local instance=${3:+-$3}
    scoped_name=${scoped_name//%N/$namespace}
    scoped_name=${scoped_name//%i/$instance}
    echo "$scoped_name"
}

function volumes_yaml() {
    local namespace=$1
    local instance=${2:-1}
    local secrets=${3:-1}
    (( secrets + ${#emptydirs[@]} + ${#configmap_files[@]} + ${#volumes[@]} + has_system_configmap )) || return;
    local -i i
    echo "volumes:"
    for i in $(seq 0 $((secrets - 1))) ; do
	local name="secret-${namespace}-${instance}-$i"
	cat<<EOF
- name: $name
  secret:
    secretName: $name
EOF
    done

    if [[ -n "${emptydirs[*]:-}" ]] ; then
	local vol
	for vol in "${emptydirs[@]}" ; do
	    cat <<EOF
- name: ${vol##*/}
  emptydDir: {}
EOF
	done
    fi
    if [[ $namespace != "$sync_namespace" && -n "${configmap_files[*]:-}" ]] ; then
	cat <<EOF
- name: "configmap-${namespace}"
  configMap:
    name: "configmap-${namespace}"
EOF
    fi
    if [[ -n "${volumes[*]:-}" ]] ; then
	local vol
	for vol in "${volumes[@]:-}" ; do
	    cat <<EOF
- name: "$vol"
EOF
	    if [[ -n "${volume_type_keys[$vol]}" ]] ; then
		cat <<EOF
  ${volume_types[$vol]}:
    ${volume_type_keys[$vol]}: "$(expand_volume "${volume_scoped_names[$vol]}" "$namespace" "$instance")"
EOF
	    else
		cat <<EOF
  ${volume_types[$vol]}: {}
EOF
	    fi
	done
    fi
    if (( has_system_configmap )) ; then
	cat <<EOF
- name: "systemconfigmap-${namespace}"
  configMap:
    name: "systemconfigmap-${namespace}"
EOF
    fi
}

function create_spec() {
    local affinity_yaml=
    local node_class=client
    local OPTIND=0
    local OPTARG
    local ns_yaml=${node_selector:+"nodeSelector:${nl}  $node_selector: \"\""}
    local runtime_class
    local use_pbench=1
    local opt
    while getopts "A:c:r:Pp" opt "$@" ; do
	case "$opt" in
	    A) affinity_yaml=$OPTARG ;;
	    c) node_class=$OPTARG    ;;
	    r) runtime_class=$OPTARG ;;
	    P) use_pbench=	     ;;
	    p) use_pbench=1	     ;;
	    *)                       ;;
	esac
    done
    shift $((OPTIND-1))
    (( $# >= 4 )) || fatal "Usage: create_spec <container_func> namespace instance secret_count args..."
    # Node specifiers override affinity
    if [[ -n "$node_class" && -n "${pin_nodes[$node_class]:-}" ]] ; then
	affinity_yaml="nodeSelector:${nl}  kubernetes.io/hostname: \"${pin_nodes[$node_class]}\""
    elif [[ -n "${pin_nodes[default]:-}" ]] ; then
	affinity_yaml="nodeSelector:${nl}  kubernetes.io/hostname: \"${pin_nodes[default]}\""
    fi
    if [[ -z "${runtime_class+x}" ]] ; then
	if [[ -n "$node_class" && -n "${runtime_classes[$node_class]:-}" ]] ; then
	    runtime_class=${runtime_classes[$node_class]}
	elif [[ -n "${runtime_classes[default]:-}" ]] ; then
	    runtime_class=${runtime_classes[default]}
	fi
    fi
    local create_container_function=$1; shift
    [[ -n "$(type -t "$create_container_function")" ]] || fatal "Cannot run $create_container_function: command not found"
    local namespace=$1
    local instance=$2
    local secret_count=$3
    cat <<EOF
spec:
  terminationGracePeriodSeconds: 1
$(indent 2 <<< "$affinity_yaml")
$(indent 2 tolerations_yaml ${tolerations+"${tolerations[@]}"})
${runtime_class:+$(indent 2 <<< "runtimeClassName: \"$runtime_class\"")}
${use_pbench:+$(indent 2 <<< "$(pbench_agent_yaml)")}
  containers:
$(indent 2 "$create_container_function" "$@")
$(indent 2 volumes_yaml "$namespace" "$instance" "$secret_count")
EOF
}

function create_pod_deployment() {
    local -i replica=0
    while (( replica++ < replicas )) ; do
	local name="${depname}-${replica}"
	create_object -n "$namespace" "$name" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: $(mkpodname "$name")
$(indent 2 standard_all_metadata_yaml "$namespace" client)
  selector:
    matchLabels:
      app: $name
$(indent 2 standard_labels_yaml "$workload" "$namespace" "${instance}-${replica}" 1)
$(create_spec ${affinity_yaml:+-A "$affinity_yaml"} "${workload}_create_containers_yaml" "$@" "$replica")
${security_context:+$(indent 2 <<< "$security_context")}
  restartPolicy: Never
EOF
    done
}

function create_replication_deployment() {
    local name="$depname"
    create_object -n "$namespace" "$name" <<EOF
apiVersion: apps/v1
kind: $deployment_type
metadata:
  name: $(mkpodname "$name")
$(indent 2 standard_deployment_metadata_yaml "$namespace" client)
$(indent 2 standard_labels_yaml "${workload}" "$namespace" "$instance" 1)
spec:
  replicas: $replicas
  selector:
    matchLabels:
      app: ${namespace}-${workload}-${instance}
  strategy:
    type: RollingUpdate
  template:
    metadata:
$(indent 6 standard_labels_yaml ${workload} "$namespace" "$instance" 1)
$(indent 6 standard_pod_metadata_yaml "$namespace" client)
$(indent 4 create_spec ${affinity_yaml:+-A "$affinity_yaml"} "${workload}_create_containers_yaml" "$@" "x")
${security_context:+$(indent 6 <<< "$security_context")}
EOF
}

function create_standard_deployment() {
    local OPTIND=0
    local OPTARG=
    local affinity_yaml=
    local workload="$requested_workload"
    local opt
    local security_context=
    local allow_replica_controller=1
    while getopts 'A:w:S:p' opt "$@" ; do
	case "$opt" in
	    A) affinity_yaml="$OPTARG"	  ;;
	    w) workload="$OPTARG"	  ;;
	    S) security_context="$OPTARG" ;;
	    p) allow_replica_controller=0 ;;
	    *)				  ;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    local instance=$2
    local -i replicas=$4
    local depname="${namespace}-${workload}-${instance}"
    case "${deployment_type,,}" in
	pod)
	    create_pod_deployment "$@"
	    ;;
	replicaset|deployment)
	    if ((allow_replica_controller)) ; then
		create_replication_deployment "$@"
	    else
		create_pod_deployment "$@"
	    fi
	    ;;
	*)
	    fatal "Unknown deployment type $deployment_type"
	    ;;
    esac
}
				       

################################################################
# Object YAML creation
################################################################

function _really_create_objects() {
    if [[ $doit -ne 0 && -n $accumulateddata ]] ; then
	__OC --validate=false apply -f - <<< "$accumulateddata" || {
	    echo "Create objects failed:" 1>&2
	    echo "$accumulateddata" 1>&2
	    killthemall "Failed to create objects"
	}
	accumulateddata=
	objs_item_count=0
    fi
}

function really_create_objects() {
    _really_create_objects "$@" | timestamp 1>&2
    accumulateddata=
    objs_item_count=0
}

function create_object() {
    local OPTIND=0
    local namespace=
    local objtype=${deployment_type^}
    local objname=
    local force=0
    while getopts 'n:t:f' opt "$@" ; do
	case "$opt" in
	    n) namespace=$OPTARG ;;
	    t) objtype=$OPTARG   ;;
	    f) force=1           ;;
	    *)                   ;;
	esac
    done
    shift $((OPTIND-1))
    objname=${1:-UNKNOWN}
    local data=
    local line=
    local -i data_found=0
    while IFS='' read -r 'line' ; do
	line=${line%% }
	if [[ $line =~ [^[:space:]] ]] ; then
	    data_found=1
	    [[ -n $data ]] || data="---$nl"
	    data+="$line$nl"
	fi
    done
    ((! data_found)) && return
    data+="$nl"
    if (( doit )) ; then
	if [[ -n "$artifactdir" ]] ; then
	    mkdir -p "$artifactdir/$objtype"
	    echo "$data" > "$artifactdir/${objtype}/${namespace}:$objname"
	fi
	accumulateddata+="$data"
	if (( force || (++objs_item_count >= objs_per_call) )) ; then
	    really_create_objects
	    (( !sleeptime )) || sleep "$sleeptime"
	fi
    else
	IFS= echo "$data"
    fi
}

function echo_if_desired() {
    if (( report_object_creation )) ; then
	cat
    else
	cat >/dev/null
    fi
}

function _create_object_from_file() {
    function _ns() {
	((use_namespaces)) && echo -n "-n $namespace"
    }
    local objtype=$1; shift
    local namespace=$1; shift
    local objname=$1; shift
    local ns=${use_namespaces:+-n "$namespace"}
    local ns1=${use_namespaces:+"$namespace"}
    local file
    for file in "$@" ; do
	[[ -r "$file" ]] || fatal "Can't read file $file!"
    done
    # shellcheck disable=SC2046
    _OC create "$objtype" $(_ns) "$objname" "${@/#/--from-file=}" || killthemall "Unable to create object $objtype/$namespace:$objname"
    # shellcheck disable=SC2046
    _OC label "$objtype" $(_ns) "$objname" "${basename}base=true" 'clusterbusterbase=true' "${basename}-uuid=${uuid}" "${basename}-id=$uuid" "${basename}=true" || killthemall "Unable to label $objtype/$namespace:$objname"
    if (( doit )) ; then
	if [[ -n "$artifactdir" ]] ; then
	    mkdir -p "$artifactdir/$objtype"
	    local outfile="$artifactdir/${objtype}/${ns1}:${objname}"
	    rm -f "$outfile"
	    for file in "$@" ; do
		echo '---' >> "$outfile"
		cat "$file" >> "$outfile"
	    done
	fi
	total_objects_created=$((total_objects_created+1))
	if [[ -z ${objects_created[$objtype]:-} ]] ; then
	    objects_created[$objtype]=1
	else
	    objects_created[$objtype]=$((${objects_created[$objtype]}+1))
	fi
	(( !sleeptime )) || sleep "$sleeptime"
    fi
}

function create_object_from_file() {
    _create_object_from_file "$@" |echo_if_desired |timestamp 1>&2
}

function create_namespace() {
    local OPTIND=0
    local force=
    local opt=
    while getopts 'f' opt "$@" ; do
	case "$opt" in
	    f) force=-f ;;
	    *)          ;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    if ((use_namespaces)) ; then
	create_object $force -t namespace "$namespace" <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: "${namespace}"
$(indent 2 standard_labels_yaml -t '')
EOF
    fi
}

function create_secrets() {
    local namespace=$1
    local deps_per_namespace=${2:-1}
    local secrets=${3:-1}
    local -i i
    local -i j
    (( secrets )) || return;
    for i in $(seq "$first_deployment" $((deps_per_namespace + first_deployment - 1))) ; do
	for j in $(seq 0 $((secrets - 1))) ; do
	    local secname="secret${namespace:+-$namespace}-${i}-${j}"
	    create_object -t secret -n "$namespace" "$secname" <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: "$secname"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml -t '')
data:
  key1: "$(base64 <<< "${namespace}X${i}Y${j}Z1")"
  key2: "$(base64 <<< "${namespace}X${i}Y${j}Z2")"
type: Opaque
EOF
	done
    done
}

function check_configmaps() {
    local errors=0
    for mapfile in "$@" ; do
	if [[ ! -r "$mapfile" ]] ; then
	    echo "Can't find configmap file $mapfile"
	    errors=$((errors+1))
	fi
    done
    if (( errors )) ; then
	exit 1
    fi
}

function create_configmaps() {
    local namespace=$1; shift
    local mapname=$1; shift
    (( $# )) || return;
    create_object_from_file configmap "$namespace" "${mapname}-${namespace}" "$@"
}

function _create_service_ports() {
    local basename=$1
    shift
    local i
    local p
    for i in "$@" ; do
	for p in TCP UDP ; do
	    cat <<EOF
- name: ${basename}-${i}-${p,,}
  protocol: $p
  port: $i
  targetPort: $i
EOF
	done
    done
}

function create_service() {
    local OPTIND=0
    local -i force_headless_services=-1
    local headless=
    while getopts 'hH' opt ; do
	case "$opt" in
	    h) force_headless_services=1 ;;
	    H) force_headless_services=0 ;;
	    *) ;;
	esac
    done
    shift $((OPTIND-1))
    ((force_headless_services < 0)) && force_headless_services=$headless_services
    ((force_headless_services)) && headless='clusterIP: "None"'
    local namespace=$1
    local deployment=$2
    shift 2
    create_object -t Service -n "$namespace" "svc-${deployment}" <<EOF
apiVersion: v1
kind: Service
metadata:
  name: "svc-${deployment}"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml -t '')
spec:
$(indent 2 <<< "$headless")
  ports:
$(indent 4 _create_service_ports "svc-${deployment}" "$@")
  selector:
    name: "${deployment}"
EOF
}

function create_external_service() {
    local namespace=$1
    local deployment=$2
    local portnum=$3
    local external_name=$4
    create_object -t Service -n "$namespace" "svc-${deployment}" <<EOF
apiVersion: v1
kind: Service
metadata:
  name: "svc-${deployment}"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml -t '')
spec:
  ports:
  - port: $portnum
  type: ExternalName
  externalName: $external_name
EOF
}

function create_container_sync() {
    local namespace=$1
    local sync_count=${4:-1}
    local expected_clients=$5
    cat <<EOF
- name: ${namespace}
  image_pull_policy: $image_pull_policy
  image: "$container_image"
  ports:
  - containerPort: $port
$(indent 2 bootstrap_command_yaml sync.pl)
  - "-f"
  - "$sync_flag_file"
  - "-t"
  - "$controller_timestamp_file"
  - "-d"
  - "$predelay"
  - "-D"
  - "$postdelay"
  - "$sync_port"
  - "$expected_clients"
  - "$sync_count"
$(indent 2 volume_mounts_yaml "$namespace" 0 0)
EOF
}

function create_sync_deployment() {
    local namespace=$1; shift
    local sync_count=$1; shift
    create_object -t Pod -n "$namespace" "${namespace}-sync" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: $(mkpodname "${namespace}")
$(indent 2 standard_all_metadata_yaml "$namespace" sync)
  selector:
    matchLabels:
      app: ${namespace}
$(indent 2 standard_labels_yaml -t 'sync')
    name: ${namespace}
    app: ${namespace}
    k8s-app: ${namespace}
    ${basename}-sync: "$uuid"
$(create_spec -P -c sync -r '' create_container_sync "$namespace" 0 0 "$sync_count" "$@")
  restartPolicy: Never
EOF
}

function create_container_drop_cache() {
    local namespace=$1
    cat <<EOF
- name: ${namespace}
  image_pull_policy: $image_pull_policy
  image: "$container_image"
  securityContext:
    privileged: true
  ports:
  - containerPort: $drop_cache_port
$(indent 2 bootstrap_command_yaml drop_cache.pl)
  - "$drop_cache_port"
$(indent 2 volume_mounts_yaml "$namespace" 0 0)
  - mountPath: "/proc-sys-vm"
    name: "proc-sys-vm"
EOF
}

function _create_drop_cache_deployment() {
    local namespace=$1
    local workload=$2
    local instance=$3
    create_object -t Pod -n "$namespace" "${namespace}-${workload}-${instance}-dc" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: $(mkpodname "${namespace}-${workload}-${instance}-dc")
$(indent 2 standard_all_metadata_yaml "$namespace" drop_cache)
  selector:
    matchLabels:
      app: ${namespace}-${workload}-${instance}-dc
$(indent 2 standard_labels_yaml -t 'dc')
    name: ${namespace}-${workload}-${instance}-dc
    app: ${namespace}-${workload}-${instance}-dc
    k8s-app: ${namespace}-${workload}-${instance}-dc
    ${basename}-dc: "$uuid"
  securityContext:
    privileged: true
$(create_spec -P -c drop-cache -r '' create_container_drop_cache "$namespace" "$instance" 0)
  - name: "proc-sys-vm"
    hostPath:
      path: "/proc/sys/vm"
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - "${namespace}-${workload}-${instance}"
        topologyKey: kubernetes.io/hostname
  restartPolicy: Never
EOF
}

function create_drop_cache_deployment() {
    local namespace=$1
    local workload=$2
    local instance=$3
    local -i replicas=$4
    local -i replica=0
    while ((replica++ < replicas)) ; do
	local name="${namespace}-${workload}-${instance}-${replica}-dc"
	create_service -H "$namespace" "$name" "$drop_cache_port"
	_create_drop_cache_deployment "$namespace" "$workload" "${instance}-${replica}"
    done
}

################################################################
# Generic Object creation
################################################################

function create_objects_n() {
    local objtype=$1; shift
    local parallel=$1; shift
    local objs_per_call=$1; shift
    local rotor=$1; shift
    local sleeptime=$1; shift
    while (( rotor < namespaces )) ; do
	"create_${objtype}" "${namespaces_to_create[$rotor]}" "$@"
	if (( sleeptime )) ; then sleep "$sleeptime"; fi
	rotor=$((rotor + parallel))
    done
    really_create_objects
}

function allocate_namespaces() {
    local -i ns_count=0
    local -i ns_idx=0
    if (( scale_ns )) ; then
        while read -r ns ; do
	    if [[ -n "$ns" ]] ; then
		namespaces_in_use[${ns#namespace/}]=1
	    fi
        done < <(__OC get ns -l "$basename" --no-headers -o name 2>/dev/null)
    fi
    while (( ns_count++ < namespaces )) ; do
        while [[ -n "${namespaces_in_use[${basename}-$ns_idx]:-}" ]] ; do
            ns_idx=$((ns_idx+1))
        done
        namespaces_to_create+=("${basename}-$((ns_idx++))")
    done
    sync_namespace="${basename}-sync"
}

function find_first_deployment() {
    if (( scale_deployments )) ; then
	local ns
	local deployment
	local stuff
	# shellcheck disable=SC2034
        while read -r ns deployment stuff ; do
	    if [[ -n "$deployment" ]] ; then
		deployment=${deployment#${ns}-}
		deployment=${deployment%-*}
		echo __OC get deployments -l "$basename" -A --no-headers 1>&2
		if (( deployment + 1 > first_deployment )) ; then
		    first_deployment=$((deployment + 1))
		fi
	    fi
        done <<< "$(__OC get deployments -l "$basename" -A --no-headers 2>/dev/null)"
    fi
}

function create_all_namespaces() {
    local i
    for i in $(seq 0 "$((parallel_namespaces - 1))") ; do
	create_objects_n namespace "$parallel_namespaces" "$objs_per_call_namespaces" "$i" "$sleep_between_namespaces" &
    done
    wait || killthemall "Unable to create namespaces"
}

function create_all_secrets() {
    local i
    for i in $(seq 0 "$((parallel_secrets - 1))") ; do
	create_objects_n secrets "$parallel_secrets" "$objs_per_call_secrets" "$i" "$sleep_between_secrets" "$deps_per_namespace" "$secrets" &
    done
    wait || killthemall "Unable to create secrets"
}

function create_system_configmap() {
    if supports_api -w "$requested_workload" list_configmaps ; then
	local -a systemfiles
	readarray -t systemfiles < <(list_configmaps)
	if [[ -n "$sync_namespace" ]] ; then
	    create_configmaps "$sync_namespace" "systemconfigmap" "${systemfiles[@]}"
	fi
	for i in $(seq 0 "$((parallel_configmaps - 1))") ; do
	    create_objects_n configmaps "$parallel_configmaps" "$objs_per_call_configmaps" "$i" "$sleep_between_configmaps" "systemconfigmap" "${systemfiles[@]}"&
	done
	wait || killthemall "Unable to create system configmap"
    fi
}

function create_all_configmaps() {
    create_system_configmap
    (( ${#configmap_files[@]} )) || return;
    local i
    for i in $(seq 0 "$((parallel_configmaps - 1))") ; do
	create_objects_n configmaps "$parallel_configmaps" "$objs_per_call_configmaps" "$i" "$sleep_between_configmaps" "configmap" "${configmap_files[@]}"&
    done
    wait || killthemall "Unable to create configmaps"
}

function drop_host_caches() {
    ((doit && (drop_node_cache || drop_all_node_cache) )) || return 0
    local -a nodes_to_drop
    if [[ -n "${pin_nodes[*]}" && $drop_all_node_cache -eq 0 ]] ; then
	local n
	local -A tmp_nodes_to_drop
	for n in "${pin_nodes[@]}" ; do
	    tmp_nodes_to_drop[$n]=1
	done
	nodes_to_drop=("${!tmp_nodes_to_drop[@]}")
    else
	readarray -t nodes_to_drop <<< "$(oc get -l node-role.kubernetes.io/worker node -ojsonpath="{range .items[*]}{.metadata.name}{'\n'}{end}")"
    fi
    for n in "${nodes_to_drop[@]}" ; do
	((report_object_creation)) && echo "Dropping buffer cache: $n"
	_OC debug --no-tty=true "node/$n" -- chroot /host sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches' 2>/dev/null &
    done
    wait
}

function create_all_deployments() {
    local i
    for i in $(seq 0 "$((parallel_deployments - 1))") ; do
	create_objects_n deployment "$parallel_deployments" "$objs_per_call_deployments" "$i" "$sleep_between_deployments" "$deps_per_namespace" "$secrets" "$replicas" "$containers_per_pod" &
    done
    wait || killthemall "Unable to create deployments"
}

function create_all_objects() {
    local found_objtype
    local objtype=$1
    shift
    while IFS= read -r 'line' ; do
	if [[ $line = *'__KUBEFAIL__ '* ]] ; then
	    echo "${line#__KUBEFAIL__ }" 1>&2
	    return 1
	elif [[ $line =~ ^[[:digit:]]{4}(-[[:digit:]]{2}){2}T([[:digit:]]{2}:){2}[[:digit:]]{2}\.[[:digit:]]{6}\ +(([-a-z0-9]+)(\.[-a-z0-9]*)?)/([-a-z0-9]+)\ +created$ ]] ; then
	    if (( report_object_creation )) ; then
		echo "$line" 1>&2
	    fi
	    total_objects_created=$((total_objects_created+1))
	    found_objtype=${BASH_REMATCH[4]}
	    if [[ -z ${objects_created[$found_objtype]:-} ]] ; then
		objects_created[$found_objtype]=1
	    else
		objects_created[$found_objtype]=$((${objects_created[$found_objtype]:-}+1))
	    fi
	elif [[ -n "$line" ]] ; then
	    if ((doit)) ; then
		(IFS=''; echo "$line" 1>&2)
	    else
		(IFS=''; echo "$line")
	    fi
	fi
    done < <("create_all_${objtype}" "$@" 2>&1)
}

function _do_cleanup_1() {
    local ltimeout="${force_cleanup_timeout:+--timeout=${timeout}s}"
    if [[ ${1:-} = '-f' ]] ; then
	shift
	# It appears that allowing processes to write to stderr inside
	# a trap results in the subprocess not doing everything
	# (in this case, oc delete doesn't complete).
	exec >/dev/null
	exec 2>/dev/null
	# shellcheck disable=SC2086
	"$OC" delete "${1:-ns}" -l "$basename" $ltimeout
    elif ((report_object_creation)) ; then
	# shellcheck disable=SC2086
	__OC delete "${1:-ns}" -l "$basename" $ltimeout 1>&2
    else
	# shellcheck disable=SC2086
	__OC delete "${1:-ns}" -l "$basename" $ltimeout >/dev/null 2>&1
    fi
}

function _do_cleanup() {
    if ! _do_cleanup_1 "$@" ; then
	if [[ -n "$force_cleanup_timeout" ]] ; then
	    warn "*** Cleanup failed, doing a force delete!"
	    __OC delete deployment,replicaset,pod,configmap,secret,service -A  -l "$basename" --force --grace-period=0 1>&2
	    if ((use_namespaces)) ; then
		__OC delete namespace -l "$basename" --force --grace-period=0 1>&2
	    fi
	fi
    fi
}

# shellcheck disable=SC2120
function do_cleanup() {
    local force=
    local OPTIND=0
    local -i precleanup=0
    local opt
    while getopts 'fp' opt "$@" ; do
	case "$opt" in
	    f) force=-f      ;;
	    p) precleanup=1 ;;
	    *)              ;;
	esac
    done
    shift "$OPTIND"
    local objects_to_clean
    local objtype
    (( doit )) || return 0
    if ((use_namespaces)) ; then
	objects_to_clean=namespace
    else
	objects_to_clean="${deployment_type},configmap,secret,service"
    fi
    if [[ -n "${injected_errors[precleanup]:-}" && $precleanup -gt 0 ]] ; then
	warn "*** Injecting precleanup error ${inject_errors[precleanup]:-}"
	sleep "${injected_errors[precleanup]:-}"
	killthemall "Cleanup timed out!"
    fi
    if [[ -n "${injected_errors[cleanup]:-}" && $precleanup -eq 0 ]] ; then
	warn "*** Injecting cleanup error ${inject_errors[cleanup]:-}"
	sleep "${injected_errors[cleanup]:-}"
	killthemall "Cleanup timed out!"
    fi
    _do_cleanup $force "$objects_to_clean" || killthemall "Cleanup timed out!"
}

################################################################
# Main work loop
################################################################

function create_secrets_top_level() {
    # If previous secrets weren't all created, this will yield
    # the wrong result.
    if (( doit && wait_for_secrets )) ; then
	local -i i
	local -i j
	local -i k
	local ns
	local nsd
	for i in $(seq 0 $((namespaces - 1))) ; do
	    ns="secret/secret-${basename}-${namespaces_to_create[$i]##*-}"
	    for j in $(seq "$first_deployment" $((deps_per_namespace + first_deployment - 1))) ; do
		nsd="${ns}-$j"
		for k in $(seq 0 $((secrets - 1))) ; do
		    expected_secrets["${nsd}-$k"]=1
		done
	    done
	done
    fi
    create_all_objects secrets
    if (( doit )) ; then
	local secname=""
	while (( ${#expected_secrets[@]} )) ; do
	    echo "Expect ${#expected_secrets[@]}" 1>&2
	    while read -r secname ; do
		[[ -n "${secname:-}" ]] && unset "expected_secrets[$secname]"
	    done < <(__OC get secret -oname --no-headers -l "$basename" -A)
	    if (( ${#expected_secrets[@]} )) ; then
		echo "Still waiting for ${#expected_secrets[@]} secrets to be created."
		sleep 10
	    else
		break
	    fi
	done
    fi
}

function do_logging() {
    ((report)) || return 0
    local -i logs_expected=0
    logs_expected="$(calculate_logs_required "$namespaces" "$deps_per_namespace" "$replicas" "$containers_per_pod")"
    ((logs_expected > 0)) || return 0
    trap 'wait; return 1' TERM

    if [[ $log_strategy = poll ]] ; then
	local -a sync_pods=()
	local namespace
	local pod
	local ignore
	while read -r namespace pod ignore ; do
	    if [[ -n "$namespace" && -n "$pod" ]] ; then
		sync_pods+=("-n $namespace $pod")
	    fi
	done <<< "$(oc get pod -A --no-headers -l "${basename}-sync=${uuid}")"
	get_logs "${sync_pods[@]}" &
    else
	log_host=$(hostname -f)
	log_port=$("${__libdir__}/find_free_port")
	(( log_port )) || killthemall "Can't get port to listen for logs on!"
	get_logs -p "$log_port" -c "$logs_expected" &
    fi
    local -i get_logs_pid=$!
    if ((timeout > 0)) ; then
	local -i itimeout=$timeout
	local -i finis=0
	while (( timeout < 0 || timeout-- )) ; do
	    # CreateContainerError is not necessarily fatal
	    if __OC get pods -A -l "$basename" |grep -q -E -e '([^r]Error|Evicted|Crash)' ; then
		echo "Run failed:" 1>&2
		__OC get pods -A -l "$basename" |grep -E -e '([^r]Error|Evicted|Crash)' | {
		    local line
		    local ns
		    local pod
		    local rest
		    while read -r line ; do
			read -r ns pod rest <<< "$line"
			echo "$line" 1>&2
			oc logs -n "$ns" "$pod" 1>&2
			echo 1>&2
		    done
		}
		finis=-1
		break
	    fi
	    # Commands in a pipeline are each run in subshells.  Background jobs are child
	    # of the parent, not any children, so running "jobs -l" inside the pipeline
	    # will not reap any exited subjobs.  The child still has the job table, so
	    # it knows what subjobs exist, but the "jobs -l" must be run at top level
	    # to actually reap them.
	    jobs -l >/dev/null
	    if jobs -l |grep -q . ; then
		sleep 1
	    else
		finis=1
		break
	    fi
	done
	if (( finis <= 0 )) ; then
	    if ((finis < 0)) ; then
		killthemall "Run failed"
	    else
		killthemall "Run did not terminate in $itimeout seconds"
	    fi
	fi
    fi
    if (( get_logs_pid > 1 )) ; then
	wait "$get_logs_pid" 2>/dev/null
	status=$?
    fi
}

function report_object_creation() {
    (( report_object_creation )) || return 0
    local objtype
    while read -r objtype ; do
	[[ -n "${objtype:-}" ]] && printf "Created %${#total_objects_created}d %s%s\n" "${objects_created[$objtype]}" "$objtype" "${plurals[$((${objects_created[$objtype]} == 1))]}" 1>&2
    done < <( (IFS=$'\n'; echo "${!objects_created[*]}") | sort)
    printf "Created %d object%s total\n" "$total_objects_created" "${plurals[$((total_objects_created == 1))]}" 1>&2
}

function expand_string() {
    local string=$1
    shift
    local -A _____dict_____=()
    local -a arg
    for arg in "$@" ; do
	local noptname1 optname optvalue
	read -r noptname optname optvalue <<< "$(parse_option "$arg")"
	_____dict_____["$optname"]="$optvalue"
    done
    while [[ $string =~ (.*)(%\{([[:alnum:]_]+(\[[[:alnum:]]+\])?)(:-([^\}]*))?\})(.*) ]] ; do
	local prefix=${BASH_REMATCH[1]:-}
	local var=${BASH_REMATCH[3]:-}
	local replacement=${BASH_REMATCH[6]:-UNKNOWN$var}
	local suffix=${BASH_REMATCH[7]:-}
	if [[ -n "${_____dict_____[$var]+isset}" ]] ; then
	    string="${prefix}${_____dict_____[$var]}${suffix}"
	else
	    string="${prefix}${!var:-$replacement}${suffix}"
	fi
    done
    echo "$string"
}

function run_clusterbuster_2() {
    local -A expected_secrets=()
    local -i status=0
    if (( precleanup && doit )) ; then
	do_cleanup -p 1>&2 || return 1
    fi
    if ((cleanup)) ; then
	trap 'killthemall "Cleaning up"; wait; remove_tmpdir; doit=1; do_cleanup -f; exit 1' TERM INT HUP
    else
	trap 'killthemall "Not cleaning up"; wait; remove_tmpdir; exit 1' TERM HUP INT
    fi
    if [[ $doit -gt 0 ]] ; then
	if [[ -n "${artifactdir:-}" && $take_prometheus_snapshot -gt 0 ]] ; then
	    start_prometheus_snapshot || return 1
	else
	    set_start_timestamps || return 1
	fi
	if [[ -n "${artifactdir:-}" ]] ; then
	    artifactdir=${artifactdir//%s/$(readable_timestamp "$prometheus_starting_timestamp")}
	    artifactdir=${artifactdir//%w/$requested_workload}
	    artifactdir=${artifactdir//%n/$job_name}
	    artifactdir="$(expand_string "$artifactdir")"
	    mkdir -p "$artifactdir" || fatal "Can't create log directory $artifactdir"
	    echo "${saved_argv[@]@Q}" > "$artifactdir/commandline"
	    exec 2> >(tee "$artifactdir/stderr.log" >&2)
	fi
    fi
    allocate_namespaces || return 1
    find_first_deployment || return 1
    if [[ -n "$sync_namespace" ]] ; then
	create_namespace -f "$sync_namespace" || return 1
    fi
    create_all_objects namespaces || return 1
    if get_sync -q ; then
	if (( use_namespaces )) ; then
	    global_sync_service="svc-${sync_namespace}.${sync_namespace}.svc.cluster.local"
	else
	    global_sync_service="svc-${basename}-0-sync"
	fi
    fi

    (( ${#configmap_files[@]} )) && check_configmaps "${configmap_files[@]}"
    create_all_objects configmaps || return 1
    supports_api -w "$requested_workload" list_configmaps && has_system_configmap=1

    create_secrets_top_level || return 1

    basetime=$(date +%s.%N)
    (drop_host_caches)
    create_all_objects deployments || return 1
    (( doit )) || return 0

    do_logging || return 1
    if ((! status)) ; then
	report_object_creation
    fi
    return 0
}

function run_clusterbuster_1() {
    local -i status=0
    run_clusterbuster_2
    status=$?
    if ((cleanup && doit)) ; then
	do_cleanup || {
	    if ((status == 0)) ; then
		status=1
	    fi
	}
    fi

    if ((status)) ; then
	fatal "Clusterbuster failed!"
    else
	if [[ $take_prometheus_snapshot -gt 0 && -n "$artifactdir" ]] ; then
	    retrieve_prometheus_snapshot "$artifactdir"
	fi
    fi
    return $status
}

################################################################
# Top level
################################################################

function remove_tmpdir() {
    [[ -n "${cb_tempdir:-}" && -d "$cb_tempdir" && $cb_tempdir = "/tmp/cbtmp_"* ]] && rm -rf "$cb_tempdir" && unset cb_tempdir
}

trap "remove_tmpdir; exit" EXIT
trap "remove_tmpdir; exit 1" TERM INT HUP
cb_tempdir=$(umask 77; mktemp -d -p /tmp -t cbtmp_XXXXXXXX) || fatal "Can't create temporary directory"

load_workloads "$__workloaddir__"

while getopts ":B:Eef:Hhno:P:Qqv-:" opt ; do
    case "$opt" in
	B) process_option "basename=$OPTARG"	 ;;
	E) process_option "exit_at_end=0"	 ;;
	e) process_option "exit_at_end=1"	 ;;
	f) process_job_file "$OPTARG"		 ;;
	h) help					 ;;
	H) help_extended			 ;;
	n) process_option "doit=0"		 ;;
	o) process_option "reportformat=$OPTARG" ;;
	P) process_option "workload=$OPTARG"     ;;
	Q) process_option "reportobjectcreate=0" ;;
	q) process_option "verbose=0"		 ;;
	v) process_option "verbose=1"		 ;;
	-) process_option "$OPTARG"		 ;;
	*) help "$OPTARG"			 ;;
    esac
done

if [[ -z "$requested_workload" ]] ; then
    help "Workload must be specified with -P"
fi

iworkload=$requested_workload
requested_workload=$(get_workload "$requested_workload") || help_extended "(Unknown workload '$iworkload')"

call_api -w "$requested_workload" -s "process_options" "${unknown_opts[@]}" || help "${unknown_opt_names[@]}"

if supports_api -w "$requested_workload" list_user_configmaps ; then
    readarray -t userfiles < <(list_user_configmaps)
    configmap_files+=("${userfiles[@]}")
fi

if (( namespaces <= 0 )) ; then
    namespaces=1
    use_namespaces=0
fi
(( parallel_configmaps )) || parallel_configmaps=$parallel
(( parallel_secrets )) || parallel_secrets=$parallel
(( parallel_namespaces )) || parallel_namespaces=$parallel
(( parallel_deployments )) || parallel_deployments=$parallel

(( !objs_per_call_configmaps )) && objs_per_call_configmaps=$objs_per_call
(( !objs_per_call_secrets )) && objs_per_call_secrets=$objs_per_call
(( !objs_per_call_namespaces )) && objs_per_call_namespaces=$objs_per_call
(( !objs_per_call_deployments )) && objs_per_call_deployments=$objs_per_call

[[ -n "$report_format" ]] && report=1

if [[ -n "$metrics_file" && ! -r "$metrics_file" ]] ; then
    fatal "Cannot read metrics file $metrics_file"
fi

case "${deployment_type,,}" in
    # If we're using deployments or replicasets, we do not want
    # to exit when workers complete, or the controller will
    # immediately try to re-create them.
    replicaset|rs)         deployment_type=ReplicaSet; exit_at_end=0 ;;
    deployment|dep|deploy) deployment_type=Deployment; exit_at_end=0 ;;
    pod) deployment_type=pod		   		  ;;
    *)
	echo "--deployment_type must be pod, deployment, or replicaset"
	help
esac

if [[ -z $OC && $doit -gt 0 ]] ; then
    fatal "Cannot find oc or kubectl command, exiting!"
fi

if [[ -z "$pbench_controller_address" && $pbench_controller_port -ne 0 ]] ; then
    fatal "Both pbench_controller_address and pbench_controller_port must be specified"
fi

if [[ -n "$pbench_controller_address" && $pbench_controller_port -eq 0 ]] ; then
    fatal "Both pbench_controller_address and pbench_controller_port must be specified"
fi

log_strategy=${log_strategy,,}
if [[ $log_strategy != poll && $log_strategy != listen ]] ; then
    fatal "Unknown --log_strategy (must be 'poll' or 'listen')"
fi

resource_validation_failed=0
validate_resource resource_request "${resource_requests[@]}" || resource_validation_failed=1
validate_resource resource_limit "${resource_limits[@]}" || resource_validation_failed=1
if ((resource_validation_failed)) ; then
    exit 1
fi

for i in $(seq 0 $((emptydir_volumes-1)) ) ; do
    emptydirs+=("/mnt/volume-$i")
done

job_name=${job_name:-${requested_workload:-}}

((virtiofsd_direct)) && virtiofsd_args+=('"-o"' '"allow_direct_io"')
((virtiofsd_writeback)) && virtiofsd_args+=('"-o"' '"writeback"')
((virtiofsd_threadpoolsize)) && virtiofsd_args+=("\"--thread-pool-size=$virtiofsd_threadpoolsize\"")
if [[ -n "${virtiofsd_args[*]:-}" ]] ; then
    pod_annotations+=("io.katacontainers.config.hypervisor.virtio_fs_extra_args: '[$(IFS=,; echo "${virtiofsd_args[*]}")]'")
fi

shift $((OPTIND - 1))
if [[ -n ${1:-} ]] ; then
    basename="$1"
    shift
fi

[[ -z "$*" ]] || warn "Warning: extraneous arguments $* after basename will be ignored!"

run_clusterbuster_1
