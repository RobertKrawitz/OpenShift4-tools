#!/bin/bash

# Copyright 2019-2022 Robert Krawitz/Red Hat
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Find our helpers
function finddir() {
    local path_to_file
    path_to_file=$(readlink -f "$0")
    if [[ -z $path_to_file ]] ; then
	return 1
    elif [[ -d $path_to_file ]] ; then
	echo "$path_to_file/"
    elif [[ -e $path_to_file ]] ; then
	echo "${path_to_file%/*}/"
    else
	return 1
    fi
    return 0
}

declare __realsc__=
declare __topsc__
if [[ -z ${__topsc__:-} ]] ; then
    export __topsc__="${0##*/}"
    # shellcheck disable=SC2155
    export __topdir__="$(finddir "$0")"
    [[ -z $__topdir__ ]] && fatal "Can't find directory for $0"
fi

function clean_startup() {
    [[ -f $__realsc__ ]] && rm -f "$__realsc__"
}

# This allows us to edit the script while another instance is running
# since this script sticks around until the user exits the spawned shell.
# It's fine for the running script to be removed, since the shell still
# has its open file descriptor.
if [[ $# = 0 || $1 != "--DoIt=$0" ]] ; then
    tmpsc=$(mktemp -t "${__topsc__}".XXXXXXXXXX)
    [[ -z $tmpsc || ! -f $tmpsc || -L $tmpsc ]] && fatal "Can't create temporary script file"
    trap clean_startup EXIT SIGHUP SIGINT SIGQUIT SIGTERM
    PATH+=${PATH:+:}$__topdir__
    cat "$0" > "$tmpsc"
    chmod +x "$tmpsc"
    exec "$tmpsc" "--DoIt=$tmpsc" "$@"
else
    # Needed for stack traceback to get function arguments
    shopt -s extdebug
    __realsc__=${1#--DoIt=}
    clean_startup
    export -n __topsc__ __topdir__
    shift
fi

# Unfortunate that there's no way to tell shellcheck to always source
# specific files.
# shellcheck disable=SC2034

# See the bash change log, differences between 4.4-beta2 and 4.4-rc2:
# a.  Using ${a[@]} or ${a[*]} with an array without any assigned elements when
#     the nounset option is enabled no longer throws an unbound variable error.
if (( BASH_VERSINFO[0] >= 5 || (BASH_VERSINFO[0] == 4 && BASH_VERSINFO[1] >= 4) )) ; then
    set -u
else
    cat 1>&2 <<EOF
Warning: bash version at least 4.4 is recommended for using ${__topsc__##*/}.
Actual version is ${BASH_VERSION}
EOF
fi

declare ___arg
for ___arg in "$@" ; do
    if [[ "${___arg:-}" = '--force-abort'* ]] ; then
	echo "*** WARNING: will abort on any shell error!" 1>&2
	set -e
	set -o errtrace
	#trap 'killthemall "Caught error, aborting!"' ERR
	break
    fi
done

declare -i namespaces=1
declare -i use_namespaces=1
declare -i deps_per_namespace=1
declare -i remove_namespaces=-1
declare -i create_namespaces_only=0
declare -i secrets=0
declare -i replicas=1
declare -i parallel=1
declare -i first_deployment=0
declare -i sleep_between_secrets=0
declare -i sleep_between_configmaps=0
declare -i sleep_between_namespaces=0
declare -i sleep_between_deployments=0
declare -i parallel_secrets=0
declare -i parallel_configmaps=0
declare -i parallel_namespaces=0
declare -i parallel_deployments=0
declare -i parallel_log_retrieval=50
declare -i retrieve_successful_logs=0
declare -i objs_per_call=1
declare -i objs_per_call_secrets=0
declare -i objs_per_call_configmaps=0
declare -i objs_per_call_namespaces=0
declare -i objs_per_call_deployments=0
declare -i containers_per_pod=1
declare -i sleeptime=0
declare -i doit=1
declare -i objs_item_count=0
declare -i port=7777
declare -i sync_port=7778
declare -i drop_cache_port=7779
declare -i sync_watchdog_port_num=7780
declare -i sync_watchdog_timeout=0
declare -i sync_ns_port=7753
declare -i sync_in_first_namespace=0
declare sync_host=
declare -i affinity=0
declare -i sync_affinity=0
declare -A pin_nodes=()
declare -A runtime_classes=()
declare -A net_interfaces=()
declare -r default_net_interface_pod=net1
declare -r default_net_interface_vm=eth1
declare runtime_class=
declare scheduler=
declare -i verbose=0
declare -i wait_for_secrets=1
declare -i bytes_transfer=0
declare -i bytes_transfer_max=0
# shellcheck disable=SC2034
declare -i default_bytes_transfer=1000000000
declare -i workload_run_time=0
declare -i workload_run_time_max=0
declare -i exit_at_end=1
declare -i report_object_creation=1
declare -A objects_created=()
declare baseoffset=0
declare -i metrics_epoch=0
declare requested_workload=
declare basename=${CLUSTERBUSTER_DEFAULT_BASENAME:-clusterbuster}
declare deployment_type=pod
declare basetime
declare opt
declare -r nl=$'\n'
declare -a resource_requests=()
declare -a resource_limits=()
declare -A namespaces_in_use=()
declare -a namespaces_to_create=()
declare sync_namespace=
declare -i scale_ns=0
declare -i scale_deployments=1
declare -i sync_start=1
declare -a volumes=()
declare -A mount_volume_map=()
declare common_workdir=/var/opt/clusterbuster
declare -i report=0
declare report_format=summary
declare -i precleanup=1
declare -i cleanup=0
declare -i cleanup_always=0
declare -i timeout=0
declare pathdir=${__topdir__%/*}
declare -a unknown_opts=()
declare -a unknown_opt_names=()
declare -i total_objects_created=0
# <name, filename with contents>
declare -a configmap_files=()
declare -a tolerations=()
declare user_configmap_mount_dir=/etc/clusterbuster
declare system_configmap_mount_dir=/var/lib/clusterbuster
declare -i has_system_configmap=1
declare -i has_user_configmap=1
declare node_selector='node-role.kubernetes.io/worker'
declare -i processes_per_pod=1
declare -i take_prometheus_snapshot=0
declare first_start_timestamp=
declare prometheus_starting_timestamp=
declare prometheus_exact_starting_timestamp=
declare second_start_timestamp=
declare prometheus_ending_timestamp=
declare -i target_data_rate=0
declare job_name=
declare global_sync_service=
declare -i predelay=0
declare -i postdelay=0
declare -r default_metrics_file="metrics-default.yaml"
declare metrics_file=default
declare -i drop_node_cache=0
declare -i drop_all_node_cache=0
declare -i headless_services=1
declare -i virtiofsd_writeback=0
declare -i virtiofsd_direct=1
declare -i virtiofsd_threadpoolsize=0
declare -a virtiofsd_args=()
declare -i liveness_probe_frequency=0
declare -i liveness_probe_sleep_time=0
declare -i metrics_support=-1
declare -i default_pod_start_timeout=60
declare -i default_vm_start_timeout=180
declare -i pod_start_timeout=-1
declare pod_prefix=
declare arch=
declare failure_status=Fail
declare -i create_pods_privileged=0
declare -i wait_forever=0
declare -a pod_labels=()
declare -i get_sync_logs_pid=0
declare -a extra_args=()
declare -i metrics_interval=30
declare -a sync_pod=()
declare workload_step_interval=0
declare -i preserve_tmpdir=0
declare -r ssh_alg=ed25519
declare -A __sysctls=()
declare -i __sysctls_retrieved=0

declare -r sync_flag_file="/tmp/syncfile";
declare -r sync_error_file="/tmp/syncerror";
declare -r controller_timestamp_file="/tmp/timing.json";
declare uuid
uuid=$(uuidgen -r)
declare xuuid=$uuid
# Ensure that pods from other runs don't inadvertently attempt to
# communicate with our sync pod
declare sync_nonce=
sync_nonce=$(uuidgen -r)

declare kata_runtime_class=kata
declare image_pull_policy=
declare container_image='quay.io/rkrawitz/clusterbuster-base:latest'

declare accumulateddata=
declare -a plurals=('s' '')
declare artifactdir=
declare -a saved_argv=("${__topsc__:-0}" "$@")
declare -a processed_options=("${__topsc__:-0}")
declare -a pod_annotations=()
declare -A injected_errors=()
declare cb_tempdir=
declare force_cleanup_timeout=
declare default_namespace_policy=restricted

# vm related variables
declare -i vm_cores=1
declare -i vm_threads=1
declare -i vm_sockets=1
declare -i vm_start_running=1
declare vm_run_strategy=
declare vm_memory=2Gi
declare vm_grace_period=30
declare vm_image='quay.io/rkrawitz/clusterbuster-vm:latest'
declare -i vm_evict_migrate=1
declare -i vm_run_as_container=0
declare -A workload_service_ports=()
declare vm_user=cluster
declare vm_password=buster
declare vm_ssh_keyfile=
declare -i vm_run_as_root=0
# Hard code for the time being
declare -a vm_net_addr=(192 168 129 1)

declare OC=${OC:-${KUBECTL:-}}
OC=${OC:-$(type -p oc)}
OC=${OC:-$(type -p kubectl)}	# kubectl might not work, though...
OC=$(type -p "$OC")

declare __virtctl_local_ssh=UNKNOWN
declare VIRTCTL=${VIRTCTL:-$(type -p virtctl)}

function fatal() {
    local OPTIND
    local waitforit=0
    local opt
    while getopts 'w' opt "$@" ; do
	case "$opt" in
	    w) waitforit=1 ;;
	    *)             ;;
	esac
    done
    shift "$((OPTIND-1))"
    timestamp <<< "$*" 1>&2
    if ((waitforit)) ; then
	wait
    fi
    exit 1
}

function warn() {
    timestamp <<< "$*" 1>&2
}

if [[ -z "$OC" || ! -x "$OC" ]] ; then
    fatal "Can't find kubectl or oc"
fi

declare __libdir__=${__topdir__}/lib/clusterbuster
export CB_LIBPATH=${CB_LIBPATH:-$__libdir__}

[[ -d "$__libdir__" ]] || fatal "Can't find my library dir!"

. "${__libdir__}"/libclusterbuster.sh

function _helpmsg() {
    local opt
    (IFS=$'\n'; echo "$*"; if [[ -n "$*" ]] ; then echo; fi)
cat <<EOF
Clusterbuster is a tool to permit you to load a configurable workload
onto an OpenShift cluster.  ClusterBuster focuses primarily on workload
scalability, including synchronization of multi-instance workloads.

Usage: ${__topsc__:-0} [options] [extra args]
    Help:
       -h              Print basic help information.
       -H              Print extended help.

    Options:
       -B basename     Base name of pods.  Default is
                       \$CLUSTERBUSTER_DEFAULT_BASENAME if defined or
                       otherwise 'clusterbuster'.
                       All objects are labeled with this name.
       -E              Don't exit after all operations are complete.
       -e              Exit after all operations are complete (default).
       -f jobfile      Job file containing settings.
                       Continuations may be escaped with a trailing
                       backslash.
                       A number of examples are provided in the
                       examples/clusterbuster directory.
       -n              Print what would be done without doing it
       -o              Specify report format, as --report-format
       -q              Do not print verbose log messages (default)
       -Q              Don't report creation of individual objects (default
                       report them)
       -v              Print verbose log messages.
       -w workload     workload (mandatory), one of:
$(print_workloads '                       - ')
       --opt[=val]     Set the specified option.
                       Use ${__topsc__##*/} -H to list the available options.
EOF
}

function _help_extended() {
    _helpmsg "$@"
    cat <<EOF

Extended Options:
    General Options (short equivalents):
       --doit=<1,0>     Run the command or not (default 1) (inverse of -n)
       --create-namespaces-only
                        Only create namespaces.
       --jobname=name   Name of the job, for logging purposes.
                        Defaults to the workload name
       --workload=type  Specify the workload (-P) (mandatory)
       --basename=name  Specify the base name for any namespaces (-B)
       --namespaces=N   Number of namespaces
       --jobfile=jobfile
                        Process job file (-f)
       --sync           Synchronize start of workload instances (default yes)
       --precleanup     Clean up any prior objects
       --cleanup        Clean up generated objects unless there's a failure
       --cleanup-always Clean up generated objects even if there is a failure
       --wait-forever   Don't exit if sync pod dies
       --remove-namespaces=<1,0>
                        Remove namespaces when cleaning up objects.  Only
                        applies when using clusterbuster-created namespaces.
                        By default, namespaces are removed only if no
                        namespaces previously existed.
       --predelay=N     Delay for the specified time after workload
                        starts before it starts running.
       --postdelay=N    Delay for the specified time after workload
                        completes.
       --stepinterval=N Delay the specified time between steps of the workload.
       --timeout=N      Time out reporting after N seconds
       --report_object_creation=<1,0>
                        Report creation of individual objects (default 1)
                        (inverse of -Q)
       --uuid=<uuid>    Use the specified UUID for the run.  Default is to
                        generate a random-based UUID.
       --exit_at_end    Exit upon completion of workload (-e/-E)
       --verbose        Print verbose log messages (-v)
       --arch=<architecture>
                        Use the specified architecture.  Default the
                        architecture of this platform.
       --containerimage=<image>
                        Use the specified container image.
       --sync_watchdog_timeout=<timeout>
                        Set watchdog timer for all pods/VMs.  After pods/VMs
                        start, if the watchdog is not reset within the
                        timeout, the run is aborted.  Currently not set
                        (timeout = 0); if set to a value greater than
                        zero, that is the watchdog period.

    Reporting Options:
       --report=<format>
                        Print report in specified format.  Meaning of
                        report types is by type.  Default is summary
                        if not specified; if that is not reported,
                        raw format will be used.
                        - none
$(list_report_formats '                        - ')
                        The following workloads support reporting:
$(print_workloads_supporting_reporting '                        - ')
       --artifactdir=<dir>
                        Save artifacts to <dir>.  <dir> can have embedded
                        format codes:
                        %n              Job name
                        %s              Timestamp of run
                        %w              Workload
                        %{var}          Variable's value is substituted
                        %{var[item]}    to reference an array variable
                        %{var:-default} to use a default value if not set
       --prometheus-snapshot
                        Take a Prometheus snapshot and save to the
                        artifacts directory
       --metrics[=<file>]
                        benchmark-runner compatible metrics file
                        for metrics extraction.  If empty or 'none',
                        no metrics extraction is done.  If 'default',
                        the default
                        ($default_metrics_file)
                        is used.
       --metrics-epoch=<seconds>
                        Number of seconds to look back for metrics prior
                        to start of run (default $metrics_epoch)
       --metrics-interval=<interval>
                        Interval between data points for metrics collection.
                        Default $metrics_interval.
       --force-no-metrics
                        Do not attempt anything that would use metrics
                        or the prometheus pod.
       --failure-status=<status>
                        Failures should be reported as specified rather
                        than "Fail"
       --retrieve-successful-logs=<0|1>
                        If retrieving artifacts, retrieve logs for all
                        pods, not just failing pods.  Default $retrieve_successful_logs.
       --parallel-logs=n
                        If retrieving artifacts, parallelize log retrieval.

    Workload sizing options:
       --containers_per_pod=N
                        Number of containers per pod
       --deployments=N  Number of deployments or pods per namespace
       --processes=N    Number of processes per pod
       --replicas=N     Number of replicas per deployment
       --secrets=N      Number of secrets

    Generic workload rate options:
       --bytestransfer=N[,M]
                        Number of bytes for workloads operating on
                        fixed amounts of data.
       --targetdatarate=N
                        Target data rate for workloads operating at fixed
                        data rates.  May have suffixes of K, Ki,
                        M, Mi, G, Gi, T, or Ti.
       --workloadruntime=N
                        Time to run the workload where applicable
                        Two comma-separated numbers may be used to
                        specify maximum time.

    Workload placement options:
       --pin_node=[class1,class2...]=<node>
                        Force pod(s) of the specified class(es) onto the
                        specified node.  Multiple comma-separated classes
                        may be specified.  The following classes are
                        defined for general workloads:
                        - sync   (sync pods)
                        - client (worker/client pods)
                        Workloads may define other classes.
                        If no class is specified, pin node applies to all
                        pods.
       --sync-in-first-namespace=<0|1>
                        Place the sync pod in the first worker namespace.
                        Default ${sync_in_first_namespace}.
       --affinity       Force affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides affinity.
       --anti-affinity  Force anti-affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides anti-affinity.
       --anti-affinity  Force anti-affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides anti-affinity.
       --sync-affinity  Force affinity between sync and all worker pods.
       --sync-anti-affinity
                        Force anti-affinity between sync and all worker pods.
       --drop_cache     Drop the buffer cache in all pin nodes; if no
                        pin nodes are defined, drop all workers' caches.
       --drop_all_cache Drop the buffer cache on all workers.

    Generic workload storage options:
       --volume=name:type:mount_path:options
                        Mount a specified volume.
                        - name is the name of the volume
                        - type is the type of volume.
                          Currently supported volume types are:
                          - emptydir (pods only; no name required)
                          - emptydisk (VMs only; no name required)
                          - pvc or persistentvolumeclaim
                        - mount_path is the path on which to mount the volume
                            (required).
                        Options currently supported include the following:
                          - claimName is the name of a PVC if it differs
                            from the volume name.  This allows use of
                            different PVCs; all occurrences of %N
                            are replaced by the namespace of the pod
                            mounting the volume; all instances of %i
                            are replaced by the instance of the pod
                            within the namespace.
                          - size in bytes (required for emptydisk; ignored for
                            other volume types).
                          - inodes in number (optional for emptydisk; ignored
                            for other volume types).
                          - bus (VMs; ignored for pods): bus to be used
                            for the volume (default virtio)
                          - cache (VMs): caching mode (none, writeback,
                            writethrough; default none)
                          - fstype (VMs): filesystem type to format
                            the volume to; empty means to not format
                            the volume (filesystem must already be present).
                            fsopts (VMs): options to use for formatting
                            the filesystem.
                          - mountopts (VMs): mount options to be used.
                          - nfsserv (VMs): NFS server for NFS-based PVCs.
                          - nfsshare (VMs): NFS share name for NFS-based
                            PVCs.  Default to '/'.
                        Notes:
                          - A previously declared mount can be overridden
                            by specifying a later mount with the same
                            mountpoint.
                          - A previously declared mount can be removed by
                            specifying a mount with the same mountpoint
                            and empty name and type.  Example:
                            --volume=:emptydir:/var/opt/clusterbuster
                            --volume=::/var/opt/clusterbuster
                            will result in no mount on /var/opt/clusterbuster
                            unless later overridden.
                          - All previously declared mounts can be removed
                            by specifying a mount with no name, type,
                            mountpoint, or options.  Example:
                            --volume=::
                            will remove all mounts from the list.
       --workdir=<dir>  Use the specified working directory for file I/O

    Pod Options:
       --container_image=<image>
                        Image to use (default $container_image).
                        Does not apply to "classic" or "pause" workloads.
       --deployment_type=<pod,deployment,replicaset,vm>
                        Deploy via individual pods, deployments, replica sets,
                        or vms (default $deployment_type).
                        Note that functionality that relies on fixed pod
                        names or recognition of distinct pods (e. g.
                        the %i functionality in volumes) will not work
                        correctly with deployments or replicasets.
       --external_sync=host:port
                        Sync to external host rather than internally
       --request=<resource=value>
                        Resource requests
       --limit=<resource=value>
                        Resource limits
       --runtimeclass=[class1,class2...]=class
                        Run the pods in the designated runtimeclass.
                        --runtimeclass=vm is a synonym for
                        --deployment_type=vm.
       --kata           Synonym for --runtimeclass=${kata_runtime_class}
       --tolerate=<key:operator:effect>
                        Apply the specified tolerations to created pods.
       --image_pull_policy=<policy>
                        Image pull policy (system default)
       --node_selector=selector
                        Annotate pods with the specified node selector
                        Default $node_selector
                        Specify empty value to not provide a node selector.
       --pod_annotation=[:class:]annotation
                        Apply the specified annotation to all pods of the
                        optionally specified class (same meaning as for
                        --pin_node as above).  This may be specified
                        multiple times.
       --headless-services=[0,1]
                        Use headless services for service creation.
                        Default ${headless_services}
       --liveness-probe=<interval>
                        Execute a simple liveness probe every <interval>
                        seconds.
       --liveness-probe-sleep=<seconds>
                        Arrange for the liveness probe to sleep for specified
                        time.
       --privileged-pods=[0,1]
                        Create pods as privileged (default $create_pods_privileged)
       --label=[:class:]label=value
                        Apply the specified label to all pods of the
                        optionally specified class (same meaning as for
                        --pin_node as above).  This may be specified
                        multiple times.
       --scheduler=<scheduler>
                        Use the specified scheduler to schedule pods.
       --interface[=:class:]=name[:internal-interface]
                        Provide the specified network interface to the pod/VM.
                        Class has the same meaning as for --pin-node.
                        Internal-interface, if specified, is the name of the
                        interface inside the pod/VM.  Normally this should not
                        be specified, and the default (net1 for pods, eth1 for
                        VMs) should be used.

    Kata Virtualization Tuning:
       --virtiofsd-writeback=[0,1]
                        Use writeback caching for virtiofsd (default $virtiofsd_writeback).
       --virtiofsd-direct=[0,1]
                        Allow use of direct I/O for virtiofsd (default $virtiofsd_direct).
       --virtiofsd-threadpoolsize=n
                        Use the specified thread pool size for virtiofsd
                        (default 1).

    OpenShift Virtualization Options:
       --vm-threads=<value>
                        Specify the number of threads on each core (default $vm_threads).
       --vm-cores=<value>
                        Specify the number of cores on each socket (default $vm_cores).
       --vm-sockets=<value>
                        Specify the number of sockets (default $vm_sockets).
       --vm-memory=<value>
                        Specify the amount of memory (default $vm_memory).
       --vm-grace-period=<value>
                        Specify the period between when a vm is signaled to
                        shutdown and the point when KubeVirt will force off
                        the vm (default $vm_grace_period).
       --vm-image=<image_url>
                        Containerdisk image to use.
                        Default $vm_image
       --vm-migrate=[0,1]
                        Allow VMs to migrate when evicted rather than be
                        deleted.  Default $vm_evict_migrate.
       --vm-run-as-container=[0,1]
                        Run the workload as a container rather than directly.
       --vm-user=<user>
                        Create the specified user on virtual machines.
                        Default $vm_user.  Empty means no user.
       --vm-password=<password>
                        Create the specified password on virtual machines.
                        Default $vm_password.  Empty means no password.
       --vm-ssh-keyfile=file
                        Inject the public key of the specified key pair into
                        VMs for log retrieval or other access purposes.
                        Default none, in which case a temporary key
                        is generated.
       --vm-run-as-root=[0,1]
                        Run test command as root.  Default $vm_run_as_root.
       --vm-start-running=[0,1]
                        Start the VMs in running state (otherwise are started
                        separately).  Default $vm_start_running.
       --vm-run_strategy=<strategy>
                        Specify the desired run strategy for VMs.

    Tuning object creation (short equivalents):
       --scale-ns=[0,1] Scale up the number of namespaces vs.
                        create new ones (default 0).
       --scale-deployments=[0,1]
                        Scale up the number of deployments vs.
                        create new ones (default 1)
       --first_deployment=N
                        Specify the index of the first deployment.
                        Default is 0 (but see --scale_deployments)
       --first_secret=N
                        Index number of first secret to be created
       --first_namespace=N
                        Index number of first namespace to be created
       --pod-prefix=prefix
                        Prefix all created pods with this prefix.
       --sleep=N        Number of seconds between object creations
                        Below options default to sleeptime
       --sleep_between_secrets=N
       --sleep_between_namespaces=N
       --sleep_between_deployments=N

       --objs_per_call=N
                        Number of objects per CLI call.  Only objects
                        within a namespace can be created this way;
                        to improve creation performance with multiple
                        namespaces, use --parallel.
                        Below options all default to objs_per_call
       --objs_per_call_secrets=N
       --objs_per_call_namespaces=N
       --objs_per_call_deployments=N

       --parallel=N     Number of operations in parallel.  Only
                        operations across namespaces can be
                        parallelized; to improve performance
                        within one namespace, use --objs_per_call.
                        Below options all default to parallel
       --parallel_secrets=N
       --parallel_namespaces=N
       --parallel_deployments=N
       --wait_secrets   Wait for secrets to be created (default 1)
       --pod-start-timeout=<seconds>
                        Wait specified time for pods to come on line.
                        Default $default_pod_start_timeout ($default_vm_start_timeout for VMs).

Workload-specific options:
$(_help_options_workloads)

Advanced options (generally not required):
       --baseoffset=N   Add specified offset to base time
                        for calculation of start time offset
                        to correct for clock skew.  May be float.
                        This normally should not be needed, as
                        ClusterBuster can correct for clock skew
                        itself.
       --podsleep=N     Time for pod to sleep before exit
       --debug=<opt>
                        For testing purposes, print debugging information.
                        Options documented only in code.
       --inject_error=<opt>
                        For testing purposes, inject the specified error
                        condition (documented only in code).
       --force-abort    Abort the run on any error.
       --preserve-tmpdir
                        Do not remove the temporary directory at
                        end of the run

Here is a brief description of all available workloads:
$(_document_workloads)

EOF
}

function help() {
    _help_extended "$@" | "${PAGER:-more}"
    exit 1
}

function help_extended() {
    _help_extended "$@" | "${PAGER:-more}"
    exit 1
}

################################################################
# Option processing
################################################################

function set_workload_bytes() {
    local sizespec=$1
    local -i scale=${2:-1}
    if [[ $sizespec = *','* ]] ; then
	bytes_transfer=$(parse_size "${sizespec#*,}")
	bytes_transfer_max=$(parse_size "${sizespec%%,*}")
	if (( bytes_transfer > bytes_transfer_max )) ; then
	    local -i tmp=$bytes_transfer
	    bytes_transfer=$bytes_transfer_max
	    bytes_transfer_max=$tmp
	fi
    else
	bytes_transfer=$((sizespec * scale))
	bytes_transfer_max=$((sizespec * scale))
    fi
}

function set_runtime() {
    local timespec=$1
    if [[ $timespec = *','* ]] ; then
	workload_run_time=${timespec#*,}
	workload_run_time_max=${timespec%%,*}
	if (( workload_run_time > workload_run_time_max )) ; then
	    local -i tmp=$workload_run_time
	    workload_run_time=$workload_run_time_max
	    workload_run_time_max=$tmp
	fi
    else
	workload_run_time=$timespec
	workload_run_time_max=$timespec
    fi
}

function process_pin_node() {
    local nodespec=$1
    nodespec=${nodespec// /}
    [[ -n "$nodespec" ]] || return
    if [[ $nodespec = *'='* ]] ; then
	local node=${nodespec#*=}
	local class=${nodespec%%=*}
	class=${class//,/ }
	# shellcheck disable=SC2206
	local -a classes=($class)
	for class in "${classes[@]}" ; do
	    pin_nodes[$class]="$node"
	done
    else
	pin_nodes[default]="$nodespec"
    fi
}

function process_interface() {
    local if_spec=$1
    if_spec=${if_spec// /}
    [[ -n "$if_spec" ]] || return
    if [[ $if_spec = *'='* ]] ; then
	local interface=${if_spec#*=}
	local class=${if_spec%%=*}
	class=${class//,/ }
	# shellcheck disable=SC2206
	local -a classes=($class)
	for class in "${classes[@]}" ; do
	    net_interfaces[$class]="$interface"
	done
    else
	net_interfaces[default]="$if_spec"
    fi
}

function process_runtimeclass() {
    local runtimespec=$1
    if [[ $runtimespec = 'vm' ]] ; then
	deployment_type=vm
	runtime_class=vm
	return
    fi
    runtimespec=${runtimespec// /}
    runtime_class=$runtimespec
    if [[ -z "$runtimespec" ]] ; then
	runtime_classes=()
    elif [[ $runtimespec = *'='* ]] ; then
	local runtime=${runtimespec#*=}
	local class=${runtimespec%%=*}
	class=${class//,/ }
	# shellcheck disable=SC2206
	local -a classes=($class)
	for class in "${classes[@]}" ; do
	    runtime_classes[$class]="$runtime"
	done
    else
	runtime_classes[default]="$runtimespec"
    fi
}

function set_metrics_file() {
    metrics_file=${1:-}
    case "$metrics_file" in
	default|1|"$default_metrics_file") metrics_file="$__libdir__/$default_metrics_file" ;;
	''|0|none) metrics_file=							    ;;
	*)										    ;;
    esac
}

function inject_error() {
    local error="$1"
    local condition
    local options
    IFS='=' read -r condition options <<< "$error"
    injected_errors["$condition"]=${options:-SET}
    warn "*** Registering error injection '$condition' = '${injected_errors[$condition]}'"
}

function artifact_dirname() {
    local basename=${1:-cb-%T}
    echo "${basename//%T/$(printf "%($(standard_snapshot_date_format))T" -1)}"
}

function process_option() {
    local noptname
    local noptname1
    local optvalue
    read -r noptname1 noptname optvalue <<< "$(parse_option "$1")"
    # shellcheck disable=SC2206
    # shellcheck disable=SC2119
    processed_options+=("--$1")
    # shellcheck disable=SC2034
    case "$noptname1" in
	# Help, verbosity
	helpall*)		    help_extended				;;
	helpeverything*)	    help_extended				;;
	help*)			    help					;;
	verbose)		    verbose=$(bool "$optvalue")			;;
	doit)			    doit=$(bool "$optvalue")			;;
	quiet)			    verbose=$((! $(bool "$optvalue")))		;;
	forceabort*)		    set -e					;;
	preservetmpdir)		    preserve_tmpdir=$(bool "$optvalue")		;;
	# Reporting
	artifactdir)		    artifactdir="$(artifact_dirname "$optvalue")";;
	metrics|metricsfile)	    set_metrics_file "$optvalue"		;;
	metricsepoch)		    metrics_epoch=$optvalue			;;
	metricsinterval)	    metrics_interval=$optvalue			;;
	reportformat)		    report_format=$optvalue			;;
	jsonreport)		    report_format=json				;;
	rawreport)		    report_format=raw				;;
	report)		    	    report_format=${optvalue:-summary}		;;
	verbosereport)		    report_format=verbose			;;
	reportobjectcreation)	    report_object_creation=$(bool "$optvalue")	;;
	prometheussnapshot)	    take_prometheus_snapshot=$(bool "$optvalue");;
	predelay)		    predelay=$optvalue				;;
	postdelay)		    postdelay=$optvalue				;;
	stepinterval)		    workload_step_interval=$optvalue		;;
	timeout)		    timeout=$optvalue				;;
	failurestatus)		    failure_status=$optvalue			;;
	parallellog*)		    parallel_log_retrieval=$optvalue		;;
	retrievesuc*)		    retrieve_successful_logs=$(bool "$optvalue");;
	logsuc*)		    retrieve_successful_logs=$(bool "$optvalue");;
	# Basic options
	jobname)		    job_name=$optvalue				;;
	workload)		    requested_workload=$optvalue		;;
	basename)		    basename=$optvalue				;;
	arch)			    arch=$optvalue				;;
	createnamespacesonly)	    create_namespaces_only=1			;;
	watchdogtimeout)	    watchdog_timeout=$(parse_size "$optvalue")  ;;
	# Object definition
	workdir)		    common_workdir=$optvalue			;;
	configmapfile)		    configmap_files+=("$optvalue")		;;
	containerimage)		    container_image=$optvalue			;;
	containers)		    containers_per_pod=$optvalue		;;
	containersperpod)	    containers_per_pod=$optvalue		;;
	deploymenttype)		    deployment_type=$optvalue			;;
	deployments|depspername*)   deps_per_namespace=$optvalue		;;
	exitatend)		    exit_at_end=$(bool "$optvalue")		;;
	imagepullpolicy)	    image_pull_policy=$optvalue			;;
	namespaces)		    namespaces=$optvalue			;;
	nodeselector)		    node_selector=$optvalue			;;
	volume)			    volumes+=("$optvalue")			;;
	processes|processesperpod)  processes_per_pod=$optvalue			;;
	jobfile)		    process_job_file "$optvalue"		;;
	pinnode)		    process_pin_node "$optvalue"		;;
	interface)		    process_interface "$optvalue"		;;
	replicas)		    replicas=$optvalue				;;
	limit|limits)		    resource_limits+=("$optvalue")		;;
	request|requests)	    resource_requests+=("$optvalue")		;;
	kata)			    process_runtimeclass "kata"			;;
	podannotation)		    pod_annotations+=("$optvalue")		;;
	label|labels)		    pod_labels+=("$optvalue")			;;
	runtimeclass)		    process_runtimeclass "$optvalue"		;;
	uuid)			    uuid=$optvalue				;;
	secrets)		    secrets=$optvalue				;;
	workloadruntime)	    set_runtime "$optvalue"			;;
	workload_size)		    set_workload_bytes "$optvalue"		;;
	targetdatarate)		    target_data_rate=$(parse_size "$optvalue")	;;
	tolerate|toleration)	    tolerations+=("$optvalue")			;;
	dropcache)                  drop_node_cache=$(bool "$optvalue")         ;;
	dropallcache)		    drop_all_node_cache=$(bool "$optvalue")     ;;
	headlessservices)	    headless_services=$(bool "$optvalue")	;;
	virtiofsdwriteback)	    virtiofsd_writeback=$(bool "$optvalue")	;;
	virtiofsddirect)	    virtiofsd_direct=$(bool "$optvalue")	;;
	virtiofsdthread*)	    virtiofsd_threadpoolsize=$optvalue		;;
	livenessprobeint*)	    liveness_probe_frequency=$optvalue		;;
	livenessprobesleep*)	    liveness_probe_sleep_time=$optvalue		;;
	privilege*)		    create_pods_privileged=$(bool "$optvalue")	;;
	syncinfirst*)		    sync_in_first_namespace=$(bool "$optvalue") ;;
	scheduler)		    scheduler=$optvalue				;;
	affinity)
	    case "$optvalue" in
		1|'') affinity=1      ;;
		2|anti) affinity=2    ;;
		*) affinity=0         ;;
	    esac
	    ;;
	antiaffinity)
	    case "$optvalue" in
		1|'') affinity=2      ;;
		*) affinity=0         ;;
	    esac
	    ;;
	syncaffinity)
	    case "$optvalue" in
		1|'') sync_affinity=1  ;;
		2|anti) sync_affinity=2;;
		*) sync_affinity=0     ;;
	    esac
	    ;;
	syncantiaffinity)
	    case "$optvalue" in
		1|'') sync_affinity=2  ;;
		*) sync_affinity=0     ;;
	    esac
	    ;;
        # vm specs
        vmcores)                    vm_cores=$optvalue                          ;;
        vmsockets)                  vm_sockets=$optvalue                        ;;
        vmthreads)                  vm_threads=$optvalue                        ;;
        vmmemory)                   vm_memory=$optvalue                         ;;
        vmgraceperiod)              vm_grace_period=$optvalue                   ;;
        vmimage)                    vm_image=$optvalue                          ;;
	vmmigrate*)		    vm_evict_migrate=$(bool "$optvalue")	;;
	vmrunascontainer)	    vm_run_as_container=$(bool "$optvalue")	;;
	vmuser)			    vm_user=$optvalue				;;
	vmpassword)		    vm_password=$optvalue			;;
	vmrunasroot)		    vm_run_as_root=$(bool "$optvalue")		;;
	vmsshkey*)		    vm_ssh_keyfile=$optvalue			;;
	vmstart*)		    vm_start_running=$(bool "$optvalue")	;;
	vmrunstrategy)		    vm_run_strategy="$optvalue"			;;
	# Object creation
	obj*spercall)		    objs_per_call=$optvalue			;;
	parallel)		    parallel=$optvalue				;;
	sleep)			    sleeptime=$optvalue				;;
	podprefix)		    pod_prefix=${optvalue:+$optvalue-}		;;
	firstdeployment)	    first_deployment=$optvalue			;;
	parallelconfigmaps)	    parallel_configmaps=$optvalue		;;
	parallelsecrets)	    parallel_secrets=$optvalue			;;
	parallelnamespaces)	    parallel_namespaces=$optvalue		;;
	paralleldeployments)	    parallel_deployments=$optvalue		;;
	obj*spercallconfigmaps)	    objs_per_call_configmaps=$optvalue		;;
	obj*spercallsecrets)	    objs_per_call_secrets=$optvalue		;;
	obj*spercallnamespaces)	    objs_per_call_namespaces=$optvalue		;;
	obj*spercalldeployments)    objs_per_call_deployments=$optvalue		;;
	sleepbetweenconfigmaps)	    sleep_between_configmaps=$optvalue		;;
	sleepbetweensecrets)	    sleep_between_secrets=$optvalue		;;
	sleepbetweennamespaces)	    sleep_between_namespaces=$optvalue		;;
	sleepbetweendeployments)    sleep_between_deployments=$optvalue		;;
	waitsecrets)		    wait_for_secrets=$(bool "$optvalue")	;;
	scalens)	   	    scale_ns=$(bool "$optvalue")		;;
	scaledeployments)   	    scale_deployments=$(bool "$optvalue")	;;
	precleanup)		    precleanup=$(bool "$optvalue")		;;
	cleanup)
	    cleanup=$(bool "$optvalue")
	    if ((! cleanup)) ; then cleanup_always=0; fi
	    ;;
	cleanupalways)
	    cleanup_always=$(bool "$optvalue")
	    if ((cleanup_always)) ; then cleanup=1; fi
	    ;;
	removenamespace*)	    remove_namespaces=$(bool "$optvalue")	;;
	baseoffset)		    baseoffset=$optvalue			;;
	# Synchronization
	sync|syncstart)		    sync_start=$((1-sync_start))		;;
	waitforever)                wait_forever=$(bool "$optvalue")		;;
	forcenometrics)		    metrics_support=$(($(bool "$optvalue")-1))  ;;
	podstart[ti]*)		    pod_start_timeout=$optvalue			;;
	externalsync)
	    if [[ $optvalue =~ ^(-|([[:alnum:]][-_[:alnum:]]*[[:alnum:]]\.)*([[:alnum:]][-_[:alnum:]]*[[:alnum:]])):([1-9][[:digit:]]{0,4})$ ]] ; then
		sync_host=${BASH_REMATCH[1]}
		sync_port=${BASH_REMATCH[4]}
		if (( sync_port > 65535 )) ; then
		    echo "Illegal external sync port (must be 1 <= port <= 65535)"
		    help
		fi
		sync_start=1
	    else
		echo "Undecipherable external sync host:port $optvalue"
		help
	    fi
	    ;;
	# Testing
	injecterror)		    inject_error "$optvalue"			;;
	debug*)			    register_debug_condition "$optvalue"	;;
	# Force delete everything after oc delete times out.
	# This is dangerous and hence not documented.
	# See https://access.redhat.com/solutions/4165791
	forcecleanupiknowthisisdangerous)
	    force_cleanup_timeout=${optvalue:-600}
	    ;;
	# Unknown options
	*)
	    unknown_opts+=("$1")
	    unknown_opt_names+=("$noptname ($noptname1)") ;;
    esac
}

function process_job_file() {
    local jobfile="$1"
    if [[ ! -f $jobfile || ! -r $jobfile ]] ; then
	fatal "Job file $jobfile cannot be read"
    fi
    while IFS= read -r line ; do
	# shellcheck disable=SC1003
	while [[ $line = *'\' ]] ; do
	    line=${line::-1}
	    local tline
	    IFS= read -r tline
	    [[ -z "$tline" ]] && break
	    line+="$tline"
	done
	line=${line%%#*}
	line=${line## }
	line=${line##	}
	if [[ -z "$line" ]] ; then continue; fi
	process_option "$line"
    done < "$jobfile"
}

################################################################
# Workload API management
################################################################

function supports_reporting() {
    local workload="${1:-${requested_workload:-}}"
    [[ -n "$workload" ]] &&
	(! supports_api -w "$workload" supports_reporting ||
	     call_api -w "$workload" supports_reporting)
}

function print_workloads_supporting_reporting() {
    local prefix="${1:-}"
    local workload
    while read -r workload ; do
	supports_reporting "$workload" && echo "$prefix$workload"
    done <<< "$(print_workloads)"
}

function list_report_formats() {
    local prefix=${1:-}
    while read -r format ; do
	echo "$prefix$format"
    done <<< "$("${pathdir:-.}/clusterbuster-report" --list_formats)"
}

function create_deployment() {
    if supports_api -w "$requested_workload" create_deployment ; then
	call_api -w "$requested_workload" create_deployment "$@"
    else
	create_generic_deployment "$@"
    fi
}

function calculate_logs_required() {
    if supports_reporting "$requested_workload" ; then
	if supports_api -w "$requested_workload" calculate_logs_required ; then
	    call_api -w "$requested_workload" calculate_logs_required "$@"
	else
	    local -i namespaces=$1
	    local -i deps_per_namespace=${2:-1}
	    local -i replicas=${3:-1}
	    local -i containers_per_pod=${4:-1}
	    echo $((namespaces * processes_per_pod * containers_per_pod * replicas * deps_per_namespace))
	fi
    else
	echo 1
    fi
}

function list_configmaps() {
    cat <<EOF
$(find_on_path pod_files "cb_util.py")
$(find_on_path pod_files "clusterbuster_pod_client.py")
$(find_on_path pod_files "sync.py")
$(find_on_path pod_files "drop_cache.py")
$(find_on_path pod_files "do-sync")
$(find_on_path pod_files "drop-cache")
EOF
    if supports_api -w "$requested_workload" list_configmaps ; then
	call_api -w "$requested_workload" -s list_configmaps |grep .
    else
	find_on_path pod_files "${requested_workload}.py"
    fi
}

function list_user_configmaps() {
    call_api -w "$requested_workload" -s list_user_configmaps | grep .
}

function generate_workload_metadata() {
    call_api -w "$requested_workload" -s generate_metadata
}

function generate_environment() {
    call_api -w "$requested_workload" -s generate_environment
}

function requires_drop_cache() {
    call_api -w "$requested_workload" requires_drop_cache
}

function get_sysctls() {
    if ((! __sysctls_retrieved)) && [[ -n "$requested_workload" ]] ; then
	local key value
	while read -r key value ; do
	    if [[ -n "$key" ]] ; then
		__sysctls[$key]="$value"
	    fi
	done <<< "$(call_api -w "$requested_workload" -s sysctls)"
	__sysctls_retrieved=1
    fi
}

function namespace_policy() {
    local policy
    if [[ $(type -t "deployment_type_${deployment_type,,}_policy") = function ]] ; then
        policy=$("deployment_type_${deployment_type,,}_policy")
    elif ((create_pods_privileged)) || requires_drop_cache ; then
	policy=privileged
    else
        policy=$(call_api -w "$requested_workload" -s namespace_policy)
    fi
    echo "${policy:-${default_namespace_policy}}"
}

function vm_required_packages() {
    call_api -w "$requested_workload" -s vm_required_packages
}

function vm_setup_commands() {
    call_api -w "$requested_workload" -s vm_setup_commands
}

function listen_ports() {
    call_api -w "$requested_workload" -s listen_ports
}

function deployment_type_vm_policy() {
    echo privileged
}
################################################################
# Helpers
################################################################

function __OC() {
    if ((doit)) ; then
	debug kubectl "$OC" "$@"
    fi
    if ((! doit)) ; then
	echo '(skipped)' "$OC" "${@@Q}" 1>&2
    elif [[ $1 = exec || $1 = rsh ]] ; then
	# Capture error output
	(set -o pipefail; "$OC" "$@" 2>&1 1>&3 |sed -e "s/^/${*//\//\\/}: /" |timestamp 1>&2) 3>&1
    elif [[ $1 = describe || $1 = get || $1 = status || $1 = logs || $1 = version ]] ; then
	"$OC" "$@"
    elif ((report_object_creation)) ; then
	"$OC" "$@" 2>&1
    else
	"$OC" "$@" 2>&1 |grep -v -E '(^No resources found|deleted|created|labeled|condition met)$'
    fi
    return "${PIPESTATUS[0]}"
}

function _OC() {
    debug kubectl "$OC" "$@"
    __OC "$@" || fatal "__KUBEFAIL__ $OC $* failed!"
}

function ___OC() {
    debug kubectl "$OC" "$@"
    __OC "$@" || fatal "$OC $* failed!"
}

function ____OC() {
    debug kubectl "$OC" "$@"
    "$OC" "$@"
}

function _____OC() {
    # When the oc/kubectl command must run in the same process
    # as the caller.
    debug kubectl "$OC" "$@"
    exec "$OC" "$@"
}

# Newer versions of virtctl need --local-ssh to force virtctl to use the local
# ssh/scp, which is required to use options to shut off unauthenticated host
# messages which require interaction.  Some older versions of virtctl, such as
# the version in Fedora 38, do not accept this option.
#
# There's no obvious "clean" way to determine what's needed.  According to the
# virtctl change log, this was introduced around 0.56, but Fedora 38 has
# 0.58.1 and does not accept this option.
function _VIRTCTL_BASE() {
    local subcmd=${1:-}
    shift
    if [[ $subcmd = ssh || $subcmd = scp ]] ; then
	if [[ $__virtctl_local_ssh = UNKNOWN ]] ; then
	    if virtctl ssh --help </dev/null 2>&1 |grep -q -e '--local-ssh[= \t]' ; then
		__virtctl_local_ssh=--local-ssh
	    else
		__virtctl_local_ssh=
	    fi
	fi
	"$VIRTCTL" "$subcmd" ${__virtctl_local_ssh:+"$__virtctl_local_ssh"} "$@"
    else
	"$VIRTCTL" "$subcmd" "$@"
    fi
}

function __VIRTCTL() {
    if ((doit)) ; then
	debug kubectl "$VIRTCTL" "$@"
	_VIRTCTL_BASE "$@"
    else
	echo '(skipped)' "$VIRTCTL" "${@@Q}" 1>&2
    fi
}

function _VIRTCTL() {
    debug kubectl "$VIRTCTL" "$@"
    __VIRTCTL "$@" || fatal "__KUBEFAIL__ $VIRTCTL $* failed!"
}

function ___VIRTCTL() {
    debug kubectl "$VIRTCTL" "$@"
    __VIRTCTL "$@" || fatal "$VIRTCTL $* failed!"
}

function ____VIRTCTL() {
    debug kubectl "$VIRTCTL" "$@"
    _VIRTCTL_BASE "$@"
}

function _____VIRTCTL() {
    # When the oc/kubectl command must run in the same process
    # as the caller.
    debug kubectl "$VIRTCTL" "$@"
    exec "$VIRTCTL" "$@"
}

function run_on_sync() {
    get_run_aborted && return 1
    ____OC exec "${sync_pod[@]}" -- "$@"
}

function run_on_sync_n() {
    get_run_aborted && return 1
    ____OC exec "${sync_pod[@]}" --stdin=false -- "$@"
}

function namespace_exists() {
    local ns=$1
    ____OC get ns "$ns" >/dev/null 2>&1
}

function object_exists() {
    ____OC get "$@" >/dev/null 2>&1
}

function delete_object_safe() {
    object_exists "$@" && _OC delete "$@"
}

################################################################
# Synchronization between worker pods
################################################################

function get_sync() {
    if (( sync_start )) ; then
	if [[ ${1:-} != -q ]] ; then
	    echo "${global_sync_service}:$sync_port:$sync_ns_port"
	fi
	return 0
    else
	return 1
    fi
}

function create_sync_service() {
    local namespace=$1
    local sync_clients=${2:-0}
    local initial_sync_clients=${3:-$sync_clients}
    if get_sync -q ; then
	if [[ $namespace = "${namespaces_to_create[0]:-}" ]] ; then
	    create_service -W -h -v "${basename}-sync-sync" "$sync_namespace" "${sync_namespace}-sync" "$sync_port" "$sync_ns_port"
	    create_sync_deployment "$sync_namespace" "$((sync_clients * ${#namespaces_to_create[@]}))" "$((initial_sync_clients * ${#namespaces_to_create[@]}))"
	fi
	create_external_service "$namespace" "${basename}-sync-sync" "${global_sync_service}" "$sync_port" "$sync_ns_port"
    fi
}

function get_drop_cache() {
    local namespace=$1
    local instance=$2
    local replica=$3
    requires_drop_cache && echo "${namespace}-${workload}-${instance}-${replica}-dc:$drop_cache_port"
}

################################################################
# Logging
################################################################

# Based on https://gist.github.com/akostadinov/33bb2606afe1b334169dfbf202991d36
function stack_trace() {
    local -a stack=()
    local stack_size=${#FUNCNAME[@]}
    local -i start=${1:-1}
    local -i max_frames=${2:-$stack_size}
    ((max_frames > stack_size)) && max_frames=$stack_size
    local -i i
    local -i max_funcname=0
    local -i stack_size_len=${#max_frames}
    local -i max_filename_len=0
    local -i max_line_len=0

    # to avoid noise we start with 1 to skip the stack function
    for (( i = start; i < max_frames; i++ )); do
	local func="${FUNCNAME[$i]:-(top level)}"
	((${#func} > max_funcname)) && max_funcname=${#func}
	local src="${BASH_SOURCE[$i]:-(no file)}"
	# Line number is used as a string here, not an int,
	# since we want the length of it as a string.
	local line="${BASH_LINENO[$(( i - 1 ))]}"
	[[ $src = "$__realsc__" ]] && src="$__topsc__"
	((${#src} > max_filename_len)) && max_filename_len=${#src}
	((${#line} > max_line_len)) && max_line_len=${#line}
    done
    local stack_frame_str="    (%${stack_size_len}d)   %${max_filename_len}s:%-${max_line_len}d  %${max_funcname}s%s"
    local -i arg_count=${BASH_ARGC[0]}
    for (( i = start; i < max_frames; i++ )); do
	local func="${FUNCNAME[$i]:-(top level)}"
	local -i line="${BASH_LINENO[$(( i - 1 ))]}"
	local src="${BASH_SOURCE[$i]:-(no file)}"
	[[ $src = "$__realsc__" ]] && src="$__topsc__"
	local -i frame_arg_count=${BASH_ARGC[$i]}
	local argstr=
	if ((frame_arg_count > 0)) ; then
	    local -i j
	    for ((j = arg_count + frame_arg_count - 1; j >= arg_count; j--)) ; do
		argstr+=" ${BASH_ARGV[$j]}"
	    done
	fi
	# We need a dynamically generated string to get the columns correct.
	# shellcheck disable=SC2059
	stack+=("$(printf "$stack_frame_str" "$((i - start))" "$src" "$line" "$func" "${argstr:+ $argstr}")")
	arg_count=$((arg_count + frame_arg_count))
    done
    (IFS=$'\n'; echo "${stack[*]}")
}

function set_run_started() {
    touch "${cb_tempdir}/___run_started"
}

function get_run_started() {
    test -f "${cb_tempdir}/___run_started"
}

function set_run_aborted() {
    echo "Run aborted: ${*:-unknown reason}" 1>&2
    stack_trace 1>&2
    touch "${cb_tempdir}/___run_aborted"
}

function get_run_aborted() {
    test -f "${cb_tempdir}/___run_aborted"
}

function get_run_failed() {
    test -f "${cb_tempdir}/___run_failed" || test -f "${cb_tempdir}/___run_aborted"
}

function get_failure_reason() {
    [[ -f "${cb_tempdir}/___run_failed" ]] && cat "${cb_tempdir}/___run_failed"
}

function set_run_failed() {
    if get_run_failed ; then
	if ! get_run_aborted ; then
	    echo "Secondary failure: ${*:-unknown reason}" 1>&2
	    stack_trace 1 3 1>&2
	fi
	return
    fi
    echo "Run failed: ${*:-unknown reason}" 1>&2
    stack_trace 1>&2
    echo "${*:-unknown reason}" > "${cb_tempdir}/___run_failed"
    run_on_sync_n sh -c "echo 'Please see logs for details.' >> '$sync_error_file'" || touch "${cb_tempdir}/___run_aborted"
}

function supports_metrics() {
    if ((metrics_support < 0)) ; then
	metrics_support=1
	____OC get pod -n openshift-monitoring prometheus-k8s-0 >/dev/null 2>&1 || metrics_support=0
    fi
    ((metrics_support > 0))
}

function get_pod_timestamp() {
    local format=${1:-%s.%N}
    shift
    ___OC exec "$@" -- /bin/sh -c "date '+$format'" || killthemall "Unable to retrieve timestamp from pod $*"
}

function get_prometheus_pod_timestamp() {
    local format=${1:-%s.%N}
    if supports_metrics ; then
	get_pod_timestamp "$format" -n openshift-monitoring prometheus-k8s-0 -c prometheus
    else
	date "+$format"
	return 1
    fi
}

function readable_timestamp() {
    local rawtime=${1:-}
    date -u "+$(standard_snapshot_date_format)" ${rawtime:+"--date=@$rawtime"}
}

function set_start_timestamps() {
    local -i retries=5
    if supports_metrics ; then
	while ((retries-- > 0)) ; do
	    first_start_timestamp=$(date +%s.%N)
	    if prometheus_exact_starting_timestamp=$(get_prometheus_pod_timestamp '%s.%N') ; then
		prometheus_starting_timestamp=${prometheus_exact_starting_timestamp%%.*}
		second_start_timestamp=$(date +%s.%N)
		return 0
	    fi
	    if ((retries > 0)) ; then
		warn "Fetch timestamp failed! $retries attempt(s) left"
		sleep 5
	    else
		warn "Unable to retrieve Prometheus timestamp (hard error)"
	    fi
	done
	return 1
    else
	first_start_timestamp=$(date +%s.%N)
	prometheus_starting_timestamp=$first_start_timestamp
	prometheus_exact_starting_timestamp=$first_start_timestamp
	second_start_timestamp=$first_start_timestamp
    fi
}

function get_pod_and_local_timestamps() {
    local -i sync_prestart
    sync_prestart=$(date +%s)
    until __OC exec "$@" -- /bin/sh -c "date +%s.%N" </dev/null >/dev/null 2>/dev/null ; do
	if get_run_failed || get_run_aborted ; then
	    return 1
	fi
	local status
	status=$(__OC get pod "$@" -o jsonpath="{.status.phase}" 2>/dev/null)
	case "$status" in
	    Error|Failed)
		echo "Sync pod failed:" 1>&2
		__OC logs "$@" | tail -10 1>&2
		killthemall "Sync pod failed"
		;;
	    *)  ;;
	esac
	if (($(date +%s) - sync_prestart > pod_start_timeout)) ; then
	    echo "Sync pod did not start!" 1>&2
	    if [[ -n "$artifactdir" ]] ; then
		__OC describe pod "$@" > "${artifactdir}/sync.desc"
	    fi
	    killthemall "Sync pod did not start in $pod_start_timeout seconds"
	fi
	sleep 2
    done
    local first_local_ts
    local remote_ts
    local second_local_ts
    first_local_ts=$(date +%s.%N)
    remote_ts=$(__OC exec "$@" -- /bin/sh -c "date +%s.%N" </dev/null) || killthemall "Unable to retrieve sync pod timestamp"
    second_local_ts=$(date +%s.%N)
    _OC exec --stdin=true "$@" -- /bin/sh -c "cat > '${controller_timestamp_file}.tmp' && mv '${controller_timestamp_file}.tmp' '${controller_timestamp_file}'" <<EOF
{
  "first_controller_ts": $first_local_ts,
  "sync_ts": $remote_ts,
  "second_controller_ts": $second_local_ts
}
EOF
    # shellcheck disable=SC2181
    if (( $? != 0 )) ; then
	set_run_failed "Unable to write timing data to sync pod"
	return 1
    fi
    echo "$first_local_ts" "$remote_ts" "$second_local_ts"
}

function start_prometheus_snapshot() {
    if ((!doit)) || ! supports_metrics ; then return 0; fi
    report_if_desired "Starting Prometheus snapshot" 1>&2
    _OC delete pod -n openshift-monitoring prometheus-k8s-0
    local -i retry=12
    until __OC get pod -n openshift-monitoring prometheus-k8s-0 >/dev/null 2>&1 ; do
	report_if_desired "Promtheus pod did not start, $retry attempt(s) left" 1>&2
	if ((retry <= 0)) ; then
	    killthemall "Prometheus pod did not restart!"
	fi
	retry=$((retry-1))
	sleep 5
    done
    __OC wait --for=condition=Ready -n openshift-monitoring pod/prometheus-k8s-0 || killthemall "Prometheus pod did not become ready"
    set_start_timestamps || return 1
    # Wait for prometheus pod to fully initialize

    sleep "$metrics_epoch"
    report_if_desired "Prometheus snapshot started" 1>&2
}

function retrieve_prometheus_snapshot() {
    if ((!doit)) || ! supports_metrics ; then return 0; fi
    report_if_desired "Retrieving Prometheus snapshot" 1>&2
    local dir=${1:-$artifactdir}
    sleep 60
    prometheus_ending_timestamp=$(get_prometheus_pod_timestamp)
    local promdb_name
    promdb_name="promdb_$(readable_timestamp "$prometheus_starting_timestamp")_$(readable_timestamp "$prometheus_ending_timestamp")"
    local promdb_path="${dir:+${dir}/}${promdb_name}.tar"
    if __OC exec -n openshift-monitoring prometheus-k8s-0 -c prometheus -- /bin/sh -c "tar cf - . -C /prometheus --transform 's,^[.],./${promdb_name},' .; true" > "$promdb_path" ; then
	report_if_desired "Prometheus snapshot retrieved" 1>&2
    else
	echo "Unable to retrieve Prometheus snapshot" 1>&2
    fi
}

function ts() {
    local dt
    dt=$(date '+%s.%N')
    local sec=${dt%.*}
    local ns=${dt#*.}
    echo "${sec}.${ns:0:6}"
}

function run_status_monitor_1() {
    # We need to kill the actual monitor reasonably quickly if the job fails.

    _____OC get pod "$@" -w -o jsonpath='{.metadata.namespace} {.metadata.name} {.status.phase}{"\n"}' &
    local oc_proc=$!
    trap 'kill -9 "$oc_proc" >/dev/null 2>&1; return 1' PIPE
    while ! get_run_failed ; do
	printf '++Mark++ ++Mark+ %(%s)T%s' -1 $'\n'
	sleep 1
    done
    kill -9 "$oc_proc" 1>&2
}

function run_status_monitor() {
    if [[ -n "$artifactdir" ]] ; then
	mkdir -p "$artifactdir"
	rm -f "$artifactdir/.artifacts"
	run_status_monitor_1 "$@" | stdbuf -i0 -o0 -e0 tee >(grep -v '+\+Mark\+\+' | timestamp >"${artifactdir}/monitor.log")
    else
	run_status_monitor_1 "$@"
    fi
}

function _monitor_pods() {
    if ((timeout > 0)) ; then
	timeout=$(($(date +%s) + timeout))
    fi
    local pods_pending=''
    local -i pod_progress_timeout=0
    if [[ -n "${injected_errors[timeout]:-}" ]] ; then
	warn "*** Injecting forced timeout error"
    fi
    # shellcheck disable=2034
    local -i running_pod_count=0
    local -i finished_pod_count=0
    local -i other_pod_count=0
    local -A pod_status=()
    local -A running_pods=()
    local -A starting_pods=()
    local pod
    local name
    local namespace
    local status
    local lstatus
    if [[ -n "${injected_errors[pending]:-}" ]] ; then
	warn "*** Injecting pending error"
	starting_pods["TEST/TEST"]=1
    fi
    local -i message_printed=0
    while read -r namespace name status ; do
	if [[ -n "${cb_tempdir:-}" && ( -f "$cb_tempdir/___run_complete" || -f "$cb_tempdir/___run_failed" ) ]] ; then
	    if [[ -z "${injected_errors[timeout]:-}" ]] ; then
		return 0
	    else
		sleep infinity
	    fi
	fi
	if [[ $namespace != ++Mark++ ]] ; then
	    # This definitely shouldn't happen, but being defensive doesn't hurt.
	    if [[ -z "$status" || -z "$namespace" || -z "$name" ]] ; then continue; fi
	    pod="${namespace,,}/${name,,}"
	    lstatus="${status,,}"
	    # No guarantee that we won't get a repeated status
	    if [[ $lstatus = "${pod_status[$name]:-}" ]] ; then continue; fi
	    debug monitor "$pod" "${pod_status[$pod]:-}" '=>' "$lstatus"
	    pod_status["$pod"]=$lstatus
	    unset "starting_pods[$pod]"
	    unset "running_pods[$pod]"
	    case "$lstatus" in
		error|failed|oomkilled)
		    echo "Pod -n $namespace $name $status!" 1>&2
		    echo "Tail end of logs:" 1>&2
		    # TODO: Need to get logs from each container
		    ____OC logs -n "$namespace" "$name" |tail -20 1>&2
		    set_run_failed "Pod -n $namespace $name failed with $status"
		    if ((get_sync_logs_pid > 1)) ; then kill -INT "$get_sync_logs_pid"; fi
		    return 1
		    ;;
		pending|containercreating)
		    starting_pods["$namespace/$name"]=1
		    ;;
		running)
		    other_pod_count=$((other_pod_count-1))
		    running_pods["$namespace/$name"]=1
		    ;;
		completed|terminat*|succeeded)
		    unset "pod_status[$pod]"
		    other_pod_count=$((other_pod_count-1))
		    finished_pod_count=$((finished_pod_count+1))
		    ;;
		*)
		    other_pod_count=$((other_pod_count+1))
		    ;;
	    esac
	else
	    # Timestamp
	    local timestamp=$status
	    if [[ -z "${injected_errors[timeout]:-}" ]] && ! supports_reporting "$requested_workload" ; then
		if (( ${#running_pods[@]} + finished_pod_count > 0 && other_pod_count == 0 )) ; then
		    set_run_failed "Run failed: running pods ${#running_pods[@]} finished pods $finished_pod_count others $other_pod_count"
		    sleep "$workload_run_time"
		    return
		fi
	    fi

	    if [[ -n "${starting_pods[*]}" ]] ; then
		pods_now_pending=$(IFS=$'\n'; echo "${!starting_pods[*]}" | sort)
		if [[ "$pods_now_pending" != "$pods_pending" ]] ; then
		    pod_progress_timeout=$((timestamp+pod_start_timeout))
		    pods_pending="$pods_now_pending"
		    if [[ -z "$pods_pending" ]] ; then
			report_if_desired "All pods are running                                 " 1>&2
		    else
			# If we've reported pods pending, and then more pods are running,
			# we want to overprint the old message so it doesn't look like things
			# are stuck.  However, to avoid log clutter, we don't do this unless
			# a pending message was previously printed.
			if ((message_printed)) ; then
			    echo -ne "                                                  \r" | report_if_desired 1>&2
			    message_printed=0
			fi
		    fi
		else
		    if ((timestamp > pod_progress_timeout)) ; then
			set_run_failed "No progress with pods after $pod_start_timeout seconds, ${#starting_pods[@]} pods still pending."
			return 1
		    fi
		    if ((pod_progress_timeout-timestamp <= 30)) ; then
			local p_seconds=seconds
			local p_pods=pods
			if ((pod_progress_timeout-timestamp == 1)) ; then p_seconds=second; fi
			if ((${#starting_pods[@]} == 1)) ; then p_pods=pod; fi
			echo -ne "${#starting_pods[@]} $p_pods pending (will retry $((pod_progress_timeout-timestamp)) $p_seconds)\r" | report_if_desired 1>&2
			message_printed=1
		    fi
		fi
	    elif ((message_printed)) ; then
		echo -ne "                                                  \r" | report_if_desired 1>&2
		message_printed=0
	    fi
	    if ((timeout > 0 && timestamp > timeout)) ; then
		set_run_failed "Run timed out"
	    fi
	fi
    done
}

function monitor_pods() {
    exec 0</dev/null 1>/dev/null
    run_status_monitor "$@" | _monitor_pods "$timeout"
}

function run_logger() {
    local -a pids_to_kill=()
    local OPTIND=0
    while getopts "p:" arg "$@" ; do
	case "$arg" in
	    p) pids_to_kill+=("$OPTARG") ;;
	    *)                           ;;
	esac
    done
    shift $((OPTIND-1))
    "$@" </dev/null
    if [[ -n "${pids_to_kill[*]}" ]] ; then
	exec 2>/dev/null
	kill -USR2 "${pids_to_kill[@]}"
	# shellcheck disable=SC2046
	[[ -n "$(jobs -p)" ]] && kill $(jobs -p)
    fi
}

# Use a separate flag file for errors to simplify the logic.
# This way we can send correct output to stdout, but handle
# errors in a separate subprocess.
function _fail_helper()  {
    trap exit TERM
    local faildata
    while : ; do
	local podstatus
	podstatus=$(____OC get pod -ojsonpath='{.status.phase}' "${sync_pod[@]}" 2>/dev/null)
	case "$podstatus" in
	    Succeeded|Terminated|Completed|OOMKilled)
		return
		;;
	    Running)
		# Need to produce periodic output to stderr to prevent
		# oc exec connection from prematurely terminating.
		# If the flag file exists, we need to exit even if there's
		# no error flag.
		faildata="$(run_on_sync_n sh -c "while [[ ! -f '$sync_error_file' && ! -f '$sync_flag_file' ]] ; do sleep 5; echo KEEPALIVE 1>&2; done; cat '$sync_error_file'" 2>/dev/null)"
		if [[ -n "$faildata" ]] ; then
		    set_run_failed "Run failed: ${nl}${faildata:-}"
		    return
		fi
		;;
	    *)
		sleep 1;
		;;
	esac
    done
}

function _log_helper() {
    local OPTIND=0
    local OPTARG
    local fail_helper=
    while getopts 'e:' opt "$@" ; do
	case "$opt" in
	    e) fail_helper=$OPTARG ;;
	    *)			   ;;
	esac
    done
    shift $((OPTIND-1))
    if [[ -n "${fail_helper:-}" ]] ; then
	"$fail_helper" &
    fi
    if [[ -n "${injected_errors[timeout]:-}" ]] ; then
	echo "*** Injecting forced timeout error" 1>&2
	sleep infinity
    fi
    local tmpfile="$cb_tempdir/_log_helper.out"
    until [[ $(____OC get pod -ojsonpath='{.status.phase}' "${sync_pod[@]}" 2>/dev/null) != 'Running' ]] ; do
	# Need to produce periodic output to stderr to prevent oc exec connection from prematurely terminating
	# We want to capture all stderr output except for the KEEPALIVE and DONE messages.
	# Unfortunately, bash does not allow direct redirection of file descriptors other than stdout into
	# pipes, so we have to redirect stderr to stdout, stdout to an unused file descriptor (4 in this case,
	# as we use 3 for other purposes).  The we pipe stdout (former stderr) through a grep command
	# to filter out the status messages, timestamp them and send them back to stderr, and finally
	# redirect fd 4 back to the temporary file.
	# If that seems enough to make your head spin...well, it is.
	if ( (run_on_sync_n sh -c "while [[ ! -f '$sync_flag_file' && ! -f '$sync_error_file' ]] ; do sleep 5; echo KEEPALIVE 1>&2; done; cat '$sync_flag_file'; sleep 5; echo DONE 1>&2") 2>&1 1>&4 | while read -r l; do if [[ $l != KEEPALIVE && $l != DONE ]] ; then echo "$l"; fi; done | timestamp 1>&2) 4> "$tmpfile" &&
	       grep -q 'worker_results' "$tmpfile" && jq -c . "$tmpfile" >/dev/null 2>&1 ; then
	    report_if_desired "Run complete, retrieving results" 1>&2
	    cat "$tmpfile"
	    rm -f "$tmpfile"
	    break
	fi
	if [[ ! -d "$cb_tempdir" ]] ; then
	    timestamp <<< "Data retrieval failed!  Cannot retry" 1>&2
	    return 1
	fi
	get_run_failed && return 1
	timestamp <<< "Data retrieval failed!  Retrying" 1>&2
	# One cause of failure is problems with cert rotation or other causes interfering
	# with communication with the apiserver.  The first connection after that resolves itself
	# results in messages such as
	# Error from server: error dialing backend: write tcp 192.168.216.10:55618->192.168.216.13:10250: use of closed network connection
	# which will result in oc/kubectl get pod (or much of anything else) returning
	# effectively an empty status.  So we want to make sure that our call to get
	# pod status only comes after a successful
	until (( "$(____OC get pod -A --no-headers 2>/dev/null | wc -l)" > 10)) ; do
	    get_run_failed && return 1
	    sleep 5
	done
	sleep 5
    done
    # Tell the pod monitor to stop.
    if [[ -n "${cb_tempdir:-}" && -d "$cb_tempdir" ]] ; then
	touch "$cb_tempdir/___run_complete"
    fi
    local -i i
    for ((i = 0; i < 5; i++)) ; do
	run_on_sync_n sh -c "rm -f '$sync_flag_file'" && return
	sleep 5
    done
    timestamp <<< "Info: Can't terminate sync" 1>&2
}

function _get_sync_logs_poll() {
    trap '' TERM
    local pod
    local status
    pod_status_found=0
    while : ; do
	# shellcheck disable=SC2086
	status=$(__OC get pod "${sync_pod[@]}" -o jsonpath="{.status.phase}" 2>/dev/null)
	case "$status" in
	    Running)
		break
		;;
	    Pending)
		pod_status_found=1
		sleep 1
		;;
	    Error|Failed|OOMKilled)
		pod_status_found=1
		set_run_failed "Status of pod $pod failed!"
		return 1
		;;
	    ''|Succeeded|Completed)	# Might be an old pod not yet deleted
		if ((pod_status_found)) ; then
		    echo "Unexpected status '$status' of pod $pod!" 1>&2
		    return 1
		else
		    sleep 1
		fi
		;;
	    *)
		pod_status_found=1
		echo "Unexpected status '$status' of pod $pod!" 1>&2
		return 1
		;;
	esac
    done
    # shellcheck disable=SC2086
    local mypid=$BASHPID
    monitor_pods -A -l "${basename}-monitor=$xuuid" &
    # shellcheck disable=SC2086
    if supports_reporting "$requested_workload" ; then
	# Ensure that if there's a lot of output that we don't remove the sync file before it has all been flushed out
	run_logger -p "$mypid" -- _log_helper -e _fail_helper -- &
    fi
    trap 'kill $(jobs -p) 2>/dev/null 1>&2 && wait' USR2
    # Wait for either the monitor or the logger to exit and terminate
    # the other one.
    wait -n
    # shellcheck disable=SC2046
    kill $(jobs -p) 2>/dev/null
    wait
    return 0
}

function get_sync_logs_poll() {
    local tfile
    tfile=$(mktemp ${cb_tempdir:+-p "$cb_tempdir"} -t '_clusterbusterlogs.XXXXXXXXXX') || fatal "Cannot create temporary file"
    trap '(retrieve_successful_logs=1 _retrieve_artifacts 1); if [[ -n "${tfile:-}" && -f "$tfile" ]] ; then rm -f "$tfile"; unset tfile; fi; echo "Status: Fail"; return 1' INT TERM HUP USR1
    _get_sync_logs_poll > "$tfile"
    local status=$?
    if ((status == 0)) ; then
	local results
	results="$(cat "$tfile")"
	if [[ -z "$results" ]] ; then results='{}'; fi
	echo -n '"Results": '"$results"
    else
	echo "Collecting error report" 1>&2
	echo -n '"Results": {}'
    fi
    if [[ -n "${tfile:-}" && -f "$tfile" ]] ; then
	rm -f "$tfile"
    fi
    unset tfile
    return $status
}

function __report_one_volume() {
    local volspec=$1
    local emptyvolid=$2
    local -a args
    IFS=: read -ra args <<< "$volspec"
    local name=${args[0]}
    local type=${args[1],,}
    if [[ ($type = emptydisk && $runtime_class != vm) ||
	      ($type = emptydir && $runtime_class = vm) ]] ; then
	return 0
    fi
    local mountpoint=${args[2]}
    args=("${args[@]:3}")
    [[ $type = pvc ]] && type=persistentvolumeclaim
    cat <<EOF
 {
  "name": "name",
  "mount_path": "mountpoint",
  "type": "$type",
  "options": {
EOF
    local opt
    local value
    local -a all_options
    for arg in "${args[@]}" ; do
	IFS='=' read -r opt value <<< "$arg"
	all_options+=("\"$opt\": \"$value\"")
    done
    (IFS=$','; echo "${options[*]}")
    cat <<EOF
  }
 }
EOF
    if [[ $type = 'empty'* ]] ; then
	return 2
    else
	return 1
    fi
}

function _report_volumes() {
    echo '"volumes": ['
    local vol
    local -a vols=()
    local -i emptyvolid=0
    for volspec in "${volumes[@]}" ; do
	vol="$(__report_one_volume "$volspec" "$emptyvolid")"
	[[ $? = 2 ]] && emptyvolid=$((emptyvolid+1))
	if [[ -n "$vol" ]] ; then
	    vols+=("$vol")
	fi
    done
    (IFS=$',\n'; echo "${vols[*]}")
    echo '],'
}

function quote_list() {
    local -a a=("$@")
    a=("${a[@]//\"/\\\"}")
    a=("${a[@]//\$'\n'/ }")
    a=("${a[@]/#/\"}")
    a=("${a[@]/%/\"}")
    (IFS=$',\n '; echo "${a[*]}")
}

function _report_pin_nodes() {
    local -a nodes
    local class
    for class in "${!pin_nodes[@]}" ; do
	nodes+=("\"$class\": \"${pin_nodes[$class]}\"")
    done
    (IFS=$',\n '; echo "${nodes[*]}")
}

function _report_runtime_classes() {
    local -a runtimeclasses
    local class
    if [[ $deployment_type = "vm" ]] ; then
	runtimeclasses=("\"default\": \"vm\"")
    else
	for class in "${!runtime_classes[@]}" ; do
	    runtimeclasses+=("\"$class\": \"${runtime_classes[$class]}\"")
	done
    fi
    (IFS=$',\n '; echo "${runtimeclasses[*]}")
}

function _extract_metrics() {
    if supports_metrics && [[ -r "$metrics_file" ]] ; then
	report_if_desired "Waiting 60 seconds for metrics data collection to complete" 1>&2
	sleep 60
	report_if_desired "Retrieving system metrics data" 1>&2
	cat <<EOF
"metrics": $("${__topdir__}/prom-extract" --indent= --define "namespace_re=${basename}-.*" -m "$metrics_file" -s "$metrics_interval" --metrics-only --start_time="$((prometheus_starting_timestamp - metrics_epoch))" --post-settling-time="$((metrics_epoch*2))"),
EOF
    fi
}

function _workload_reporting_class() {
    if supports_api -w "$requested_workload" workload_reporting_class ; then
	call_api -w "$requested_workload" workload_reporting_class
    else
	echo "$requested_workload"
    fi
}

function _report_liveness_probe() {
    cat <<EOF
"liveness_probe_frequency": $liveness_probe_frequency,
"liveness_probe_sleep_time": $liveness_probe_sleep_time,
EOF
}

function _get_csv_version() {
    local product
    local namespace=
    local field='.spec.version'
    local OPTARG
    local OPTIND=0
    local opt
    while getopts 'n:f:' opt "$@" ; do
	case "$opt" in
	    n) namespace=$OPTARG ;;
	    f) field=$OPTARG	 ;;
	    *)			 ;;
	esac
    done
    shift $((OPTIND-1))
    local product=$1
    namespace=${namespace:-openshift-${product}}
    if namespace_exists "$namespace" ; then
	local version
	version=$(____OC get csv -n "$namespace" -o jsonpath="{.items[0]${field}}" 2>/dev/null)
	if [[ -n "$version" ]] ; then
	    cat <<EOF
"${product}_version": "${version:-}",
EOF
	fi
    fi
}

function _get_kata_version() {
    if [[ -n "$(____OC get kataconfig --no-headers 2>/dev/null)" ]] ; then
	local node
	while read -r node ; do
	    local answer
	    answer=$(____OC debug "$node" -- chroot /host sh -c "rpm -q --queryformat='%{NAME}-%{VERSION}-%{RELEASE}.%{ARCH}'$'\n' kata-containers" 2>/dev/null | head -1 |grep -v 'is not installed')
	    if [[ -n "$answer" ]] ; then
		cat <<EOF
"kata_containers_version": "$answer",
EOF
		return
	    fi
	done <<< "$(____OC get node -oname --no-headers 2>/dev/null)"
    fi
}

function _report_metadata_and_objects() {
    local status=$1
    local enddate=
    enddate=$(date +%s.%N)
    cat <<EOF
"Status": "$status",
"metadata": {
 "kind": "clusterbusterResults",
 "controller_first_start_timestamp": $first_start_timestamp,
 "prometheus_starting_timestamp": $prometheus_exact_starting_timestamp,
 "controller_second_start_timestamp": $second_start_timestamp,
 "controller_end_timestamp": $enddate,
 "controller_elapsed_time": $(bc <<< "$enddate - ${second_start_timestamp:-0}"),
 "cluster_start_time": "$(readable_timestamp "${prometheus_starting_timestamp}")",
 "job_name": "$job_name",
 "uuid": "$uuid",
 "workload": "$requested_workload",
 "workload_reporting_class": "$(_workload_reporting_class)",
 "kubernetes_version": $(indent_hang 1 __OC version -ojson),
 "command_line": [$(quote_list "${saved_argv[@]}")],
 "expanded_command_line": [$(quote_list "${processed_options[@]}")],
 "run_host": "$(hostname -f)",
 "workload_metadata": {$(indent_hang 1 generate_workload_metadata)},
 "controller_presync_timestamp": $first_local_ts,
 "sync_timestamp": $remote_ts,
 "controller_postsync_timestamp": $second_local_ts,
 "artifact_directory": "$artifactdir",
$(indent 1 _get_csv_version -n openshift-sandboxed-containers-operator kata)
$(indent 1 _get_kata_version)
$(indent 1 _get_csv_version cnv)
$(indent 1 _get_csv_version -f .spec.labels.full_version -n openshift-storage odf)
 "options": {
  "basename": "$basename",
  "containers_per_pod": $containers_per_pod,
  "deployments_per_namespace": $deps_per_namespace,
  "namespaces": $namespaces,
  "bytes_transfer": $bytes_transfer,
  "bytes_transfer_max": $bytes_transfer_max,
  "workload_run_time": $workload_run_time,
  "workload_run_time_max": $workload_run_time_max,
  "headless_services": $headless_services,
  "drop_cache": $drop_node_cache,
  "always_drop_cache": $drop_all_node_cache,
  "pin_nodes": {$(_report_pin_nodes)},
$(indent 2 _report_volumes)
$(indent 2 _report_liveness_probe)
  "secrets": $secrets,
  "replicas": $replicas,
  "container_image": "$container_image",
  "node_selector": "$node_selector",
$(indent 2 container_resources_json)
  "runtime_classes": {$(_report_runtime_classes)},
  "target_data_rate": $target_data_rate,
  "workload_options": {
$(indent 3 call_api -w "$requested_workload" "report_options")
  }
 }
},
$(indent 1 _extract_metrics)
"nodes": $(__OC get nodes -ojson 2>/dev/null | jq -c .),
"api_objects": $(__OC get all -A -l "${basename}-id=$uuid" -ojson 2>/dev/null |jq -c .items?),
"csvs": $(__OC get csv -A -ojson 2>/dev/null | jq -r '[foreach .items[]? as $item ([[],[]];0; {name: $item.metadata.name, namespace: $item.metadata.namespace, version: $item.spec.version})]')
EOF
}

function _report_failure() {
    cat <<EOF
{
 "Results": {},
$(indent 1 _report_metadata_and_objects "$failure_status")
}
EOF
}

function _get_sync_logs() {
    local first_local_ts
    local remote_ts
    local second_local_ts
    local status=$failure_status
    local do_retrieve_artifacts=1
    protect_pids "$BASHPID"
    read -r first_local_ts remote_ts second_local_ts <<< "$(get_pod_and_local_timestamps "${sync_pod[@]}")"
    if [[ -z "$remote_ts" ]] ; then
	killthemall "Unable to retrieve sync timestamp"
    fi
    local -i reported=0
    local tfile1=
    local tfile2=
    local -i retval=1
    trap 'if [[ -n "$tfile2" && -f "$tfile2" ]] ; then rm -f "$tfile1" "$tfile2" ; fi; if ((! reported)) ; then _report_failure; reported=1; fi; retrieve_successful_logs=1 _retrieve_artifacts 2; return 1' TERM
    tfile1=$(mktemp ${cb_tempdir:+-p "$cb_tempdir"} -t '_clusterbusterlogs1.XXXXXXXXXX') || fatal "Cannot create temporary file"
    tfile2=$(mktemp ${cb_tempdir:+-p "$cb_tempdir"} -t '_clusterbusterlogs2.XXXXXXXXXX') || fatal "Cannot create temporary file"
    "$@" > "$tfile1" && {
	status=Success
	do_retrieve_artifacts=
	retval=0
    }
    report_if_desired "Generating run data" 1>&2
    cat > "$tfile2" <<EOF
{
$(indent 1 < "$tfile1"),
$(indent 1 _report_metadata_and_objects "$status")
}
EOF
    ((reported)) || cat "$tfile2"
    reported=1
    rm -f "$tfile1" "$tfile2"
    # shellcheck disable=SC2086
    _retrieve_artifacts $do_retrieve_artifacts || return 1
    return $retval
}

function _describe_entity() {
    local namespace=$1
    local name=$2
    local entitytype=${3:-pod}
    local namedesc="$artifactdir/Describe/${3:+$3+}${namespace}:${name}"
    if __OC describe "${entitytype}" -n "$namespace" "$name" > "${namedesc}.tmp" ; then
	mv "${namedesc}.tmp" "$namedesc"
	[[ -s "$namedesc" ]] || echo "Warning: empty $entitytype description for ${namespace}:${name}" 1>&2
    else
	echo "Unable to retrieve $entitytype description for ${namespace}:${name}" 1>&2
	rm -f "${namedesc}.tmp"
	false
    fi
}

function _log_container() {
    local namespace=$1
    local pod=$2
    local container=$3
    local clog="$artifactdir/Logs/${namespace}:${pod}:${container}"
    if __OC logs -n "$namespace" "$pod" -c "$container" > "${clog}.tmp" ; then
	mv "${clog}.tmp" "$clog"
	[[ -s "$clog" ]] || echo "Warning: empty logs for ${namespace}:${pod}:${container}" 1>&2
    else
	echo "Unable to retrieve container log for ${namespace}:${pod}:${container}" 1>&2
	rm -f "${clog}.tmp"
	false
    fi
}

function _retrieve_vm_xml() {
    local namespace=$1
    local vm=$2
    local clog="$artifactdir/VMXML/${namespace}_${vm}.xml"
    if __OC exec -n "$namespace" "$(__OC get pod -n "$namespace" -l "vm.kubevirt.io/name=$vm" '-ojsonpath={.items[0].metadata.name}')" -- /bin/sh -c "cat '/var/run/libvirt/qemu/run/${namespace}_${vm}.xml'" > "${clog}.tmp" ; then
	mv "${clog}.tmp" "$clog"
	[[ -s "$clog" ]] || echo "Warning: empty qemu XML file for ${namespace}:${vm}" 1>&2
    else
	echo "Unable to retrieve qemu XML file for ${namespace}:${vm}" 1>&2
	rm -f "${clog}.tmp"
	false
    fi
}

function _retrieve_artifacts() {
    local always_retrieve_artifacts=${1:-}
    get_run_started || return 1
    get_run_failed && always_retrieve_artifacts=1
    trap 'echo "Interrupt received; aborting" 1>&2; exit' INT TERM HUP
    if [[ -n "$artifactdir" && ! -f "$artifactdir/.artifacts" ]] ; then
	report_if_desired "Retrieving run artifacts" 1>&2
	mkdir -p "$artifactdir/Logs"
	mkdir -p "$artifactdir/Describe"
	touch "$artifactdir/.artifacts"
	local -A jobs_pending
	local job
	local -A pods_described=()
	local -A containers_logged=()
	local namespace
	local pod
	local container
	local status
	while read -r namespace pod container status; do
	    if [[ ($namespace != "$sync_namespace" || $sync_in_first_namespace -ne 0) &&
		      -z $always_retrieve_artifacts &&
		      $retrieve_successful_logs -eq 0 &&
		      ($status = Running || $status = Completed || $status = Succeeded) ]] ; then
		continue
	    fi
	    local podname="${namespace}:${pod}"
	    local name="${podname}:${container}"
	    if [[ -z "${pods_described[$podname]:-}" ]] ; then
		if ((parallel_log_retrieval > 1)) ; then
		    _describe_entity "$namespace" "$pod" &
		    jobs_pending[$!]=$podname
		else
		    _describe_entity "$namespace" "$pod"
		fi
		pods_described[$podname]=1
	    fi
	    if [[ -z "${containers_logged[$name]:-}" ]] ; then
		if ((parallel_log_retrieval > 1)) ; then
		    _log_container "$namespace" "$pod" "$container" &
		    jobs_pending[$!]=$name
		else
		    _log_container "$namespace" "$pod" "$container"
		fi
		containers_logged[$name]=1
	    fi
	    if (( ${#jobs_pending[@]} > parallel_log_retrieval )) ; then
		local -a jobs_to_wait=("${!jobs_pending[@]}")
		for job in "${jobs_to_wait[@]}" ; do
		    wait "$job" 2>/dev/null
		    unset "jobs_pending[$job]"
		done
	    fi
	done <<< "$(__OC get pod -A -l "${basename}-id=$uuid" -ojson 2>/dev/null | jq -r '[foreach .items[]? as $item ([[],[]];0; (if ($item.kind == "Pod") then ([foreach $item.spec.containers[]? as $container ([[],[]];0; $item.metadata.namespace + " " + $item.metadata.name + " " + $container.name + " " + $item.status.phase)]) else null end))] | flatten | map (select (. != null))[]')"
	local -a jobs_to_wait=("${!jobs_pending[@]}")
	for job in "${jobs_to_wait[@]}" ; do
	    wait "$job" 2>/dev/null
	    unset "jobs_pending[$job]"
	done
	if [[ $deployment_type = vm ]] ; then
	    [[ ! -d "$artifactdir/VMXML" ]] && mkdir "$artifactdir/VMXML"
	    local vm
	    local entitytype
	    local ignore
	    for entitytype in vm vmi ; do
		jobs_to_wait=()
		# shellcheck disable=SC2034
		while read -r namespace vm ignore ; do
		    if ((parallel_log_retrieval > 1)) ; then
			_describe_entity "$namespace" "$vm" "$entitytype"&
			jobs_pending[$!]=$podname
		    else
			_describe_entity "$namespace" "$vm" "$entitytype"
		    fi
		    # Don't try to parallelize something that exec's into pods
		    if [[ $entitytype = vm ]] ; then
			_retrieve_vm_xml "$namespace" "$vm"
		    fi
		    pods_described[$name]=1
		    if (( ${#jobs_pending[@]} > parallel_log_retrieval )) ; then
			local -a jobs_to_wait=("${!jobs_pending[@]}")
			for job in "${jobs_to_wait[@]}" ; do
			    wait "$job" 2>/dev/null
			    unset "jobs_pending[$job]"
			done
		    fi
		done <<< "$(__OC get "${entitytype}" -A -l "${basename}-id=$uuid" --no-headers 2>/dev/null)"
	    done
	fi
	if [[ $deployment_type = vm && -f "$VIRTCTL" && -f "$vm_ssh_keyfile" ]] ; then
	    [[ ! -d "$artifactdir/VMLogs" ]] && mkdir "$artifactdir/VMLogs"
	    while read -r namespace vm ; do
		# virtctl scp assumes that any colons in the destination filename are remote.
		____VIRTCTL scp -i "$vm_ssh_keyfile" -t "-oBatchMode=yes" -t "-oStrictHostKeyChecking=no" -t "-oGlobalKnownHostsFile=/dev/null" -t "-oUserKnownHostsFile=/dev/null" -t "-oLogLevel ERROR" "root@$vm.$namespace:/var/log/cloud-init-output.log" "$artifactdir/VMLogs/$namespace.$vm" </dev/null 1>&2 && mv "$artifactdir/VMLogs/$namespace.$vm" "$artifactdir/VMLogs/$namespace:$vm"
	    done <<< "$(__OC get vm -A -l "${basename}-id=$uuid" --no-headers | awk '{print $1, $2}')"
	fi
    fi
}

function print_report() {
    trap '' TERM
    if [[ $report_format = raw ]] ; then
	cat
    else
	"${pathdir:-.}/clusterbuster-report" ${report_format:+-o "$report_format"}
    fi
}

function get_sync_logs() {
    trap - TERM
    protect_pids "$BASHPID"
    log_cmd=get_sync_logs_poll
    if supports_reporting "$requested_workload"; then
	if [[ -n "$artifactdir" ]] ; then
	    # Even if reporting fails, make sure we capture the JSON
	    _get_sync_logs "$log_cmd" > "$artifactdir/clusterbuster-report.tmp.json"
	    status=$?
	    mv "$artifactdir/clusterbuster-report.tmp.json" "$artifactdir/clusterbuster-report.json" &&
		print_report < "$artifactdir/clusterbuster-report.json"
	    return $status
	else
	    trap '' TERM
	    _get_sync_logs "$log_cmd" | print_report
	    return $((PIPESTATUS[0] || PIPESTATUS[1]))
	fi
    else
	echo "Workload $requested_workload does not support reporting" 1>&2
    fi
}

################################################################
# YAML fragment generation
################################################################

function privileged_security_context_content() {
    cat <<'EOF'
privileged: true
runAsUser: 0
EOF
}

function restricted_security_context_content() {
    cat <<'EOF'
allowPrivilegeEscalation: False
capabilities:
  drop:
  - ALL
runAsNonRoot: True
seccompProfile:
  type: RuntimeDefault
EOF
}

function default_security_context_content() {
    if ((create_pods_privileged)) ; then
	privileged_security_context_content
    else
	restricted_security_context_content
    fi
}

function sysctls_security_context() {
    get_sysctls
    if [[ -n "${__sysctls[*]}" ]] ; then
	echo "sysctls:"
	local key=
	for key in "${!__sysctls[@]}" ; do
	    echo "- name: $key"
	    echo "  value: ${__sysctls[$key]}"
	done
    fi
}

function default_security_context() {
    cat <<EOF
securityContext:
$(indent 2 default_security_context_content)
$(indent 2 sysctls_security_context)
EOF
}

function privileged_security_context() {
    cat <<EOF
securityContext:
$(indent 2 privileged_security_context_content)
$(indent 2 sysctls_security_context)
EOF
}

function restricted_security_context() {
    cat <<EOF
securityContext:
$(indent 2 restricted_security_context_content)
$(indent 2 sysctls_security_context)
EOF
}

function indent() {
    local -i column="$1"
    local -a cmd
    shift
    local str
    str=$(printf "%${column}s" '')
    if [[ -n "${hang_indent:-}" ]] ; then
	# shellcheck disable=SC2016
	cmd=(awk '/./ {if (a == 0) {print} else {print "'"$str"'", $0}; a = 1}')
    else
	cmd=(sed -e '/^$/d' -e "s/^/${str}/")
    fi
    if [[ -n "$*" ]] ; then
	"$@" | "${cmd[@]}"
    else
	"${cmd[@]}"
    fi
}

function indent_hang() {
    hang_indent=1 indent "$@"
}

function mkpodname() {
    local podname
    podname="${pod_prefix}$(IFS=-; echo "$*")"
    echo "${podname//_/-}"
}

function get_pin_node() {
    if [[ -n "${1:-}" ]] ; then
	if [[ -n "${pin_nodes[$1]:-}" ]] ; then
	    echo "${pin_nodes[$1]}"
	elif [[ -n "${pin_nodes[default]:-}" ]] ; then
	    echo "${pin_nodes[default]}"
	fi
    fi
}

function get_net_interface() {
    if [[ -n "${1:-}" ]] ; then
	if [[ -n "${net_interfaces[$1]:-}" ]] ; then
	    echo "${net_interfaces[$1]:-}"
	else
	    echo "${net_interfaces[default]:-}"
	fi
    fi
}

function get_net_interface_external() {
    local net
    net=$(get_net_interface "${1:-}")
    echo "${net%%:*}"
}

function get_net_interface_local() {
    local net
    net=$(get_net_interface "${1:-}")
    if [[ -n "$net" ]] ; then
	if [[ $net = *":"* ]] ; then
	    echo "${net#*:}"
	elif [[ ${deployment_type:-} = vm ]] ; then
	    echo "$default_net_interface_vm"
	else
	    echo "$default_net_interface_pod"
	fi
    fi
}

function tolerations_yaml() {
    local toleration
    (( ${#tolerations[@]} )) || return
    echo "tolerations:"
    for toleration in "${tolerations[@]}" ; do
	# shellcheck disable=SC2206
	local -a tol=(${toleration//:/ })
	cat <<EOF
- key: "${tol[0]:-}"
  operator: "${tol[1]:-}"
  effect: "${tol[2]:-}"
EOF
    done
}

function _resources_yaml() {
    local token
    local -A resource_map=()
    local resource
    local value
    # Remove duplicate resources
    for token in "$@" ; do
	resource=${token%%=*}
	value=${token#*=}
	resource_map[$resource]=$value
    done
    for resource in "${!resource_map[@]}" ; do
	echo "$resource: ${resource_map[$resource]}"
    done
}

function container_resources_yaml() {
    if (( ${#resource_limits[@]} + ${#resource_requests[@]} )) ; then
	echo "resources:"
	if (( ${#resource_limits[@]} )) ; then
	    echo "  limits:"
	    indent 4 _resources_yaml "${resource_limits[@]}"
	fi
	if (( ${#resource_requests[@]} )) ; then
	    echo "  requests:"
	    indent 4 _resources_yaml "${resource_requests[@]}"
	fi
    fi
}

function liveness_probe_yaml() {
    if ((liveness_probe_frequency > 0)) ; then
	cat <<EOF
livenessProbe:
  exec:
    command:
    - /bin/sleep "$liveness_probe_sleep_time"
  initialDelaySeconds: 10
  periodSeconds: $liveness_probe_frequency
EOF
    fi
}

function mk_selector() {
    local selector=$1
    local key=$selector
    local value=''
    if [[ $selector = *'='* ]] ; then
	key=${selector%%=*}
	value=${selector#*=}
    fi
    echo "nodeSelector:${nl}  $key: \"$value\""
}

function mk_yaml_args() {
    local OPTARG
    local OPTIND=0
    local arg
    local vm_mode=0
    args=()
    while getopts 'Vs' opt "$@" ; do
	case "$opt" in
            V) vm_mode=1 ;;
	    s)
		while read -r arg ; do
                    args+=("$arg")
		done
		;;
	    *)
		;;
	esac
    done
    if [[ ${ARGLIST_FORMAT:-} = vm ]] ; then vm_mode=1; fi
    shift $((OPTIND-1))
    args+=("$@")
    if ((vm_mode)) ; then
        IFS=' ' echo "${args[*]@Q}"
    else
        for arg in "$@" ; do
	    if [[ $arg =~ $'\n' ]] ; then
	        echo "- |"$'\n'"  ${arg//$'\n'/$'\n  '}"
	    else
	        echo "- \"$arg\""
	    fi
        done
    fi
}

function create_standard_containers() {
    local OPTARG
    local OPTIND=0
    local opt
    local image="$container_image"
    local replica=0
    while getopts 'i:R:' opt "$@" ; do
	case "$opt" in
	    i) image="$OPTARG"  ;;
	    R) replica="$OPTARG";;
	    *)		        ;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    local instance=$2
    local secret_count=$3
    local replicas=$4
    local containers=$5
    local -i container
    local sync_service=
    local sync_port_num=
    local sync_ns_port_num=
    local drop_cache_service=
    local drop_cache_port_num=
    IFS=: read -r sync_service sync_port_num sync_ns_port_num <<< "$(get_sync)"
    IFS=: read -r drop_cache_service drop_cache_port_num <<< "$(get_drop_cache "$namespace" "$instance" "$replica")"
    for ((container = 0; container < containers; container++)) ; do
	cat <<EOF
- name: "c${container}"
  imagePullPolicy: $image_pull_policy
  image: "$image"
$(indent 2 container_standard_auxiliary_yaml)
$(indent 2 standard_environment)
$(indent 2 generate_environment)
  command:
$(if [[ -n "${arglist_function}" ]] ; then
     "${arglist_function}" "${system_configmap_mount_dir}/" "$@" "$container" -- \
     "$sync_nonce" "$namespace" "c$container" "$basetime" "$baseoffset" "$(ts)" "$exit_at_end" "$sync_service" \
     "$sync_port_num" "$sync_ns_port_num" "$sync_watchdog_port_num" "$sync_watchdog_timeout" \
     "$drop_cache_service" "$drop_cache_port_num" | indent 2 ; fi)
$(indent 2 volume_mounts_yaml "$namespace" "${instance}" "$secret_count")
$(indent 2 <<< "${security_context:-}")
EOF
    done
}

function container_standard_auxiliary_yaml() {
    container_resources_yaml
    liveness_probe_yaml
}

function resources() {
    local token
    for token in "$@" ; do
	local resource=${token%%=*}
	local value=${token#*=}
	if [[ ! $value =~ ^-?[[:digit:]]+(\.[[:digit:]]+)?$ ]] ; then
	    value="\"$value\""
	fi
	echo "\"$resource\": $value"
    done
}

function resource_json() {
    local request_type=$1
    shift
    echo "    \"$request_type\": {"
    local -A resource_map=()
    local token
    local resource
    local value
    # Remove duplicate resources
    for token in "$@" ; do
	resource=${token%%=*}
	resource_map[$resource]=$token
    done
    local -a resources=()
    for resource in "${resource_map[@]}" ; do
	resources+=("$(resources "$resource")")
    done
    (IFS=$',\n        '; echo "${resources[*]}")
    echo "    }"
}

function container_resources_json() {
    local -a res_data=()
    if (( ${#resource_limits[@]} + ${#resource_requests[@]} )) ; then
	echo '"resources": {'
	if (( ${#resource_limits[@]} )) ; then
	    res_data+=("$(resource_json limits "${resource_limits[@]}")")
	fi
	if (( ${#resource_requests[@]} )) ; then
	    res_data+=("$(resource_json requests "${resource_requests[@]}")")
	fi
	(IFS=$',\n    '; echo "${res_data[*]}")
	echo "},"
    fi
}

function standard_labels_yaml() {
    local objtype=worker
    local objclass=
    local OPTIND=0
    local logger=
    local suffix=
    local -i worker_label=1
    while getopts 'St:ls:c:' opt "$@" ; do
	case "$opt" in
	    S) worker_label=0	 ;;
	    t) objtype="$OPTARG" ;;
	    l) logger=true	 ;;
	    s) suffix="$OPTARG"  ;;
	    c) objclass="$OPTARG";;
	    *)			 ;;
	esac
    done
    objclass=${objclass:-$objtype}
    shift $((OPTIND-1))
    local workload=${1:-}
    local namespace=${2:-}
    local instance=${3:-}
    local replica=${4:-}
    # Different versions of bash expand quotes inside the ${logger:+...} differently.
    # true and "true" are interpreted differently.
    local true='"true"'
    cat <<EOF
labels:
  ${basename}-xuuid: "$xuuid"
  ${basename}-uuid: "$uuid"
  ${basename}-id: "$uuid"
  ${basename}: "true"
  ${basename}base: "true"
${objtype:+  ${basename}-${objtype}: "$uuid"}
${objtype:+  ${basename}-x-${objtype}: "$xuuid"}
${logger:+  ${basename}-logger: $true}
  ${basename}-objtype: "$objtype"
EOF
    if [[ $objtype = worker || $objtype = sync ]] ; then
	cat <<EOF
  ${basename}-monitor: "$xuuid"
EOF
    fi
    if [[ $basename != clusterbuster ]] ; then
	cat <<EOF
  clusterbusterbase: "true"
  clusterbuster-objtype: "$objtype"
EOF
    fi
    if ((worker_label)) ; then
	cat <<EOF
  ${basename}-workload: "true"
EOF
	if [[ $basename != clusterbuster ]] ; then
	cat <<EOF
  clusterbuster-workload: "true"
EOF
	fi
    fi
    if [[ -n "$workload" ]] ; then
	cat <<EOF
  ${basename}-${workload}: "$uuid"
  name: "${namespace}${workload:+-$workload}${instance:+-$instance}${suffix:+-$suffix}"
  app: "${namespace}${workload:+-$workload}${suffix:+-$suffix}"
  k8s-app: "${namespace}${workload:+-$workload}${suffix:+-$suffix}"
  instance: "${namespace}${workload:+-$workload}${instance:+-$instance}${suffix:+-$suffix}"
  replica: "${namespace}${workload:+-$workload}${instance:+-$instance}${replica:+-$replica}${suffix:+-$suffix}"
  ${workload}: "true"
EOF
    fi
    for label in "${pod_labels[@]}" ; do
	if [[ $label =~ ^:([^:]*):(.*)$ ]] ; then
	    # shellcheck disable=SC2206
	    local -a classes=(${BASH_REMATCH[1]//,/ })
	    label=${BASH_REMATCH[2]:-}
	    local class
	    local -i class_found=0
	    for class in "${classes[@]}" ; do
		if [[ $objclass = "$class" ]] ; then
		    class_found=1
		    break
		fi
	    done
	    if (( ! class_found )) ; then
		continue
	    fi
	fi
	if [[ $label = *'='* ]] ; then
	    echo "  ${label%=*}: \"${label##*=}\""
	else
	    echo "  ${label}: \"true\""
	fi
    done
}

function standard_environment() {
    local vm_mode=0
    local vm_container_mode=0
    local OPTIND=0
    while getopts VC opt "$@" ; do
	case "$opt" in
	    V) vm_mode=1 ;;
	    C) vm_container_mode=1 ;;
	    *)		 ;;
	esac
    done
    if ((vm_container_mode)) ; then
	echo -n "-e VERBOSE='$verbose' -e SYSTEM_PODFILE_DIR='$system_configmap_mount_dir' -e USER_PODFILE_DIR='$user_configmap_mount_dir' -e PYTHONPATH='$system_configmap_mount_dir' -e __CB_HOSTNAME=\$(hostname -s)"
    elif ((vm_mode)) ; then
	echo -n "VERBOSE='$verbose' SYSTEM_PODFILE_DIR='$system_configmap_mount_dir' USER_PODFILE_DIR='$user_configmap_mount_dir' PYTHONPATH='$system_configmap_mount_dir'"
    else
	cat <<EOF
env:
- name: VERBOSE
  value: "$verbose"
- name: SYSTEM_PODFILE_DIR
  value: "$system_configmap_mount_dir"
- name: USER_PODFILE_DIR
  value: "$user_configmap_mount_dir"
- name: PYTHONPATH
  value: "$system_configmap_mount_dir"
EOF
    fi
}

function expand_volume() {
    local scoped_name=$1
    local namespace=${2:+$2}
    local instance=${3:+$3}
    local replica=${4:+$4}
    scoped_name=${scoped_name//%N/$namespace}
    scoped_name=${scoped_name//%i/$instance}
    scoped_name=${scoped_name//%r/$replica}
    echo "$scoped_name"
}

function volume_mounts_yaml() {
    local OPTIND=0
    local use_extra_volumes=1
    while getopts V opt "$@" ; do
	case "$opt" in
	    V) use_extra_volumes=0 ;;
	    *)			   ;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    local instance=${2:-1}
    local secrets=${3:-1}
    (( secrets + ${#configmap_files[@]} + ${#volumes[@]} + has_system_configmap + has_user_configmap )) || return;
    local -i i
    echo volumeMounts:
    for ((i = 0; i < secrets; i++)) ; do
	local name="secret-${namespace}-${instance}-$i"
	cat <<EOF
- name: $name
  mountPath: /etc/$name
  readOnly: true
EOF
    done
    if [[ ($namespace != "$sync_namespace" || $sync_in_first_namespace -ne 0) && $has_user_configmap -ne 0 ]] ; then
	cat <<EOF
- name: "userconfigmap-${namespace}"
  mountPath: "${user_configmap_mount_dir}"
  readOnly: true
EOF
    fi
    if (( has_system_configmap )) ; then
	cat <<EOF
- name: "systemconfigmap-${namespace}"
  mountPath: "${system_configmap_mount_dir}"
  readOnly: true
EOF
    fi
    if ((use_extra_volumes)) ; then
	local volspec
	local -i emptyvolid=0
	for volspec in "${volumes[@]}" ; do
	    local -a args
	    IFS=: read -ra args <<< "$volspec"
	    local name=${args[0]}
	    local type=${args[1]}
	    local mountpoint=${args[2]}
	    name=$(expand_volume "$name" "$namespace" "$instance" "$replica")

	    args=("${args[@]:3}")
	    case "${type,,}" in
		emptydir)
		    name=cbemptydir$((emptyvolid++))
		    cat <<EOF
- name: $name
  mountPath: "$mountpoint"
EOF
		    ;;
		pvc|persistentvolumeclaim)
		    cat <<EOF
- name: $name
  mountPath: "$mountpoint"
EOF
		    ;;
		*)
		    continue
		    ;;
	    esac
	done
    fi
}

function list_volume_mount_points() {
    local volspec
    for volspec in "${volumes[@]}" ; do
	local name
	local type
	local mountpoint
	local args
	read -r name type mountpoint <<< "$volspec"
	if [[ ($type = emptydisk && $runtime_class != vm) ||
		  ($type = emptydir && $runtime_class = vm) ]] ; then
	    continue
	fi
	echo "$mountpoint"
    done
}

function namespace_yaml() {
    local namespace=$1
    (( use_namespaces )) && echo "namespace: \"$namespace\""
}

function annotations_yaml() {
    [[ -z "${pod_annotations[*]}" ]] && return
    local desired_class=${1:-client}
    local annotation
    echo "annotations:"
    for annotation in "${pod_annotations[@]}" ; do
	if [[ $annotation =~ ^:([^:]*):(.*)$ ]] ; then
	    # shellcheck disable=SC2206
	    local -a classes=(${BASH_REMATCH[1]//,/ })
	    annotation=${BASH_REMATCH[2]:-}
	    local class
	    local -i class_found=0
	    for class in "${classes[@]}" ; do
		if [[ $desired_class = "$class" ]] ; then
		    class_found=1
		    break
		fi
	    done
	    if (( ! class_found )) ; then
		continue
	    fi
	fi
	echo "  $annotation"
    done
}

function standard_deployment_metadata_yaml() {
    local namespace=${1:-}
    local class=${2:-client}
    namespace_yaml "$namespace"
}

function standard_pod_metadata_yaml() {
    local namespace=${1:-}
    local class=${2:-client}
    annotations_yaml "$class"
}

function extra_volumes_yaml() {
    local volspec
    local -i emptyvolid=0
    local opt
    local value
    for volspec in "${volumes[@]}" ; do
	local -a args
	IFS=: read -ra args <<< "$volspec"
	local name=${args[0]}
	local type=${args[1]}
	local mountpoint=${args[2]}
	args=("${args[@]:3}")
	case "${type,,}" in
	    pvc|persistentvolumeclaim)
		local claim_name="$name"
		local arg
		local size=
		for arg in "${args[@]}" ; do
		    IFS='=' read -r opt value <<< "$arg"
		    case "${opt,,}" in
			claimname) claim_name="$value"   	 ;;
			size)	   size="$(parse_size "$value")" ;;
			*)					 ;;
		    esac
		done
		claim_name=$(expand_volume "$claim_name" "$namespace" "$instance" "$replica")
		cat <<EOF
- name: $claim_name
  persistentVolumeClaim:
    claimName: $claim_name
${size:+    capacity: "$size"}
EOF
		;;
	    emptydisk)
		[[ ${runtime_class:-} != vm ]] && continue
		local arg
		local size=
		for arg in "${args[@]}" ; do
		    local opt
		    local value
		    IFS='=' read -r opt value <<< "$arg"
		    case "${opt,,}" in
			size)	   size="$(parse_size "$value")" ;;
			*)					 ;;
		    esac
		done
		name="cbemptydisk$((emptyvolid++))"
		cat <<EOF
- name: $name
  emptyDisk:
    capacity: "$size"
EOF
		;;
	    emptydir)
		[[ $runtime_class = vm ]] && continue
		name="cbemptydir$((emptyvolid++))"
		cat <<EOF
- name: $name
  emptydDir: {}
EOF
		;;
	    *)  ;;
	esac
    done
}

function volumes_yaml() {
    local OPTIND=0
    local -i print_volumes=-1
    local opt
    local -i use_extra_volumes=1
    while getopts 'ynV' opt ; do
	case "$opt" in
	    y) print_volumes=1     ;;
	    n) print_volumes=0	   ;;
	    V) use_extra_volumes=0 ;;
	    *)		           ;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    local instance=${2:-1}
    local secrets=${3:-1}
    (( print_volumes > 0)) && echo "volumes:"
    (( secrets + ${#configmap_files[@]} + ${#volumes[@]} + has_system_configmap + has_user_configmap )) || return;
    local -i i
    (( print_volumes < 0)) && echo "volumes:"
    for ((i = 0; i < secrets; i++)) ; do
	local name="secret-${namespace}-${instance}-$i"
	cat<<EOF
- name: $name
  secret:
    secretName: $name
EOF
    done
    if [[ ($namespace != "$sync_namespace" || $sync_in_first_namespace -ne 0) && $has_user_configmap -ne 0 ]] ; then
	cat <<EOF
- name: "userconfigmap-${namespace}"
  configMap:
    name: "userconfigmap-${namespace}"
EOF
    fi
    if (( has_system_configmap )) ; then
	cat <<EOF
- name: "systemconfigmap-${namespace}"
  configMap:
    name: "systemconfigmap-${namespace}"
EOF

    fi
    ((use_extra_volumes)) && extra_volumes_yaml
}

function create_affinity_yaml() {
    local OPTIND=0
    local OPTARG
    local _affinity="$affinity"
    local topo_key="kubernetes.io/hostname"
    local key="instance"
    local operator="In"
    local opt
    while getopts "a:k:o:t:" opt "$@" ; do
	case "$opt" in
	    a) _affinity="$OPTARG" ;;
	    k) key="$OPTARG"       ;;
	    o) operator="$OPTARG"  ;;
	    t) topo_key="$OPTARG"  ;;
	    *)                     ;;
	esac
    done
    case "${_affinity,,}" in
	0)	     return		    ;;
	1|af*|podaf) _affinity=Affinity	    ;;
	*)	     _affinity=AntiAffinity ;;
    esac
    shift $((OPTIND-1))
    if [[ -n "${1:-}" ]] ; then
	cat <<EOF
affinity:
  pod$_affinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: $key
          operator: $operator
          values:
          - "$1"
      topologyKey: $topo_key
EOF
    fi
}

################################################################
# Object YAML creation
################################################################

function _really_create_objects() {
    if [[ $doit -ne 0 && -n $accumulateddata ]] ; then
	__OC --validate=false create -f - 2>&1 <<< "$accumulateddata" || {
	    echo "Create objects failed:" 1>&2
	    echo "$accumulateddata" 1>&2
	    set_run_aborted "Failed to create objects"
	    return 1
	}
    fi
}

function really_create_objects() {
    _really_create_objects | timestamp 1>&2
    accumulateddata=
    objs_item_count=0
    return "${PIPESTATUS[0]}"
}

function create_object() {
    local OPTIND=0
    local namespace=
    local objtype=${deployment_type^}
    local objname=
    local force=0
    local record_object=0
    while getopts 'n:t:fr' opt "$@" ; do
	case "$opt" in
	    n) namespace=$OPTARG ;;
	    t) objtype=$OPTARG   ;;
	    f) force=1           ;;
	    r) record_object=1	 ;;
	    *)                   ;;
	esac
    done
    shift $((OPTIND-1))
    objname=${1:-UNKNOWN}
    local data=
    local line=
    local -i data_found=0
    while IFS='' read -r 'line' ; do
	line=${line%% }
	if [[ $line =~ [^[:space:]] ]] ; then
	    data_found=1
	    [[ -n $data ]] || data="---$nl"
	    data+="$line$nl"
	fi
    done
    if ((record_object)) ; then
	total_objects_created=$((total_objects_created+1))
	if [[ -z ${objects_created[$objtype]:-} ]] ; then
	    objects_created[$objtype]=1
	else
	    objects_created[$objtype]=$((${objects_created[$objtype]:-}+1))
	fi
    fi
    ((! data_found)) && return
    data+="$nl"
    if (( doit )) ; then
	if [[ -n "$artifactdir" ]] ; then
	    mkdir -p "$artifactdir/$objtype"
	    echo "$data" > "$artifactdir/${objtype}/${namespace:+${namespace}:}$objname"
	fi
	accumulateddata+="$data"
	if (( force || (++objs_item_count >= objs_per_call) )) ; then
	    really_create_objects
	    (( !sleeptime )) || sleep "$sleeptime"
	fi
    else
	IFS= echo "$data"
    fi
}

function report_if_desired() {
    if (( report_object_creation )) ; then
	if [[ -n "$*" ]] ; then
	    timestamp <<< "$*"
	else
	    timestamp
	fi
    else
	cat >/dev/null
    fi
}

function _ns() {
    ((use_namespaces)) && echo -n "-n $namespace"
}

function _create_object_from_file() {
    if (($# < 3)) ; then
	warn "_create_object_from_file: type, namespace, and name must be specified"
	return 1
    fi
    local objtype=$1; shift
    local namespace=$1; shift
    local objname=$1; shift
    local ns=${use_namespaces:+-n "$namespace"}
    local ns1=${use_namespaces:+"$namespace"}
    local file
    for file in "$@" ; do
	[[ -r "$file" ]] || fatal "Can't read file $file!"
    done
    # shellcheck disable=SC2046
    delete_object_safe "$objtype" $(_ns) "$objname" 2>&1
    # shellcheck disable=SC2046
    _OC create "$objtype" $(_ns) "$objname" "${@/#/--from-file=}" 2>&1 || killthemall "Unable to create object $objtype/$namespace:$objname"
    # shellcheck disable=SC2046
    _OC label "$objtype" $(_ns) "$objname" "${basename}base=true" 'clusterbusterbase=true' "${basename}-uuid=${uuid}" "${basename}-id=$uuid" "${basename}=true" 2>&1 || killthemall "Unable to label $objtype/$namespace:$objname"
    if (( doit )) ; then
	if [[ -n "$artifactdir" ]] ; then
	    mkdir -p "$artifactdir/$objtype"
	    local outfile="$artifactdir/${objtype}/${ns1}:${objname}"
	    rm -f "$outfile"
	    for file in "$@" ; do
		echo '---' >> "$outfile"
		cat "$file" >> "$outfile"
	    done
	fi
	total_objects_created=$((total_objects_created+1))
	if [[ -z ${objects_created[$objtype]:-} ]] ; then
	    objects_created[$objtype]=1
	else
	    objects_created[$objtype]=$((${objects_created[$objtype]}+1))
	fi
	(( !sleeptime )) || sleep "$sleeptime"
    fi
}

function create_object_from_file() {
    _create_object_from_file "$@" |report_if_desired 1>&2
}

function create_namespace() {
    local OPTIND=0
    local force=
    local opt=
    local policy
    local worker_label=
    policy=$(namespace_policy)
    while getopts 'Sfp:' opt "$@" ; do
	case "$opt" in
	    S) worker_label=-S	;;
	    f) force=-f		;;
	    p) policy="$OPTARG" ;;
	    *)			;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=${1:-}
    if [[ -z "$namespace" ]] ; then
	warn "create_namespace: namespace must be specified"
	return 1
    fi
    if ((use_namespaces)) ; then
	local -i ns_exists=0
	namespace_exists "$namespace" && ns_exists=1
	((ns_exists && remove_namespaces == -1)) && remove_namespaces=0
	((ns_exists)) || create_object $force -t namespace "$namespace" <<EOF 2>&1
apiVersion: v1
kind: Namespace
metadata:
  name: "${namespace}"
$(indent 2 standard_labels_yaml $worker_label -t namespace)
EOF
	_OC label namespace --overwrite "$namespace" "pod-security.kubernetes.io/enforce=$policy" 2>&1 | report_if_desired
	_OC label namespace --overwrite "$namespace" "pod-security.kubernetes.io/audit=$policy" 2>&1 | report_if_desired
	_OC label namespace --overwrite "$namespace" "pod-security.kubernetes.io/warn=$policy" 2>&1 | report_if_desired
	if [[ $policy = privileged ]] ; then
	    if ! __OC get serviceaccount -n "$namespace" "$namespace" >/dev/null 2>&1 ; then
		_OC create serviceaccount -n "$namespace" "$namespace" 2>&1 | report_if_desired
	    fi
	    _OC label serviceaccount --overwrite -n "$namespace" "$namespace" "${basename}=true" 2>&1 | report_if_desired
	    _OC adm policy add-scc-to-user -n "$namespace" privileged -z "$namespace" 2>&1 | report_if_desired
	fi
    fi
}

function create_secrets() {
    local namespace=${1:-}
    if [[ -z "$namespace" ]] ; then
	warn "create_secrets: namespace must be specified"
	return 1
    fi
    local deps_per_namespace=${2:-1}
    local secrets=${3:-1}
    local -i i
    local -i j
    (( secrets )) || return;
    for ((i = first_deployment; i < deps_per_namespace + first_deployment; i++)) ; do
	for ((j = 0; j < secrets; j++)) ; do
	    local secname="secret${namespace:+-$namespace}-${i}-${j}"
	    create_object -t secret -n "$namespace" "$secname" <<EOF 2>&1 | report_if_desired
apiVersion: v1
kind: Secret
metadata:
  name: "$secname"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml -t service)
data:
  key1: "$(base64 <<< "${namespace}X${i}Y${j}Z1")"
  key2: "$(base64 <<< "${namespace}X${i}Y${j}Z2")"
type: Opaque
EOF
	done
    done
}

function check_configmaps() {
    local errors=0
    for mapfile in "$@" ; do
	if [[ ! -r "$mapfile" ]] ; then
	    echo "Can't find configmap file $mapfile"
	    errors=$((errors+1))
	fi
    done
    if (( errors )) ; then
	exit 1
    fi
}

function create_configmaps() {
    local namespace=$1; shift
    local mapname=$1; shift
    create_object_from_file configmap "$namespace" "${mapname}-${namespace}" "$@"
}

function _create_service_ports() {
    local basename=$1
    shift
    local i
    local p
    for i in "$@" ; do
	for p in TCP UDP ; do
	    cat <<EOF
- name: ${basename}-${i}-${p,,}
  protocol: $p
  port: $i
  targetPort: $i
EOF
	done
    done
}

function create_service() {
    local OPTIND=0
    local OPTARG
    local -i force_headless_services=-1
    local headless=
    local key=name
    local prefix='svc-'
    local selector_value=
    local workload_service=1
    while getopts 'eEhHk:v:Ww' opt ; do
	case "$opt" in
	    w) workload_service=1	 ;;
	    W) workload_service=0	 ;;
	    e) prefix=			 ;;
	    E) prefix='svc-'		 ;;
	    h) force_headless_services=1 ;;
	    H) force_headless_services=0 ;;
	    k) key="$OPTARG"		 ;;
	    v) selector_value="$OPTARG"	 ;;
	    *) ;;
	esac
    done
    shift $((OPTIND-1))
    ((force_headless_services < 0)) && force_headless_services=$headless_services
    ((force_headless_services)) && headless='clusterIP: "None"'
    local namespace=$1
    local deployment=$2
    selector_value=${selector_value:-$deployment}
    shift 2
    local portnum
    if ((workload_service)) ; then
	for portnum in "$@" ; do
	    workload_service_ports[$portnum]=1
	done
    fi
    create_object -t Service -n "$namespace" "svc-${deployment}" <<EOF
apiVersion: v1
kind: Service
metadata:
  name: "${prefix}${deployment}"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml -t service)
spec:
$(indent 2 <<< "$headless")
  ports:
$(indent 4 _create_service_ports "svc-${deployment}" "$@")
  selector:
    ${key}: "${selector_value}"
EOF
}

function list_ports() {
    local portnum
    for portnum in "$@" ; do
	cat <<EOF
- port: $portnum
  name: "svc-${deployment}-${portnum}"
EOF
    done
}

function create_external_service() {
    local namespace=$1
    local deployment=$2
    local external_name=$3
    shift 3
    create_object -t Service -n "$namespace" "svc-${deployment}" <<EOF
apiVersion: v1
kind: Service
metadata:
  name: "svc-${deployment}"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml -t service)
spec:
  type: ExternalName
  externalName: $external_name
  ports:
$(indent 2 list_ports "$@")
EOF
}

function create_spec() {
    local affinity_yaml=
    local node_class=client
    local OPTIND=0
    local OPTARG
    local runtime_class
    local opt
    local pin_node
    local scheduler_name
    local node_selector=$node_selector
    local replica=0
    local use_extra_volumes=
    while getopts "A:c:r:s:E:PpR:V" opt "$@" ; do
	case "$opt" in
	    A) affinity_yaml=$OPTARG ;;
	    c) node_class=$OPTARG    ;;
	    r) runtime_class=$OPTARG ;;
	    s) node_selector=$OPTARG ;;
	    E) scheduler_name=$OPTARG;;
	    R) replica=$OPTARG	     ;;
	    P)			     ;;
	    p)			     ;;
	    V) use_extra_volumes=-V  ;;
	    *)                       ;;
	esac
    done
    shift $((OPTIND-1))
    (( $# >= 3 )) || fatal "Usage: create_spec namespace instance secret_count args..."
    # Node specifiers override affinity
    pin_node=$(get_pin_node "$node_class")
    if [[ -z "$affinity_yaml" ]] ; then
	if [[ -n "${pin_node:-}" ]] ; then
	    affinity_yaml="nodeSelector:${nl}  kubernetes.io/hostname: \"$pin_node\""
	elif [[ -n "$node_selector" ]] ; then
	    affinity_yaml="$(mk_selector "$node_selector")"
	fi
    fi
    if [[ -z "${runtime_class+x}" ]] ; then
	if [[ -n "$node_class" && -n "${runtime_classes[$node_class]:-}" ]] ; then
	    runtime_class=${runtime_classes[$node_class]}
	elif [[ -n "${runtime_classes[default]:-}" ]] ; then
	    runtime_class=${runtime_classes[default]}
	fi
    fi
    [[ -n "$(type -t "$create_container_function")" ]] || fatal "Cannot run $create_container_function: command not found"
    local namespace=$1
    local instance=$2
    local secret_count=$3
    local runtime_class=${runtime_class:-}
    if [[ $runtime_class = runc ]] ; then
	runtime_class=
    fi
    # shellcheck disable=SC2046
    cat <<EOF
spec:
  terminationGracePeriodSeconds: 1
${scheduler_name:+$(indent 2 <<< "schedulerName: $scheduler_name")}
$(indent 2 tolerations_yaml ${tolerations+"${tolerations[@]}"})
${runtime_class:+$(indent 2 <<< "runtimeClassName: \"$runtime_class\"")}
${affinity_yaml:+$(indent 2 <<< "$affinity_yaml")}
  containers:
$(indent 2 "$create_container_function" -R "$replica" "$@")
$(indent 2 volumes_yaml $use_extra_volumes "$namespace" "$instance" "$secret_count")
EOF
}

function create_pod_deployment() {
    local namespace=$1
    local instance=$2
    local -i replicas=$4
    local -i replica=0
    local depname="${namespace}-${workload}-${instance}"
    local net_if=
    net_if=$(get_net_interface_external "$node_class")
    if [[ -n "$net_if" ]] ; then
	pod_annotations+=("k8s.v1.cni.cncf.io/networks: $net_if")
    fi
    while (( replica++ < replicas )) ; do
	local name="${depname}-${replica}"
	create_object -n "$namespace" "$name" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: $(mkpodname "$name")
$(indent 2 standard_deployment_metadata_yaml "$namespace" client)
$(indent 2 standard_pod_metadata_yaml "$namespace" client)
  selector:
    matchLabels:
      app: $name
$(indent 2 standard_labels_yaml -c "$node_class" -l "$workload" "$namespace" "${instance}" "${replica}")
$(create_spec -A "$affinity_yaml" -c "$node_class" ${scheduler:+-E "$scheduler"} -R "$replica" "$@")
  restartPolicy: Never
EOF
    done
}

function vm_volume_devices_yaml() {
    local volspec
    local -i emptydiskid=0
    for volspec in "${volumes[@]}" ; do
	local -a args
	IFS=: read -ra args <<< "$volspec"
	local name=${args[0]}
	local type=${args[1]}
	local mountpoint=${args[2]}
	name=$(expand_volume "$name" "$namespace" "$instance" "$replica")
	[[ ${type,,} = emptydir ]] && continue
	args=("${args[@]:3}")
	local opt
	local value
	local bus=virtio
	local cache=
	local arg
	for arg in "${args[@]}" ; do
	    IFS='=' read -r opt value <<< "$arg"
	    case "$opt" in
		bus)   bus="$value"   ;;
		cache) cache="$value" ;;
		*)		      ;;
	    esac
	done
	[[ "${type,,}" = emptydisk ]] && name="cbemptydisk$((emptydiskid++))"
	cat <<EOF
- name: $name
  serial: $name
  disk:
    bus: $bus
${cache:+    cache: $cache}
EOF
    done
}

function vm_volume_mounts_yaml() {
    local volspec
    local -i emptydiskid=0
    for volspec in "${volumes[@]}" ; do
	IFS=: read -ra args <<< "$volspec"
	local name=${args[0]}
	local type=${args[1]}
	local mountpoint=${args[2]}
	[[ ${type,,} = emptydir ]] && continue
	args=("${args[@]:3}")
	local mountopts=
	local fstype
	if [[ ${type,,} = emptydisk ]] ; then
	    fstype=ext4
	else
	    fstype=
	fi
	local fsopts=
	local bus=virtio
	local inodes=
	local nfssrv=
	local nfsshare=/
	for arg in "${args[@]}" ; do
	    local opt
	    local value
	    IFS='=' read -r opt value <<< "$arg"
	    case "$opt" in
		bus)	   bus="$value"	      ;;
		inodes)    inodes="$value"    ;;
		mountopts) mountopts="$value" ;;
		fstype)    fstype="$value"    ;;
		fsopts)    fsopts="$value"    ;;
		nfssrv)    nfssrv="$value"    ;;
		nfsshare)  nfsshare="$value"  ;;
		*)			      ;;
	    esac
	done
	if [[ -n "${nfssrv}" ]] ; then
	    local remote="${nfssrv}:${nfsshare}"
	    command="mkdir -p '$mountpoint' && mount $mountopts '$remote' '$mountpoint' && chmod 777 '$mountpoint'"
	else
	    [[ "${type,,}" = emptydisk ]] && name="cbemptydisk$((emptydiskid++))"
	    name=$(expand_volume "$name" "$namespace" "$instance" "$replica")
	    name="${bus}-${name}"
	    if [[ -n "$fstype" ]] ; then
		local sinodes=
		[[ $fstype = ext* ]] && sinodes=${inodes:+"-N '$inodes'"}
		local force=
		[[ $fstype = xfs ]] && force=-f
		# shellcheck disable=SC2016
		command="'mkfs.${fstype}' $force $sinodes $fsopts '/dev/disk/by-id/$name' && "
	    fi
	    command+="mkdir -p '$mountpoint' && mount $mountopts '/dev/disk/by-id/$name' '$mountpoint' && chmod 777 '$mountpoint'"
	fi
	cat <<EOF
- "$command"
EOF
    done
}

function _vm_evict_strategy() {
    if ((vm_evict_migrate)) ; then
	echo "evictionStrategy: LiveMigrate"
    fi
}

function _run_as_container() {
    local -a ports=("${!workload_service_ports[@]}")
    local port
    local -a nports
    for port in "${ports[@]}" ; do
	nports+=("-p ${port}:${port}")
    done
    if ((vm_run_as_container)) ; then
	local setup_command=
	local sysctls=
	local key
	local value
	for key in "${!__sysctls[@]}" ; do
	    sysctls+=" '--sysctl=$key=${__sysctls[$key]}'"
	done
	echo "podman run --rm --privileged $sysctls -P ${nports[*]} $(standard_environment -C) '-v${system_configmap_mount_dir}:${system_configmap_mount_dir}' '-v${user_configmap_mount_dir}:${user_configmap_mount_dir}' '$container_image' $setup_command"
    fi
}

function _run_as_root() {
    if ((vm_run_as_root)) ; then
	echo -n "sudo -E "
    fi
}

function _install_required_packages() {
    local -a packages_needed=()
    readarray -t packages_needed <<< "$(vm_required_packages)"
    if [[ $vm_run_as_container -eq 0 && -n "${packages_needed[*]}" ]] ; then
	echo "- \"dnf install -y ${packages_needed[*]}"\"
    fi
}

function _setup_commands() {
    sed -E -e 's/(^.+$)/- "\1"/' <<< "$(vm_setup_commands)"
    if ((vm_run_as_container)) ; then
	echo "- sudo dnf install -y podman"
    fi
}

function _setup_sysctls() {
    if ((! vm_run_as_container)) ; then
	local key
	for key in "${!__sysctls[@]}" ; do
	    echo "- \"sysctl -w '$key=${__sysctls[$key]}'\""
	done
    fi
}

function _insert_ssh_keys() {
    local file
    for file in "$@" ; do
	if [[ -z "$file" ]] ; then continue; fi
	if [[ -f "$file" ]] ; then
	    local ftype
	    ftype=$(file "$file")
	    case "$ftype" in
		*'private key')
		    warn "$file is a private key, not inserting!" ;;
		*'public key')
		    local contents
		    read -r contents < "$file"
		    cat <<EOF
- "echo '$contents' >> \"/root/.ssh/authorized_keys\" && chmod 600 \"/root/.ssh/authorized_keys\""
EOF
		    ;;
		*)
		    warn "Do not recognize type of $file ($ftype), not inserting" ;;
	    esac
	else
	    warn "$file does not exist or is not a plain file"
	fi
    done
}


function _vm_interfaces() {
    local net_if=
    net_if=$(get_net_interface_external "$node_class")
    if [[ -n "$net_if" ]] ; then
	cat <<EOF
interfaces:
  - name: default
    masquerade: {}
  - name: $net_if
    bridge: {}
EOF
    fi
}

function _vm_networks() {
    local net_if=
    net_if=$(get_net_interface_external "$node_class")
    if [[ -n "$net_if" ]] ; then
	cat <<EOF
networks:
  - name: default
    pod: {}
  - name: $net_if
    multus:
      networkName: $net_if
EOF
    fi
}

function _increment_vm_addr() {
    local -i a
    a=${vm_net_addr[3]}
    local -i b
    b=${vm_net_addr[2]}
    a=$((a + 1))
    if ((a > 254)) ; then
	a=1
	b=$((b + 1))
	if ((b > 255)) ; then
	    fatal "Out of IP addresses to assign!"
	fi
    fi
    vm_net_addr[3]=$a
    vm_net_addr[2]=$b
}

function _vm_network_data() {
    local vm_addr=${1:-192.192.192.192}
    local net_if=
    net_if=$(get_net_interface_local "$node_class")
    if [[ -n "$net_if" ]] ; then
	cat <<EOF
networkData: |-
  version: 2
  ethernets:
    $net_if:
      dhcp4: false
      dhcp6: false
      addresses:
        - ${vm_addr}/16
EOF
    fi
}

function _vm_container_disk_yaml() {
    if [[ -n "${vm_image:-}" ]] ; then
	cat <<EOF
- name: containerdisk
  disk:
    bus: virtio
  containerDisk:
    image: $vm_image
EOF
    fi
}

function _vm_run_yaml() {
    if [[ -n "$vm_run_strategy" ]] ; then
	echo "runStrategy: $vm_run_strategy"
    else
	echo "running: $vm_running"
    fi
}

function create_vm_deployment() {
    get_sysctls
    local node_class=${node_class:-client}
    local namespace=$1
    local instance=$2
    local secret_count=$3
    local -i replicas=$4
    local containers=$5
    local sync_service=
    local sync_port_num=
    local sync_ns_port_num=
    local drop_cache_service=
    local drop_cache_port_num=
    local vm_running=true
    local vm_addr
    ((vm_start_running)) || vm_running=false
    pin_node=$(get_pin_node "$node_class")
    if [[ -z "$affinity_yaml" ]] ; then
	if [[ -n "${pin_node:-}" ]] ; then
	    affinity_yaml="nodeSelector:${nl}  kubernetes.io/hostname: \"$pin_node\""
	elif [[ -n "$node_selector" ]] ; then
	    affinity_yaml="$(mk_selector "$node_selector")"
	fi
    fi
    local -i replica=0
    local depname="${namespace}-${workload}-${instance}"
    local net_if=
    net_if=$(get_net_interface_external "$node_class")
    if [[ -n "$net_if" ]] ; then
	pod_annotations+=("k8s.v1.cni.cncf.io/networks: $net_if")
    fi
    local net_if
    net_if=$(get_net_interface_local "$node_class")
    while (( replica++ < replicas )) ; do
        local name="${depname}-${replica}"
        local vmname
	vmname=$(mkpodname "$name")
	IFS=: read -r sync_service sync_port_num sync_ns_port_num <<< "$(get_sync)"
	IFS=: read -r drop_cache_service drop_cache_port_num <<< "$(get_drop_cache "$namespace" "$instance" "$replica")"
	vm_addr=$(IFS=.; echo "${vm_net_addr[*]}")
	# We can't do this as part of retrieving the VM addr because that
	# gets read from a subshell.  Incrementing the VM address needs
	# to be done at top level.
	_increment_vm_addr
        create_object -t VM -n "$namespace" "$name" <<EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
$(indent 2 standard_labels_yaml -c "$node_class" -l "$workload" "$namespace" "$instance" "$replica")
  name: "$vmname"
$(indent 2 standard_deployment_metadata_yaml "$namespace" client)
spec:
$(indent 2 _vm_run_yaml)
  template:
    metadata:
$(indent 6 standard_labels_yaml -c "$node_class" -l "$workload" "$namespace" "$instance" "$replica")
        kubevirt-vm: "$vmname"
$(indent 6 standard_deployment_metadata_yaml "$namespace" client)
$(indent 6 standard_pod_metadata_yaml "$namespace" client)
    spec:
${scheduler:+$(indent 6 <<< "schedulerName: $scheduler")}
$(indent 6 _vm_evict_strategy)
      domain:
        cpu:
          cores: $vm_cores
          sockets: $vm_sockets
          threads: $vm_threads
        memory:
          guest: $vm_memory
        devices:
          disks:
$(indent 12 _vm_container_disk_yaml)
$(indent 12 vm_volume_devices_yaml)
            - name: systemconfigmap-${namespace}
              serial: systemconfigmap
              volumeName: systemconfigmap-${namespace}
            - name: userconfigmap-${namespace}
              serial: userconfigmap
              volumeName: userconfigmap-${namespace}
$(indent 10 _vm_interfaces)
$(indent 8 container_resources_yaml)
$(indent 6 _vm_networks)
      terminationGracePeriodSeconds: $vm_grace_period
$(indent 6 <<< "$affinity_yaml")
      volumes:
$(indent 8 volumes_yaml -n "$namespace" "$instance" "$secret_count")
        - name: containerdisk
          containerDisk:
            image: $vm_image
        - name: cloudinitdisk
          cloudInitNoCloud:
$(indent 12 _vm_network_data "$vm_addr")
            userData: |-
              #cloud-config
              ${vm_user:+user: $vm_user}
              ${vm_user:+username: $vm_user}
              ${vm_password:+password: $vm_password}
              chpasswd: { expire: False }
              bootcmd:
                - "mkdir -p '$system_configmap_mount_dir' '$user_configmap_mount_dir'"
$(indent 16 vm_volume_mounts_yaml)
                - "mount -o ro /dev/disk/by-id/ata-QEMU_HARDDISK_systemconfigmap '$system_configmap_mount_dir'"
                - "mount -o ro /dev/disk/by-id/ata-QEMU_HARDDISK_userconfigmap '$user_configmap_mount_dir'"
$(indent 16 _insert_ssh_keys "${vm_ssh_keyfile}.pub")
                $(_install_required_packages)
$(indent 16 _setup_commands)
$(indent 16 _setup_sysctls)
              runcmd:
                - "$(standard_environment -V) $(_run_as_root)  $(_run_as_container)$(if [[ -n "${arglist_function}" ]] ; then
     ARGLIST_FORMAT=vm "${arglist_function}" "${system_configmap_mount_dir}/" "$@" "0" -- \
     "$sync_nonce" "$namespace" "c0" "$basetime" "$baseoffset" "$(ts)" "$exit_at_end" "$sync_service" \
     "$sync_port_num" "$sync_ns_port_num" "$sync_watchdog_port_num" "$sync_watchdog_timeout" \
     "$drop_cache_service" "$drop_cache_port_num" ; fi)"
EOF
    done
}

function create_replication_deployment() {
    local namespace=$1
    local instance=$2
    local -i replicas=$4
    local name="${namespace}-${workload}-${instance}"
    local pod_annotations=("${pod_annotations[@]}")
    local net_if=
    net_if=$(get_net_interface_external "$node_class")
    if [[ -n "$net_if" ]] ; then
	pod_annotations+=("k8s.v1.cni.cncf.io/networks: $net_if")
    fi
    create_object -t "$deployment_type" -n "$namespace" "$name" <<EOF
apiVersion: apps/v1
kind: $deployment_type
metadata:
  name: $(mkpodname "$name")
$(indent 2 standard_deployment_metadata_yaml "$namespace" client)
$(indent 2 standard_labels_yaml -c "$node_class" -l "$workload" "$namespace" "$instance")
spec:
  replicas: $replicas
  selector:
    matchLabels:
      instance: ${namespace}-${workload}-${instance}
  strategy:
    type: RollingUpdate
  template:
    metadata:
$(indent 6 standard_labels_yaml -c "$node_class" -l "$workload" "$namespace" "$instance")
$(indent 6 standard_pod_metadata_yaml "$namespace" client)
$(indent 4 create_spec -A "$affinity_yaml" -c "$node_class" ${scheduler:+-E "$scheduler"} "$@")
EOF
}

function create_standard_deployment() {
    local OPTIND=0
    local OPTARG=
    local affinity_yaml=
    local workload="$requested_workload"
    local opt
    local security_context
    local allow_replica_controller=1
    local node_class=client
    local deployment_type=$deployment_type
    local arglist_function=
    local create_container_function=create_standard_containers
    local pod_annotations=("${pod_annotations[@]}")
    security_context="$(default_security_context)"
    while getopts 'A:w:S:pc:d:e:C:a:N:' opt "$@" ; do
	case "$opt" in
	    A) affinity_yaml="$OPTARG"		   ;;
	    a) arglist_function="$OPTARG"	   ;;
	    C) create_container_function="$OPTARG" ;;
	    c) node_class="$OPTARG"		   ;;
	    d) deployment_type="$OPTARG"	   ;;
	    N) pod_annotations+=("$OPTARG")	   ;;
	    p) allow_replica_controller=0	   ;;
	    S) security_context="$OPTARG"	   ;;
	    w) workload="$OPTARG"	  	   ;;
	    *)					   ;;
	esac
    done
    [[ -n "$(type -t "$create_container_function")" ]] || fatal "Cannot run create container function $create_container_function: command not found"
    [[ -z "$arglist_function" || -n "$(type -t "$arglist_function")" ]] || fatal "Cannot run arglist function $arglist_function: command not found"
    shift $((OPTIND-1))
    if (($# != 5)) ; then
	fatal "Usage: create_standard_deployment <args> namespace instance secret_count replicas containers"
    fi
    requires_drop_cache && create_drop_cache_deployment "$1" "$requested_workload" "$2" "$4"

    case "${deployment_type,,}" in
	pod)
	    create_pod_deployment "$@"
	    ;;
	replicaset|deployment)
	    if ((allow_replica_controller)) ; then
		create_replication_deployment "$@"
	    else
		create_pod_deployment "$@"
	    fi
	    ;;
        vm)
            create_vm_deployment "$@"
            ;;
	*)
	    fatal "Unknown deployment type $deployment_type"
	    ;;
    esac
}

function create_generic_deployment() {
    local OPTIND=0
    local OPTARG=0
    local pflag=
    local arglist_function="${requested_workload}_arglist"
    while getopts 'pa:' opt "$@" ; do
	case "$opt" in
	    p) pflag=-p ;;
	    a) arglist_function="$OPTARG" ;;
	    *)		;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    local count=${2:-1}
    local secret_count=${3:-1}
    local replicas=${4:-1}
    local containers_per_pod=${5:-1}
    local -i instance
    local -i extra_sync_count=0
    requires_drop_cache && extra_sync_count=1
    create_sync_service "$namespace" \
			"$((containers_per_pod * replicas * processes_per_pod * count))" \
			"$(((containers_per_pod + extra_sync_count) * replicas * count))"
    for ((instance = first_deployment; instance < count + first_deployment; instance++)) ; do
	create_standard_deployment -a "$arglist_function" $pflag \
				   "$namespace" "$instance" "$secret_count" "$replicas" "$containers_per_pod"
    done
}

function create_container_sync() {
    local OPTARG
    local OPTIND=0
    local opt
    while getopts 'i:R:' opt "$@" ; do
	case "$opt" in
	    *)		        ;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    local expected_clients=$4
    local initial_expected_clients=$5
    cat <<EOF
- name: ${namespace}
  image_pull_policy: $image_pull_policy
  image: "$container_image"
  ports:
  - containerPort: $port
$(indent 2 standard_environment)
  command:
  - "python3"
  - "$system_configmap_mount_dir/sync.py"
  - "$sync_nonce"
  - "$sync_flag_file"
  - "$sync_error_file"
  - "$controller_timestamp_file"
  - "$predelay"
  - "$postdelay"
  - "$workload_step_interval"
  - "$sync_port"
  - "$sync_ns_port"
  - "$sync_watchdog_port_num"
  - "$sync_watchdog_timeout"
  - "$expected_clients"
  - "$initial_expected_clients"
$(indent 2 volume_mounts_yaml -V "$namespace" 0 0)
$(indent 2 restricted_security_context)
EOF
}

function create_sync_deployment() {
    local namespace=$1; shift
    local -a tolerations=('node-role.kubernetes.io/infra:Equal:NoSchedule' "${tolerations[@]}")
    local affinity_yaml
    affinity_yaml="$(affinity=$sync_affinity create_affinity_yaml -k "${basename}-worker" "true")"
    create_object -t Pod -n "$namespace" "${namespace}-sync" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: $(mkpodname "${basename}-sync")
$(indent 2 standard_deployment_metadata_yaml "$namespace" sync)
$(indent 2 standard_labels_yaml -c sync -S -t 'sync' 'sync' "${basename}-sync")
    ${basename}-sync-dep: "true"
  selector:
    matchLabels:
      app: ${namespace}
$(create_container_function=create_container_sync create_spec -V -P -s '' -c sync -r '' "$namespace" 0 0 "$@")
$(indent 2 <<< "$affinity_yaml")
  restartPolicy: Never
EOF
}

function create_container_drop_cache() {
    local OPTARG
    local OPTIND=0
    local opt
    while getopts 'i:R:' opt "$@" ; do
	case "$opt" in
	    *)		        ;;
	esac
    done
    shift $((OPTIND-1))
    local namespace=$1
    local sync_service=
    local sync_port_num=
    local sync_ns_port_num=
    IFS=: read -r sync_service sync_port_num sync_ns_port_num <<< "$(get_sync)"
    cat <<EOF
- name: ${namespace}-dc
  image_pull_policy: $image_pull_policy
  image: "$container_image"
  securityContext:
    privileged: true
    runAsUser: 0
  ports:
  - containerPort: $drop_cache_port
$(indent 2 standard_environment)
  command:
  - "python3"
  - "${system_configmap_mount_dir}/drop_cache.py"
  - "$sync_nonce"
  - "$namespace"
  - "c0"
  - "$basetime"
  - "$baseoffset"
  - "$(ts)"
  - "0"
  - "$sync_service"
  - "$sync_port_num"
  - "$sync_ns_port"
  - "none"
  - "$drop_cache_port"
$(indent 2 volume_mounts_yaml -V "$namespace" 0 0)
  - mountPath: "/proc-sys-vm"
    name: "proc-sys-vm"
EOF
}

function create_drop_cache_deployment() {
    local namespace=$1
    local workload=$2
    local instance=$3
    local -i replicas=$4
    local -i replica=0
    while ((replica++ < replicas)) ; do
	local name="${namespace}-${workload}-${instance}-${replica}"
	local dcname="${name}-dc"
	create_object -t Pod -n "$namespace" "${namespace}-${workload}-${instance}-${replica}-dc" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: $(mkpodname "$dcname")
$(indent 2 standard_pod_metadata_yaml "$namespace" drop_cache)
$(indent 2 standard_deployment_metadata_yaml "$namespace" drop_cache)
$(indent 2 standard_labels_yaml -c dc -s dc -S -t drop-cache "$workload" "$namespace" "$instance" "$replica")
  selector:
    matchLabels:
      replica: ${namespace}-${workload}-${instance}-${replica}
$(create_container_function=create_container_drop_cache create_spec -V -P -s '' -c drop-cache -r '' "$namespace" "$instance" 0)
  - name: "proc-sys-vm"
    hostPath:
      path: "/proc/sys/vm"
$(indent 2 create_affinity_yaml -a affinity -k replica "$name")
  restartPolicy: Never
EOF
    done
}

################################################################
# Generic Object creation
################################################################

function create_objects_n() {
    local objtype=$1; shift
    local parallel=$1; shift
    local objs_per_call=$1; shift
    local rotor=$1; shift
    local sleeptime=$1; shift
    while (( rotor < namespaces )) ; do
	"create_${objtype}" "${namespaces_to_create[$rotor]}" "$@"
	if (( sleeptime )) ; then sleep "$sleeptime"; fi
	rotor=$((rotor + parallel))
    done
    really_create_objects
}

function allocate_namespaces() {
    local -i ns_count=0
    local -i ns_idx=0
    if (( scale_ns )) ; then
        while read -r ns ; do
	    if [[ -n "$ns" ]] ; then
		namespaces_in_use[${ns#namespace/}]=1
	    fi
        done < <(__OC get ns -l "$basename" --no-headers -o name 2>/dev/null)
    fi
    while (( ns_count++ < namespaces )) ; do
        while [[ -n "${namespaces_in_use[${basename}-$ns_idx]:-}" ]] ; do
            ns_idx=$((ns_idx+1))
        done
        namespaces_to_create+=("${basename}-$((ns_idx++))")
    done
    if ((sync_start)) ; then
	if ((sync_in_first_namespace)) ; then
	    sync_namespace=${namespaces_to_create[0]}
	else
	    sync_namespace="${basename}-sync"
	fi
	sync_pod=("-n" "$sync_namespace" "${basename}-sync")
    fi
}

function find_first_deployment() {
    if (( scale_deployments )) ; then
	local ns
	local deployment
	local stuff
	# shellcheck disable=SC2034
        while read -r ns deployment stuff ; do
	    if [[ -n "$deployment" ]] ; then
		deployment=${deployment#"${ns}"-}
		deployment=${deployment%-*}
		echo __OC get deployments -l "$basename" -A --no-headers 1>&2
		if (( deployment + 1 > first_deployment )) ; then
		    first_deployment=$((deployment + 1))
		fi
	    fi
        done <<< "$(__OC get deployments -l "$basename" -A --no-headers 2>/dev/null)"
    fi
}

function create_all_namespaces() {
    local i
    local -a pids=()
    for ((i = 0; i < parallel_namespaces; i++)) ; do
	create_objects_n namespace "$parallel_namespaces" "$objs_per_call_namespaces" "$i" "$sleep_between_namespaces" &
	pids+=("$!")
    done
    wait "${pids[@]}" || killthemall "Unable to create namespaces"
}

function create_all_secrets() {
    local i
    local -a pids=()
    for ((i = 0; i < parallel_secrets; i++)) ; do
	create_objects_n secrets "$parallel_secrets" "$objs_per_call_secrets" "$i" "$sleep_between_secrets" "$deps_per_namespace" "$secrets" &
	pids+=("$!")
    done
    wait "${pids[@]}" || killthemall "Unable to create secrets"
}

function create_system_configmap() {
    local -a systemfiles
    readarray -t systemfiles < <(list_configmaps | grep .)
    if [[ -n "$artifactdir" && -n "${systemfiles[*]}" ]] ; then
	mkdir -p "$artifactdir/SYSFILES" || fatal "Can't create system artifacts directory"
	cp -p "${systemfiles[@]}" "$artifactdir/SYSFILES"
    fi
    if [[ ${#systemfiles[@]} -gt 0 && $sync_start -ne 0 && -n "$sync_namespace" && $sync_in_first_namespace -eq 0 ]] ; then
	create_configmaps "$sync_namespace" "systemconfigmap" "${systemfiles[@]}"
    fi
    local -a userfiles
    readarray -t userfiles < <(list_user_configmaps | grep .)
    if [[ -n "$artifactdir" && -n "${userfiles[*]}" ]] ; then
	mkdir -p "$artifactdir/USERFILES" || fatal "Can't create user artifacts directory"
	cp -p "${userfiles[@]}" "$artifactdir/USERFILES"
    fi
    for ((i = 0; i < parallel_configmaps; i++)) ; do
	create_objects_n configmaps "$parallel_configmaps" "$objs_per_call_configmaps" "$i" "$sleep_between_configmaps" "systemconfigmap" "${systemfiles[@]}"&
	pids+=("$!")
	create_objects_n configmaps "$parallel_configmaps" "$objs_per_call_configmaps" "$i" "$sleep_between_configmaps" "userconfigmap" "${userfiles[@]}"&
	pids+=("$!")
    done
    wait "${pids[@]}" || killthemall "Unable to create system configmap"
}

function create_all_configmaps() {
    create_system_configmap
    (( ${#configmap_files[@]} )) || return;
    local i
    local -a pids=()
    for ((i = 0; i < parallel_configmaps; i++)) ; do
	create_objects_n configmaps "$parallel_configmaps" "$objs_per_call_configmaps" "$i" "$sleep_between_configmaps" "configmap" "${configmap_files[@]}"&
	pids+=("$!")
    done
    wait "${pids[@]}" || killthemall "Unable to create configmaps"
}

function drop_host_caches() {
    ((doit && (drop_node_cache || drop_all_node_cache) )) || return 0
    local -a nodes_to_drop
    if [[ -n "${pin_nodes[*]}" && $drop_all_node_cache -eq 0 ]] ; then
	local n
	local -A tmp_nodes_to_drop
	for n in "${pin_nodes[@]}" ; do
	    tmp_nodes_to_drop[$n]=1
	done
	nodes_to_drop=("${!tmp_nodes_to_drop[@]}")
    else
	readarray -t nodes_to_drop <<< "$(____OC get -l node-role.kubernetes.io/worker node -ojsonpath="{range .items[*]}{.metadata.name}{'\n'}{end}")"
    fi
    local -a pids=()
    for n in "${nodes_to_drop[@]}" ; do
	((report_object_creation)) && echo "Dropping buffer cache: $n"
	_OC debug --no-tty=true "node/$n" -- chroot /host sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches' 2>/dev/null &
	pids+=("$!")
    done
    wait "${pids[@]}"
}

function create_all_deployments() {
    local i
    local -a pids=()
    for ((i = 0; i < parallel_deployments; i++)) ; do
	create_objects_n deployment "$parallel_deployments" "$objs_per_call_deployments" "$i" "$sleep_between_deployments" "$deps_per_namespace" "$secrets" "$replicas" "$containers_per_pod" &
	pids+=("$!")
    done
    wait "${pids[@]}" || killthemall "Unable to create deployments"
}

function create_deployments_post_hook() {
    if [[ $deployment_type = vm && ($vm_run_strategy != Always || $vm_start_running -eq 0) ]] ; then
	local namespace
	local name
	if [[ -z "$VIRTCTL" ]] ; then
	    warn "*** Unable to find virtctl command, cannot start VMs!"
	    return
	fi
	# shellcheck disable=SC2034
	while read -r namespace name ignore ; do
	    "$VIRTCTL" start -n "$namespace" "$name"
	done < <(__OC get vm --no-headers -A -l "${basename}-xuuid=$xuuid")
    fi
}

function create_all_objects() {
    local found_objtype
    local objtype=$1
    shift
    get_run_failed && return 1
    while IFS= read -r 'line' ; do
	if [[ $line = *'__KUBEFAIL__ '* ]] ; then
	    echo "${line#__KUBEFAIL__ }" 1>&2
	    return 1
	elif [[ $line =~ ^[[:digit:]]{4}(-[[:digit:]]{2}){2}T([[:digit:]]{2}:){2}[[:digit:]]{2}\.[[:digit:]]{6}\ +((([-a-z0-9]+)(\.[-a-z0-9]*)?)/([-a-z0-9]+))\ +created$ ]] ; then
	    if (( report_object_creation )) ; then
		echo "$line" 1>&2
	    fi
	    total_objects_created=$((total_objects_created+1))
	    found_objtype=${BASH_REMATCH[4]}
	    if [[ -z ${objects_created[$found_objtype]:-} ]] ; then
		objects_created[$found_objtype]=1
	    else
		objects_created[$found_objtype]=$((${objects_created[$found_objtype]:-}+1))
	    fi
	elif [[ -n "$line" ]] ; then
	    if ((doit)) ; then
		(IFS=''; echo "$line" 1>&2)
	    else
		(IFS=''; echo "$line")
	    fi
	fi
    done < <("create_all_${objtype}" "$@" 2>&1)
    if [[ $(type -t "create_${objtype}_post_hook") = function ]] ; then
	"create_${objtype}_post_hook" "${objects_created[@]}"
    fi
}

function _do_cleanup_1() {
    local ltimeout="${force_cleanup_timeout:+--timeout=${timeout}s}"
    local force=0
    local -i precleanup=0
    local OPTIND=0
    local opt
    while getopts 'fp' opt "$@" ; do
	case "$opt" in
	    f) force=1 	    ;;
	    p) precleanup=1 ;;
	    *)		    ;;
	esac
    done
    shift $((OPTIND-1))
    local objects_to_clean
    if [[ $use_namespaces -ne 0 && $precleanup -ne 0 && $remove_namespaces -eq -1 &&
	      -n "$(__OC get ns -l "${basename}-sync" 2>/dev/null; __OC get ns -l "${basename}" 2>/dev/null)" ]] ; then
	remove_namespaces=0
    fi
    if ((use_namespaces && remove_namespaces)) ; then
	objects_to_clean=ns
    else
	objects_to_clean="-A all"
	fi
    if ((force)) ; then
	# It appears that allowing processes to write to stderr inside
	# a trap results in the subprocess not doing everything
	# (in this case, oc delete doesn't complete).
	exec >/dev/null
	exec 2>/dev/null
	# shellcheck disable=SC2086
	____OC delete $objects_to_clean -l "${basename}-sync" $ltimeout
	# shellcheck disable=SC2086
	____OC delete $objects_to_clean -l "$basename" $ltimeout
    elif ((report_object_creation)) ; then
	# shellcheck disable=SC2086
	__OC delete $objects_to_clean -l "${basename}-sync" $ltimeout 1>&2
	# shellcheck disable=SC2086
	__OC delete $objects_to_clean -l "${basename}" $ltimeout 1>&2
    else
	# shellcheck disable=SC2086
	__OC delete $objects_to_clean -l "${basename}-sync" $ltimeout 2>&1 |grep 'DEBUG kubectl:' 1>&2
	# shellcheck disable=SC2086
	__OC delete $objects_to_clean -l "$basename" $ltimeout 2>&1 |grep 'DEBUG kubectl:' 1>&2
    fi
}

function _do_cleanup() {
    if ! _do_cleanup_1 "$@" 2>&1 | timestamp 1>&2 ; then
	if [[ -n "$force_cleanup_timeout" ]] ; then
	    warn "*** Cleanup failed, doing a force delete!"
	    if [[ $deployment_type = vm ]] ; then
		__OC delete all -A -l "${basename}-sync" --force --grace-period=0 2>&1 | timestamp 1>&2
	    fi
	    __OC delete all -A  -l "$basename" --force --grace-period=0 2>&1 | timestamp 1>&2
	    if ((use_namespaces && remove_namespaces)) ; then
		__OC delete namespace -l "$basename" --force --grace-period=0 2>&1 | timestamp 1>&2
	    fi
	fi
    fi
}

# shellcheck disable=SC2120
function do_cleanup() {
    local force=
    local OPTIND=0
    local precleanup=
    local opt
    local -a args=("$@")
    while getopts 'fp' opt "$@" ; do
	case "$opt" in
	    f) force=-f     ;;
	    p) precleanup=-p;;
	    *)              ;;
	esac
    done
    shift "$OPTIND"
    (( doit )) || return 0
    if [[ -n "${injected_errors[precleanup]:-}" && $precleanup = -p ]] ; then
	warn "*** Injecting precleanup error ${inject_errors[precleanup]:-}"
	sleep "${injected_errors[precleanup]:-}"
	killthemall "Cleanup timed out!"
    fi
    if [[ -n "${injected_errors[cleanup]:-}" && $precleanup = -p ]] ; then
	warn "*** Injecting cleanup error ${inject_errors[cleanup]:-}"
	sleep "${injected_errors[cleanup]:-}"
	killthemall "Cleanup timed out!"
    fi
    # shellcheck disable=SC2086
    _do_cleanup "$@" || killthemall "Cleanup timed out!"
}

################################################################
# Main work loop
################################################################

function create_secrets_top_level() {
    # If previous secrets weren't all created, this will yield
    # the wrong result.
    if (( doit && wait_for_secrets )) ; then
	local -i i
	local -i j
	local -i k
	local ns
	local nsd
	for ((i = 0; i < namespaces; i++)) ; do
	    ns="secret/secret-${basename}-${namespaces_to_create[$i]##*-}"
	    for ((j = 0; j < deps_per_namespace + first_deployment; j++)) ; do
		nsd="${ns}-$j"
		for ((k = 0; i < secrets; k++)) ; do
		    expected_secrets["${nsd}-$k"]=1
		done
	    done
	done
    fi
    create_all_objects secrets 2>&1 || return 1
    if (( doit )) ; then
	local secname=""
	while (( ${#expected_secrets[@]} )) ; do
	    echo "Expect ${#expected_secrets[@]}" 1>&2
	    while read -r secname ; do
		[[ -n "${secname:-}" ]] && unset "expected_secrets[$secname]"
	    done < <(__OC get secret -oname --no-headers -l "$basename" -A)
	    if (( ${#expected_secrets[@]} )) ; then
		echo "Still waiting for ${#expected_secrets[@]} secrets to be created."
		sleep 10
	    else
		break
	    fi
	done
    fi
}

function do_logging() {
    if ((wait_forever)) ; then
	while ! get_run_failed ; do
	    sleep 5
	done
	return 1
    fi
    ((wait_forever)) && sleep infinity
    ((report)) || return 0
    local -i logs_expected=0
    logs_expected="$(calculate_logs_required "$namespaces" "$deps_per_namespace" "$replicas" "$containers_per_pod")"
    ((logs_expected > 0)) || return 0
    trap 'if ((get_sync_logs_pid > 1)) ; then kill -TERM "$get_sync_logs_pid"; wait "$get_sync_logs_pid" 2>/dev/null; get_sync_logs_pid=0; fi; return 1' TERM

    get_sync_logs &
    get_sync_logs_pid=$!
    protect_pids "$get_sync_logs_pid $BASHPID"
    if ((timeout > 0)) ; then
	local -i itimeout=$timeout
	local -i finis=0
	while (( timeout < 0 || timeout-- )) ; do
	    if get_run_failed ; then
		finis=-1
		break
	    fi
	    # CreateContainerError is not necessarily fatal
	    if __OC get pods -A -l "$basename" |grep -q -E -e '([^r]Error|Evicted|Crash)' ; then
		echo "Run failed:" 1>&2
		__OC get pods -A -l "$basename" |grep -E -e '([^r]Error|Evicted|Crash)' | {
		    local line
		    local ns
		    local pod
		    local rest
		    while read -r line ; do
			read -r ns pod rest <<< "$line"
			echo "$line" 1>&2
			____OC logs -n "$ns" "$pod" 1>&2
			echo 1>&2
		    done
		}
		finis=-1
		break
	    fi
	    # Commands in a pipeline are each run in subshells.  Background jobs are child
	    # of the parent, not any children, so running "jobs -l" inside the pipeline
	    # will not reap any exited subjobs.  The child still has the job table, so
	    # it knows what subjobs exist, but the "jobs -l" must be run at top level
	    # to actually reap them.
	    jobs -l >/dev/null
	    if jobs -l |grep -q . ; then
		sleep 1
	    else
		finis=1
		break
	    fi
	done
	if (( finis <= 0 )) ; then
	    if ((finis < 0)) ; then
		set_run_failed "Run failed: $(get_failure_reason)"
	    else
		set_run_failed "Run did not terminate in $itimeout seconds"
	    fi
	    kill -INT "$get_sync_logs_pid"
	fi
    fi
    if (( get_sync_logs_pid > 1 )) ; then
	wait "$get_sync_logs_pid" 2>/dev/null
	local -i status=$?
	get_sync_logs_pid=0
	return $status
    fi
}

function report_object_creation() {
    (( report_object_creation )) || return 0
    local objtype
    while read -r objtype ; do
	[[ -n "${objtype:-}" ]] && printf "Created %${#total_objects_created}d %s%s\n" "${objects_created[$objtype]}" "$objtype" "${plurals[$((${objects_created[$objtype]} == 1))]}" 1>&2
    done < <( (IFS=$'\n'; echo "${!objects_created[*]}") | sort)
    printf "Created %d object%s total\n" "$total_objects_created" "${plurals[$((total_objects_created == 1))]}" 1>&2
}

function expand_string() {
    local string=$1
    shift
    local -A _____dict_____=()
    local -a arg
    for arg in "$@" ; do
	local noptname1 optname optvalue
	read -r noptname optname optvalue <<< "$(parse_option "$arg")"
	_____dict_____["$optname"]="$optvalue"
    done
    while [[ $string =~ (.*)(%\{([[:alnum:]_]+(\[[[:alnum:]]+\])?)(:-([^\}]*))?\})(.*) ]] ; do
	local prefix=${BASH_REMATCH[1]:-}
	local var=${BASH_REMATCH[3]:-}
	local replacement=${BASH_REMATCH[6]:-UNKNOWN$var}
	local suffix=${BASH_REMATCH[7]:-}
	if [[ -n "${_____dict_____[$var]+isset}" ]] ; then
	    string="${prefix}${_____dict_____[$var]}${suffix}"
	else
	    string="${prefix}${!var:-$replacement}${suffix}"
	fi
    done
    echo "$string"
}

function setup_namespaces() {
    allocate_namespaces || return 1
    if [[ $sync_start -ne 0 && -n "$sync_namespace" && $sync_in_first_namespace -eq 0 ]] ; then
	create_namespace -S -f "$sync_namespace" 1>&2 || return 1
    fi
}

function run_clusterbuster_2() {
    local -A expected_secrets=()
    local -i status=0
    if (( precleanup && doit )) ; then
	do_cleanup -p 1>&2 || return 1
    fi
    if ((cleanup_always)) ; then
	trap 'echo "Cleaning up" 1>&2; doit=1; if ((cleanup_always)) ; then cleanup_always=0; do_cleanup -f; if ((get_sync_logs_pid > 0)) ; then wait "$get_sync_logs_pid"; get_sync_logs_pid=0; fi; fi; killthemall ""; wait; remove_tmpdir; doit=1; do_cleanup -f; exit 1' TERM INT HUP
    else
	trap 'echo "Not cleaning up" 1>&2; if ((get_sync_logs_pid > 0)) ; then wait "$get_sync_logs_pid"; get_sync_logs_pid=0; fi; killthemall ""; wait; remove_tmpdir; exit 1' TERM HUP INT
    fi
    if [[ $doit -gt 0 ]] ; then
	if [[ -n "${artifactdir:-}" && $take_prometheus_snapshot -gt 0 ]] ; then
	    start_prometheus_snapshot || return 1
	else
	    set_start_timestamps || return 1
	fi
	if [[ -n "${artifactdir:-}" ]] ; then
	    artifactdir=${artifactdir//%s/$(readable_timestamp "$prometheus_starting_timestamp")}
	    artifactdir=${artifactdir//%w/$requested_workload}
	    artifactdir=${artifactdir//%n/$job_name}
	    artifactdir="$(expand_string "$artifactdir")"
	    mkdir -p "$artifactdir" || fatal "Can't create log directory $artifactdir"
	    echo "${saved_argv[@]@Q}" > "$artifactdir/commandline"
	    # The process substitutions need to be protected against signals; otherwise they will terminate
	    # causing upstream broken pipes and loss of error data.
	    exec 2> >(trap '' TERM INT HUP USR1 USR2; stdbuf -i0 -o0 -e0 tee >(trap '' TERM INT HUP USR1 USR2; stdbuf -i0 -o0 -e0 tr "\r" "\n" > "$artifactdir/stderr.log") >&2)
	fi
    else
	artifactdir=
    fi
    setup_namespaces || return 1
    create_all_objects namespaces || return 1
    find_first_deployment || return 1
    if get_sync -q ; then
	if (( use_namespaces )) ; then
	    global_sync_service="svc-${sync_namespace}-sync.${sync_namespace}.svc.cluster.local"
	else
	    global_sync_service="svc-${basename}-sync-0-sync"
	fi
    fi

    (( ${#configmap_files[@]} )) && check_configmaps "${configmap_files[@]}"
    create_all_objects configmaps || return 1
    supports_api -w "$requested_workload" list_configmaps && has_system_configmap=1

    create_secrets_top_level || return 1

    basetime=$(date +%s.%N)
    (drop_host_caches)
    create_all_objects deployments || return 1
    (( doit )) || return 0
    set_run_started

    do_logging || return 1
    if ! get_run_failed || ((! status)) ; then
	report_object_creation
    fi
    return 0
}

function run_clusterbuster_1() {
    protect_pids "$BASHPID"
    local -i status=0
    if ((create_namespaces_only)) ; then
	setup_namespaces
    else
	# This needs to be run in a subshell so that we do not redirect the
	# stderr of the top level and lock things up.
	(run_clusterbuster_2)
	status=$?
	if (( (cleanup_always || (cleanup && status == 0)) && doit)) ; then
	    do_cleanup || {
		if ((status == 0)) ; then
		    status=1
		fi
	    }
	fi

	if ((status)) ; then
	    fatal -w "Clusterbuster failed!"
	else
	    if [[ $take_prometheus_snapshot -gt 0 && -n "$artifactdir" ]] ; then
		retrieve_prometheus_snapshot "$artifactdir"
	    fi
	fi
	get_run_failed && status=1
	return $status
    fi
}

################################################################
# Top level
################################################################

function remove_tmpdir() {
    if [[ -n "${cb_tempdir:-}" && -d "$cb_tempdir" && $cb_tempdir = "/tmp/cbtmp_"* ]] ; then
	if ((preserve_tmpdir)) ; then
	    warn "*** Preserving temporary directory $cb_tempdir"
	else
	    rm -rf "$cb_tempdir"
	fi
	unset cb_tempdir
    fi
}

function validate_resource() {
    local rtype=$1
    local token
    local status=0
    shift
    for token in "$@" ; do
	if [[ $token != *'='* ]] ; then
	    warn "Invalid $rtype specification $token (must be <resource>=<quantity>)"
	    status=1
	fi
    done
    return $status
}

function validate_volumes() {
    local -a vol_errors=()
    local -A mount_volume_map=()
    local -A mount_name_map=()
    local volspec
    for volspec in "${volumes[@]}" ; do
	local -a args
	IFS=: read -ra args <<< "$volspec"
	((${#args[@]} < 3)) && vol_errors+=("Volume name, type, and mountpoint must be specified for $volspec")
	local name="${args[0]}"
	local type="${args[1],,}"
	local mountpoint="${args[2]}"
	if [[ -z "$name" && -z "$type" && -z "$mountpoint" ]] ; then
	    mount_volume_map=()
	    continue
	fi
	mount_volume_map["$mountpoint"]="$volspec"
	[[ -n "$name" ]] && mount_name_map["$mountpoint"]="$name"
	# Remove a mountpoint from the list
	[[ -z "$type" ]] && continue
	args=("${args[@]:3}")
	case "${type}" in
	    pvc|persistentvolumeclaim)
		[[ -z "$name" ]] && warn "Name ignored for PVC: $volspec"
		;;
	    emptydisk|emptydir)
		[[ -n "$name" ]] && warn "Name ignored for $type volume: $volspec"
		;;
	    *)
		fatal "Unknown volume type $type"
		;;
	esac
	local -i size=
	for arg in "${args[@]}" ; do
	    if [[ $arg != *'='* ]] ; then
		vol_errors+=("Cannot parse volume argument $arg from $volspec")
	    fi
	    local opt
	    local value
	    IFS='=' read -r opt value <<< "$arg"
	    case "${opt,,}" in
		size)
		    size=$(parse_size "$value") || vol_errors+=("Cannot parse size $value for $volspec")
		    ((size <= 0)) && vol_errors+=("Volume size must be greater than zero for $volspec")
		    ;;
		*)
		    ;;
	    esac
	done
	if [[ $type = emptydisk && (-z "$size" || $size -le 0) ]] ; then
	    vol_errors+=("Positive size must be provided for emptydisk volumes ($volspec)")
	fi
    done
    local -A used_names=()
    local name
    for name in "${mount_name_map[@]}" ; do
	if [[ -n "$name" && -n "${used_names[$name]:-}" ]] ; then
	    vol_errors+=("Duplicate volume name $name")
	fi
	used_names[$name]=1
    done
    local nvolumes=()
    local -A used_volumes=()
    for volspec in "${volumes[@]}" ; do
	local -a args
	IFS=: read -ra args <<< "$volspec"
	local mountpoint="${args[2]}"
	if [[ -z "${used_volumes[$mountpoint]:-}" && $volspec = "${mount_volume_map[$mountpoint]}" ]] ; then
	    used_volumes[$mountpoint]=1
	    nvolumes+=("$volspec")
	fi
    done
    if [[ -n "${vol_errors[*]}" ]] ; then
	local errstr
	errstr=$(IFS=$'\n'; echo -e "The following errors occurred while validating volumes:\n${vol_errors[*]}")
	fatal "$errstr"
    fi
    volumes=("${nvolumes[@]}")
}

function validate_options() {
    [[ -z "$requested_workload" ]] && help "Workload must be specified with --workload"

    local iworkload=$requested_workload
    requested_workload=$(get_workload "$requested_workload") || help_extended "(Unknown workload '$iworkload')"
    arch=${arch:-$(uname -m)}

    call_api -w "$requested_workload" -s "process_options" "${unknown_opts[@]}" || help "${unknown_opt_names[@]/#/Unknown option }"

    if supports_api -w "$requested_workload" list_user_configmaps ; then
	readarray -t userfiles < <(list_user_configmaps)
	configmap_files+=("${userfiles[@]}")
    fi

    if (( namespaces <= 0 )) ; then
	namespaces=1
	use_namespaces=0
    fi
    (( parallel_configmaps )) || parallel_configmaps=$parallel
    (( parallel_secrets )) || parallel_secrets=$parallel
    (( parallel_namespaces )) || parallel_namespaces=$parallel
    (( parallel_deployments )) || parallel_deployments=$parallel

    (( !objs_per_call_configmaps )) && objs_per_call_configmaps=$objs_per_call
    (( !objs_per_call_secrets )) && objs_per_call_secrets=$objs_per_call
    (( !objs_per_call_namespaces )) && objs_per_call_namespaces=$objs_per_call
    (( !objs_per_call_deployments )) && objs_per_call_deployments=$objs_per_call

    (( sync_watchdog_timeout <= 0 )) && sync_watchdog_port_num=0

    [[ -n "$report_format" ]] && report=1

    if [[ -n "$metrics_file" && ! -r "$metrics_file" ]] ; then
	fatal "Cannot read metrics file $metrics_file"
    fi

    case "${deployment_type,,}" in
	# If we're using deployments or replicasets, we do not want
	# to exit when workers complete, or the controller will
	# immediately try to re-create them.
	replicaset|rs)         deployment_type=ReplicaSet; exit_at_end=0 ;;
	deployment|dep|deploy) deployment_type=Deployment; exit_at_end=0 ;;
	pod) deployment_type=pod                                         ;;
	vm)
	    if ((containers_per_pod > 1)) ; then
		help "ERROR: containers_per_pod must equal 1 for vm deployments"
	    fi
	    deployment_type=vm
	    runtime_class=vm
	    ;;
	*)
	    help "--deployment_type must be pod, vm, deployment, or replicaset"
	    ;;
    esac

    [[ -z "$OC" && $doit -gt 0 ]] && fatal "Cannot find oc or kubectl command, exiting!"

    resource_validation_failed=0
    validate_resource resource_request "${resource_requests[@]}" || resource_validation_failed=1
    validate_resource resource_limit "${resource_limits[@]}" || resource_validation_failed=1
    ((resource_validation_failed)) && exit 1

    validate_volumes

    job_name=${job_name:-${requested_workload:-}}

    case "$runtime_class" in
	vm)
	    if [[ -z "$vm_ssh_keyfile" ]] ; then
		ssh-keygen -C '' -t "$ssh_alg" -q -f "$cb_tempdir/id_${ssh_alg}" -N ''
		vm_ssh_keyfile="$cb_tempdir/id_${ssh_alg}"
	    fi
	    ;;
	kata)
	    ((virtiofsd_direct)) && virtiofsd_args+=('"-o"' '"allow_direct_io"')
	    ((virtiofsd_writeback)) && virtiofsd_args+=('"-o"' '"writeback"')
	    ((virtiofsd_threadpoolsize)) && virtiofsd_args+=("\"--thread-pool-size=$virtiofsd_threadpoolsize\"")
	    if [[ -n "${virtiofsd_args[*]:-}" ]] ; then
		pod_annotations+=("io.katacontainers.config.hypervisor.virtio_fs_extra_args: '[$(IFS=,; echo "${virtiofsd_args[*]}")]'")
	    fi
	    ;;
	*)
	    ;;
    esac
    if ((pod_start_timeout < 0)) ; then
	if [[ $deployment_type = vm ]] ; then
	    pod_start_timeout=$default_vm_start_timeout
	else
	    pod_start_timeout=$default_pod_start_timeout
	fi
    fi
}

trap 'if (($$ == BASHPID)) ; then wait; fi; remove_tmpdir; exit' EXIT
trap 'remove_tmpdir; if (($$ == BASHPID)) ; then wait; fi; exit 1' TERM INT HUP
cb_tempdir=$(umask 77; mktemp -d -p /tmp -t cbtmp_XXXXXXXX) || fatal "Can't create temporary directory"
export TMPDIR=$cb_tempdir

declare -a __workloads__
load_workloads "$CB_LIBPATH"

set_metrics_file default

while getopts ":B:Eef:Hhno:P:w:QqvN-:" opt ; do
    case "$opt" in
	B) process_option "basename=$OPTARG"	 ;;
	E) process_option "exit_at_end=0"	 ;;
	e) process_option "exit_at_end=1"	 ;;
	f) process_job_file "$OPTARG"		 ;;
	h) help					 ;;
	H) help_extended			 ;;
	N) process_option "createnamespacesonly=1";;
	n) process_option "doit=0"		 ;;
	o) process_option "reportformat=$OPTARG" ;;
	P) process_option "workload=$OPTARG"     ;;
	Q) process_option "reportobjectcreate=0" ;;
	q) process_option "verbose=0"		 ;;
	v) process_option "verbose=1"		 ;;
	w) process_option "workload=$OPTARG"     ;;
	-) process_option "$OPTARG"		 ;;
	*) help "Unknown option -$OPTARG"	 ;;
    esac
done

shift $((OPTIND - 1))

# shellcheck disable=SC2034
extra_args=("$@")

validate_options

(run_clusterbuster_1) 4>&2
