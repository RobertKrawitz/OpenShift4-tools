#!/bin/bash

# Copyright 2019-2022 Robert Krawitz/Red Hat
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Unfortunate that there's no way to tell shellcheck to always source
# specific files.
# shellcheck disable=SC2034

set -u

declare -A registered_workloads=()
declare -A workload_aliases=()
declare -i namespaces=1
declare -i use_namespaces=1
declare -i deps_per_namespace=1
declare -i secrets=0
declare -i replicas=1
declare -i parallel=1
declare -i first_deployment=0
declare -i sleep_between_secrets=0
declare -i sleep_between_configmaps=0
declare -i sleep_between_namespaces=0
declare -i sleep_between_deployments=0
declare -i parallel_secrets=0
declare -i parallel_configmaps=0
declare -i parallel_namespaces=0
declare -i parallel_deployments=0
declare -i objs_per_call=1
declare -i objs_per_call_secrets=0
declare -i objs_per_call_configmaps=0
declare -i objs_per_call_namespaces=0
declare -i objs_per_call_deployments=0
declare -i containers_per_pod=1
declare -i sleeptime=0
declare -i doit=1
declare -i objs_item_count=0
declare -i port=7777
declare -i sync_port=7778
declare log_host=
declare -i log_port=0
declare sync_host=
declare -i affinity=0
declare -A pin_nodes=()
declare -i verbose=0
declare -i wait_for_secrets=1
declare -i bytes_transfer=0
declare -i bytes_transfer_max=0
declare -i default_bytes_transfer=1000000000
declare -i poddelay=0
declare -i workload_run_time=0
declare -i workload_run_time_max=0
declare -i exit_at_end=1
declare -i report_object_creation=1
declare -A objects_created=()
declare baseoffset=0
declare requested_workload=ClusterBuster
declare basename=clusterbuster
declare deployment_type=pod
declare -i basetime
declare opt
declare -r nl=$'\n'
declare -a resource_requests=()
declare -a resource_limits=()
declare -A namespaces_in_use=()
declare -a namespaces_to_create=()
declare -i scale_ns=0
declare -i scale_deployments=1
declare -i sync_start=1
declare runtime_class=
declare -a emptydirs=()
declare -a volumes=()
declare -A volume_mount_paths=()
declare -A volume_types=()
declare -A volume_type_keys=()
declare -A volume_scoped_names=()
declare common_workdir=
declare -i report=0
declare report_format=summary
declare log_strategy=poll
declare -i precleanup=0
declare -i cleanup=0
declare -i timeout=-1
declare pathdir=${0%/*}
declare -a unknown_opts=()
declare -a unknown_opt_names=()
declare -i total_objects_created=0
# <name, filename with contents>
declare -a configmap_files=()
declare -a tolerations=()
declare configmap_mount_dir=/etc/map
declare system_configmap_mount_dir=/etc/bootstrap
declare -i has_system_configmap=0
declare node_selector='node-role.kubernetes.io/worker'
declare -i pbench_controller_port=0
declare pbench_controller_address=
declare -i processes_per_pod=1
declare -i emptydir_volumes=0
declare -i take_prometheus_snapshot=0
declare prometheus_starting_timestamp=
declare prometheus_ending_timestamp=
declare -i target_data_rate=0
declare metrics_file=
declare job_name=
declare debug=

declare -r sync_flag_file="/tmp/syncfile";
# shellcheck disable=2155
declare -r run_uuid=$(uuidgen -r)

declare kata_runtime_class=kata
declare image_pull_policy=Always
declare container_image=quay.io/rkrawitz/bench-army-knife

declare accumulateddata=
declare -a plurals=('s' '')
declare KUBECTL_VALIDATE=
declare artifactdir=
declare -a saved_argv=("$0" "$@")
declare -a processed_options=("$0")

declare OC=${OC:-${KUBECTL:-}}
OC=${OC:-$(type -p oc)}
OC=${OC:-$(type -p kubectl)}	# kubectl might not work, though...

function fatal() {
    echo "$*" 1>&2
    exit 1
}

function warn() {
    echo "$*" 1>&2
}

if [[ -z "$OC" ]] ; then
    fatal "Can't find kubectl or oc"
elif [[ $OC = 'kubectl'* ]] ; then
    KUBECTL_VALIDATE=--validate=false
fi

declare __me__
__me__=$(realpath -e "$0")

if [[ -z "$__me__" ]] ; then
    echo "Can't find my path!" 1>&2
    exit 1
fi

declare __mydir__=${__me__%/*}
declare __libdir__=${__mydir__}/lib/clusterbuster
declare __podfile_dir__=${__libdir__}/pod_files
declare __workloaddir__=${__libdir__}/workloads

[[ -d "$__libdir__" ]] || fatal "Can't find my library dir!"
[[ -d "$__workloaddir__" ]] || fatal "Can't find my workload directory $__workloaddir__!"

function print_workloads() {
    local prefix="${1:-}"
    local workloads
    while read -r workload ; do
	echo "$prefix$workload"
    done <<< "$(IFS=$'\n'; echo "${!registered_workloads[*]}" |sort)"
}

function print_workloads_supporting_reporting() {
    local prefix="${1:-}"
    local workloads
    while read -r workload ; do
	echo "$prefix$workload"
    done <<< "$(workloads_supporting_api supports_reporting)"
}

function _help_options_workloads() {
    local workload
    while read -r workload ; do
	if [[ -z "$workload" ]] ; then
	    continue
	fi
	echo
	call_api -w "$workload" help_options
    done <<< "$(workloads_supporting_api help_options)"
}

function _document_workloads() {
    local workload
    while read -r workload ; do
	if [[ -z "$workload" ]] ; then
	    continue
	fi
	echo
	call_api -w "$workload" document
    done <<< "$(workloads_supporting_api document)"
}

function list_report_formats() {
    local prefix=${1:-}
    while read -r format ; do
	echo "$prefix$format"
    done <<< "$("${pathdir:-.}/clusterbuster-report" --list_formats)"
}

function _helpmsg() {
    local opt
    for opt in "$@" ; do
	echo "Unknown option $opt"
    done
cat <<EOF
Clusterbuster is a tool to permit you to load a configurable workload
onto an OpenShift cluster.  ClusterBuster focuses primarily on workload
scalability, including synchronization of multi-instance workloads.

Usage: $0 [options] [name]

    Help:
       -h              Print basic help information.
       -H              Print extended help.

    Options:
       -B basename     Base name of pods (default clusterbuster).
                       All objects are labeled with this name.
       -E              Don't exit after all operations are complete.
       -e              Exit after all operations are complete (default).
       -f jobfile      Job file containing settings
                       A number of examples are provided in the
                       examples/clusterbuster directory.
       -n              Print what would be done without doing it
       -P workload     workload (mandatory), one of:
$(print_workloads '                       - ')
       -q              Do not print verbose log messages (default)
       -Q              Don't report creation of individual objects (default
                       report them)
       -v              Print verbose log messages.
       --opt[=val]     Set the specified option.
                       Use $0 -H to list the available options.
EOF
}

function _help_extended() {
    _helpmsg "$@"
    cat <<EOF

Extended Options:
    General Options (short equivalents):
       --doit=<1,0>     Run the command or not (default 1) (inverse of -n)
       --jobname=name   Name of the job, for logging purposes.
                        Defaults to the workload name
       --workload=type  Specify the workload (-P) (mandatory)
       --basename=name  Specify the base name for any namespaces (-B)
       --namespaces=N   Number of namespaces
       --jobfile=jobfile
			Process job file (-f)
       --sync           Synchronize start of workload instances (default yes)
       --precleanup     Clean up any prior objects
       --cleanup        Clean up generated objects
       --timeout=N      Time out reporting after N seconds
       --report_object_creation=<1,0>
                        Report creation of individual objects (default 1)
                        (inverse of -Q)
       --uuid=<uuid>    Use the specified UUID for the run.  Default is to
                        generate a random-based UUID.
       --exit_at_end    Exit upon completion of workload (-e/-E)
       --verbose        Print verbose log messages (-v)

    Reporting Options:
       --report-format=<format>
                        Print report in specified format.  Meaning of
                        report types is by type.  Default is summary
                        if not specified; if that is not reported,
                        raw format will be used.
                        - none
$(list_report_formats '                        - ')
                        The following workloads support reporting:
$(print_workloads_supporting_reporting '                        - ')
       --artifactdir=<dir>
                        Save artifacts to <dir>
       --prometheus-snapshot
                        Take a Prometheus snapshot and save to the
                        artifacts directory
       --metrics-file=<file>
                        benchmark-runner compatible metrics file
                        for metrics extraction if promextract
                        reporting is in use.
       --report         Print summary report (deprecated)
       --verbose-report Print detailed report (deprecated)
       --json-report    Print JSON report (deprecated)
       --raw-report     Print unprocessed report (deprecated)

    Workload sizing options:
       --containers_per_pod=N
                        Number of containers per pod
       --deployments=N  Number of deployments or pods per namespace
       --processes=N    Number of processes per pod
       --replicas=N     Number of replicas per deployment
       --secrets=N      Number of secrets

    Generic workload rate options:
       --bytestransfer=N[,M]
                        Number of bytes for workloads operating on
                        fixed amounts of data.
       --targetdatarate=N
                        Target data rate for workloads operating at fixed
                        data rates.  May have suffixes of K, Ki,
                        M, Mi, G, Gi, T, or Ti.
       --workloadruntime=N
                        Time to run the workload where applicable
                        Two comma-separated numbers may be used to
                        specify maximum time.

    Workload placement options:
       --pin_node=<class>=<node>
                        Force pod(s) of the specified class onto the
                        specified node.  The following classes are
                        defined for general workloads:
                        - sync   (sync pods)
                        - worker (worker pods)
                        Workloads may define other classes.
       --affinity       Force affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides affinity.
       --anti-affinity  Force anti-affinity between client and server pods
                        in a client-server workload.  Default is neither
                        affinity nor anti-affinity.  Use of a pin node
                        overrides anti-affinity.

    Generic workload storage options:
       --emptydir=dir   Mount an emptydir volume at the specified mount point.
       --volumes=N      Mount the specified number of emptydir volumes.
                        If this is not provided, file-based workloads will
                        use a default location, generally /tmp.
       --volume=name:type:name_type:mount_name:mount_path
                        Mount a specified persistent volume
                        name is the name of the volume (required).
                        type is the type of volume (required).
                        mount_path is the path on which to mount the volume
                            (required).
                        type_key is the key for the volume (e. g.
                            claimName for persistentVolumeClaim)
                        scoped_name is the volume's name as recognized
                            by the description.  All occurrences of %N
                            are replaced by the namespace of the pod
                            mounting the volume; all instances of %i
                            are replaced by the instance of the pod
                            within the namespace.
       --workdir=<dir>  Use the specified working directory for file I/O

    Pod Options:
       --container_image=<image>
                        Image to use (default $container_image).
                        Does not apply to "classic" or "pause" workloads.
       --deployment_type=<pod,deployment>
                        Deploy via individual pods or deployments (default $deployment_type)
       --external_sync=host:port
                        Sync to external host rather than internally
       --request=<resource=value>
                        Resource requests
       --limit=<resource=value>
                        Resource limits
       --runtimeclass=class
                        Run the pods in the designated runtimeclass.
       -K
       --kata           Synonym for --runtimeclass=${kata_runtime_class}
       --tolerate=<key:operator:effect>
                        Apply the specified tolerations to created pods.
       --image_pull_policy=<policy>
                        Image pull policy (default $image_pull_policy)
       --node_selector=selector
                        Annotate pods with the specified node selector
                        Default $node_selector
                        Specify empty value to not provide a node selector.

    Tuning object creation (short equivalents):
       --scale-ns=[0,1] Scale up the number of namespaces vs.
                        create new ones (default 0).
       --scale-deployments=[0,1]
                        Scale up the number of deployments vs.
                        create new ones (default 1)
       --first_deployment=N
                        Specify the index of the first deployment.
                        Default is 0 (but see --scale_deployments)
       --first_secret=N
                        Index number of first secret to be created
       --first_namespace=N
                        Index number of first namespace to be created
       --first_deployment=N
                        Index number of first pod to be created
       --objs_per_call=N  Number of objects per CLI call
                        Below options default to objs_per_call
       --objs_per_call_secrets=N
       --objs_per_call_namespaces=N
       --objs_per_call_deployments=N

       --sleep=N        Number of seconds between object creations
                        Below options default to sleeptime
       --sleep_between_secrets=N
       --sleep_between_namespaces=N
       --sleep_between_deployments=N

       --parallel=N     Number of operations in parallel
                        Below options default to parallel
       --parallel_secrets=N
       --parallel_namespaces=N
       --parallel_deployments=N
       --wait_secrets   Wait for secrets to be created (default 1)

    Using PBench:
       If pbench is to be used, both arguments below must be provided.
       --pbench_controller_address=address
                        Use the specified address to connect to the controller.
       --pbench_controller_port=port
                        Use the specified port to connect to the controller.

Workload-specific options:
$(_help_options_workloads)

    Advanced options (generally not required):
       --baseoffset=N   Add specified offset to base time
                        for calculation of start time offset
                        to correct for clock skew.  May be float.
       --log-strategy={poll,listen}
                        Use the desired logging strategy to retrieve logs.
                        - Poll indicates that sync pods should be polled
                          for logs.
                        - Listen indicates that pods should report log
                          data directly to this tool, which will listen
                          for the data.
                        The default strategy is polling, which should work
                        in all cases.  The older listen strategy will not
                        work if pods cannot connect to the host from which
                        this tool is run.  Polling should be used unless
                        there is reason to believe it is not functioning.
       --poddelay=N     Time in seconds to wait for pods to start data xfer
       --podsleep=N     Time for pod to sleep before exit

Here is a brief description of all available workloads:
$(_document_workloads)

EOF
}

function help() {
    _helpmsg "$@" | "${PAGER:-more}" 1>&2
    exit 1
}

function help_extended() {
    _help_extended "$@" | "${PAGER:-more}" 1>&2
    exit 1
}

################################################################
# Option processing
################################################################

function parse_size() {
    if [[ $1 =~ (-?[[:digit:]]+)([[:alpha:]]*) ]] ; then
	local size=${BASH_REMATCH[1]}
	local size_modifier=${BASH_REMATCH[2],,}
	local -i size_multiplier=1
	case "$size_modifier" in
	    ''|b)          size_multiplier=1                ;;
	    k|kb|kilobytes)  size_multiplier=1000           ;;
	    ki|kib|kibibytes) size_multiplier=1024          ;;
	    m|mb|megabytes)  size_multiplier=1000000        ;;
	    mi|mib|mebibytes) size_multiplier=1048576       ;;
	    g|gb|gigabytes)  size_multiplier=1000000000     ;;
	    gi|gib|gibibytes) size_multiplier=1073741824    ;;
	    t|tb|terabytes)  size_multiplier=1000000000000  ;;
	    ti|tib|tebibytes) size_multiplier=1099511627776 ;;
	    *) fatal "Cannot parse size $optvalue"          ;;
	esac
	echo $((size*size_multiplier))
    else
	fatal "Cannot parse filesize $optvalue"
    fi
}

function set_workload_bytes() {
    local sizespec=$1
    local -i scale=${2:-1}
    if [[ $sizespec = *','* ]] ; then
	bytes_transfer=$(parse_size "${sizespec#*,}")
	bytes_transfer_max=$(parse_size "${sizespec%%,*}")
	if (( bytes_transfer > bytes_transfer_max )) ; then
	    local -i tmp=$bytes_transfer
	    bytes_transfer=$bytes_transfer_max
	    bytes_transfer_max=$tmp
	fi
    else
	bytes_transfer=$((sizespec * scale))
	bytes_transfer_max=$((sizespec * scale))
    fi
}

function set_runtime() {
    local timespec=$1
    if [[ $timespec = *','* ]] ; then
	workload_run_time=${timespec#*,}
	workload_run_time_max=${timespec%%,*}
	if (( workload_run_time > workload_run_time_max )) ; then
	    local -i tmp=$workload_run_time
	    workload_run_time=$workload_run_time_max
	    workload_run_time_max=$tmp
	fi
    else
	workload_run_time=$timespec
	workload_run_time_max=$timespec
    fi
}

function parse_volume_spec() {
    local volspec=$1
    local vname=
    local vtype=
    local vmount_path=
    IFS=':' read -r vname vtype vmount_path vtype_key vscoped_name <<< "$volspec"
    if [[ -z "$vname" || -z "$vtype" || -z "$vmount_path" ]] ; then
	echo "name, type, type_key, scoped name, and mount path must be provided"
	echo "for volumes"
	exit 1
    fi
    volumes+=("$vname")
    volume_mount_paths["$vname"]=$vmount_path
    volume_types["$vname"]=$vtype
    volume_type_keys["$vname"]=$vtype_key
    volume_scoped_names["$vname"]=$vscoped_name
}

function bool() {
    local value=${1:-}
    case "${value,,}" in
	''|1|y|yes|tru*) echo 1 ;;
	*)               echo 0 ;;
    esac
}

function parse_option() {
    local option=$1
    option=${option// /}
    [[ -n "$option" ]] || return
    local optname
    local optvalue
    optname=${option%%=*}
    optname=${optname,,}
    optvalue=${option#*=}
    noptname=${optname//-/_}
    if [[ $option != *'='* ]] ; then
	if [[ $noptname = "no_"* || $optname = "dont_"* ]] ; then
	    noptname=${noptname#dont_}
	    noptname=${noptname#no_}
	    optvalue=0
	else
	    optvalue=1
	fi
    fi
    local noptname1=${noptname//_/}
    echo "$noptname1 $noptname $optvalue"
}

function process_pin_node() {
    local nodespec=$1
    nodespec=${nodespec// /}
    [[ -n "$nodespec" ]] || return
    if [[ $nodespec != *'='* ]] ; then
	fatal "Pin node specifier must be of the form <class>=<node>"
    fi
    local class=${nodespec%%=*}
    local node=${nodespec#*=}
    pin_nodes["$class"]="$node"
}

function process_option() {
    local noptname
    local noptname1
    local optvalue
    read -r noptname1 noptname optvalue <<< "$(parse_option "$1")"
    # shellcheck disable=SC2206
    # shellcheck disable=SC2119
    processed_options+=("--$1")
    case "$noptname1" in
	# Help, verbosity
	helpall*)		    help_extended				;;
	helpeverything*)	    help_extended				;;
	help*)			    help					;;
	verbose)		    verbose=$(bool "$optvalue")			;;
	doit)			    doit=$(bool "$optvalue")			;;
	quiet)			    verbose=$((! $(bool "$optvalue")))		;;
	# Reporting
	artifactdir)		    artifactdir="$optvalue"			;;
	logstrategy)		    log_strategy=$optvalue			;;
	metricsfile)		    metrics_file="$optvalue"			;;
	reportformat)		    report_format=$optvalue			;;
	jsonreport)		    report_format=json				;;
	rawreport)		    report_format=raw				;;
	report)		    	    report_format=summary			;;
	verbosereport)		    report_format=verbose			;;
	reportobjectcreation)	    report_object_creation=$(bool "$optvalue")	;;
	prometheussnapshot)	    take_prometheus_snapshot=$(bool "$optvalue");;
	timeout)		    timeout=$optvalue				;;
	# Basic options
	jobname)		    job_name=$optvalue				;;
	workload)		    requested_workload=$optvalue		;;
	basename)		    basename=$optvalue				;;
	# Object definition
	workdir)		    common_workdir=$optvalue			;;
	configmapfile)		    configmap_files+=("$optvalue")		;;
	containerimage)		    container_image=$optvalue			;;
	containers)		    containers_per_pod=$optvalue		;;
	containersperpod)	    containers_per_pod=$optvalue		;;
	deploymenttype)		    deployment_type=$optvalue			;;
	deployments|depspername*)   deps_per_namespace=$optvalue		;;
	emptydir)		    emptydirs+=("$optvalue")			;;
	volumes)		    emptydir_volumes=$optvalue			;;
	exitatend)		    exit_at_end=$(bool "$optvalue")		;;
	imagepullpolicy)	    image_pull_policy=$optvalue			;;
	namespaces)		    namespaces=$optvalue			;;
	nodeselector)		    node_selector=$optvalue			;;
	volume)			    parse_volume_spec "$optvalue"		;;
	pbenchcontrolleraddress)    pbench_controller_address=$optvalue		;;
	pbenchcontrollerport)	    pbench_controller_port=$optvalue		;;
	processes|processesperpod)  processes_per_pod=$optvalue			;;
	jobfile)		    process_job_file "$optvalue"		;;
	pinnode)		    process_pin_node "$optvalue"		;;
	replicas)		    replicas=$optvalue				;;
	limit)			    resource_limits+=("$optvalue")		;;
	request)		    resource_requests+=("$optvalue")		;;
	kata)			    runtime_class=${kata_runtime_class}		;;
	runtimeclass)		    runtime_class=$optvalue			;;
	uuid)			    run_uuid=$optvalue				;;
	secrets)		    secrets=$optvalue				;;
	workloadruntime)	    set_runtime "$optvalue"			;;
	workload_size)		    set_workload_bytes "$optvalue"		;;
	targetdatarate)		    target_data_rate=$(parse_size "$optvalue")	;;
	tolerate|toleration)	    tolerations+=("$optvalue")			;;
	affinity)
	    case "$optvalue" in
		1|'') affinity=1      ;;
		2|anti) affinity=2    ;;
		*) affinity=0         ;;
	    esac
	    ;;
	antiaffinity)
	    case "$optvalue" in
		1|'') affinity=2      ;;
		*) affinity=0         ;;
	    esac
	    ;;
	# Object creation
	objspercall)		    objs_per_call=$optvalue			;;
	parallel)		    parallel=$optvalue				;;
	sleep)			    sleeptime=$optvalue				;;
	poddelay)		    poddelay=$optvalue				;;
	firstdeployment)	    first_deployment=$optvalue			;;
	parallelconfigmaps)	    parallel_configmaps=$optvalue		;;
	parallelsecrets)	    parallel_secrets=$optvalue			;;
	parallelnamespaces)	    parallel_namespaces=$optvalue		;;
	paralleldeployments)	    parallel_deployments=$optvalue		;;
	objspercallconfigmaps)	    objs_per_call_configmaps=$optvalue		;;
	objspercallsecrets)	    objs_per_call_secrets=$optvalue		;;
	objspercallnamespaces)	    objs_per_call_namespaces=$optvalue		;;
	objspercalldeployments)	    objs_per_call_deployments=$optvalue		;;
	sleepbetweenconfigmaps)	    sleep_between_configmaps=$optvalue		;;
	sleepbetweensecrets)	    sleep_between_secrets=$optvalue		;;
	sleepbetweennamespaces)	    sleep_between_namespaces=$optvalue		;;
	sleepbetweendeployments)    sleep_between_deployments=$optvalue		;;
	waitsecrets)		    wait_for_secrets=$(bool "$optvalue")	;;
	scalens)	   	    scale_ns=$(bool "$optvalue")		;;
	scaledeployments)   	    scale_deployments=$(bool "$optvalue")	;;
	precleanup)		    precleanup=$(bool "$optvalue")		;;
	cleanup)		    cleanup=$(bool "$optvalue")			;;
	baseoffset)		    baseoffset=$optvalue			;;
	# Synchronization
	sync|syncstart)		    sync_start=$((1-sync_start))			;;
	externalsync)
	    if [[ $optvalue =~ ^(-|([[:alnum:]][-_[:alnum:]]*[[:alnum:]]\.)*([[:alnum:]][-_[:alnum:]]*[[:alnum:]])):([1-9][[:digit:]]{0,4})$ ]] ; then
		sync_host=${BASH_REMATCH[1]}
		sync_port=${BASH_REMATCH[4]}
		if (( sync_port > 65535 )) ; then
		    echo "Illegal external sync port (must be 1 <= port <= 65535)"
		    help
		fi
		sync_start=1
	    else
		echo "Undecipherable external sync host:port $optvalue"
		help
	    fi
	    ;;
	# Unknown options
	*)
	    unknown_opts+=("$1")
	    unknown_opt_names+=("$noptname ($noptname1)") ;;
    esac
}

function process_job_file() {
    local jobfile="$1"
    if [[ ! -f $jobfile || ! -r $jobfile ]] ; then
	fatal "Job file $jobfile cannot be read"
    fi
    while IFS= read -r line ; do
	line=${line%%#*}
	line=${line## }
	line=${line##	}
	if [[ -z "$line" ]] ; then continue; fi
	process_option "$line"
    done < "$jobfile"
}

function validate_resource() {
    local rtype=$1
    local token
    local status=0
    shift
    for token in "$@" ; do
	if [[ $token != *'='* ]] ; then
	    warn "Invalid $rtype specification $token (must be <resource>=<quantity>)"
	    status=1
	fi
    done
    return $status
}

################################################################
# Workload management
################################################################

function load_workloads() {
    local workload
    for workload in "${__workloaddir__}"/*.workload ; do
	. "$workload" || fatal "Can't load workload $workload"
    done
}

function dispatch_generic() {
    local -i probe_only=0
    if [[ $1 = '-p' ]] ; then
	probe_only=1
	shift
    fi
    local workload=$1
    local API=$2
    shift 2
    if type -t "${API}_${workload}" >/dev/null ; then
	((probe_only)) && return 0
	"${API}_${workload}" "$@"
    else
	return 1
    fi
}

function register_workload() {
    local workload_name=$1
    local workload_dispatch_function=$2
    if [[ ! $workload =~ [[:alpha:]][[:alnum:]_]+ ]] ; then
	fatal "Illegal workload name $workload"
    elif [[ -n "${workload_aliases[${workload_name,,}]:-}" ]] ; then
	fatal "Attempting to re-register workload $workload_name"
    elif ! type -t "$workload_dispatch_function" >/dev/null ; then
	fatal "Workload dispatch function $workload_dispatch_function not defined"
    fi
    registered_workloads[$workload_name]=$workload_dispatch_function
    workload_aliases[${workload_name,,}]=$workload_name
    shift 2
    local walias
    for walias in "$@" ; do
	if [[ -n "${registered_workloads[$walias]:-}" || -n "${workload_aliases[${walias,,}]:-}" ]] ; then
	    fatal "Attempting to register workload alias that already exists"
	fi
	workload_aliases[${walias,,}]=${workload_name}
    done
}

function supports_api() {
    local workload
    local OPTIND=0
    workload="$requested_workload"
    while getopts 'w:' opt "$@" ; do
	case "$opt" in
	    w) workload=$OPTARG ;;
	    *)			;;
	esac
    done
    shift $((OPTIND - 1))
    local api=$1
    [[ -z "$workload" ]] && return 1
    [[ -z "${registered_workloads[$workload]:-}" ]] && fatal "supports_api $workload $api: workload not known"
    "${registered_workloads[$workload]}" -p "$workload" "$api"
}

function workloads_supporting_api() {
    local api=$1
    local workload
    while read -r workload ; do
	supports_api -w "$workload" "$api" && echo "$workload"
    done <<< "$(print_workloads)"
}

function call_api() {
    (( $# < 1 )) && fatal "call_api [-w workload] API args..."
    local workload
    local -i safe=0
    local OPTIND=0
    workload="$requested_workload"
    while getopts 'w:s' opt "$@" ; do
	case "$opt" in
	    w) workload=$OPTARG ;;
	    s) safe=1		;;
	    *)			;;
	esac
    done
    shift $((OPTIND - 1))
    local api=$1
    if supports_api -w "$workload" "$api" ; then
	"${registered_workloads[$workload]}" "$workload" "$@"
    elif ((safe)) ; then
	return 0
    else
	return 1
    fi
}

function json_encode_settings() {
    call_api json_encode_settings
}

function create_deployment() {
    call_api create_deployment "$@"
}

function calculate_logs_required() {
    if supports_api supports_reporting ; then
	call_api calculate_logs_required "$@"
    else
	echo 1
    fi
}

function list_configmaps() {
    call_api list_configmaps
}

function list_user_configmaps() {
    call_api list_user_configmaps
}

################################################################
# Helpers
################################################################

function __OC() {
    if ((! doit)) ; then
	echo "$OC" "$@" 1>&2
    elif [[ $1 = describe || $1 = get || $1 = status || $1 = exec || $1 = logs ]] ; then
	"$OC" "$@"
    elif ((report_object_creation)) ; then
	if [[ -n "$debug" ]] ; then
	    echo "$OC" "$@" 1>&2
	fi
	"$OC" "$@" 2>&1
    else
	"$OC" "$@" 2>&1 |grep -v -E '(^No resources found|deleted|created|labeled|condition met)$'
    fi
    return "${PIPESTATUS[0]}"
}

function _OC() {
    if ! __OC "$@" ; then
	echo "__KUBEFAIL__ $OC $* failed!"
	exit 1
    fi
}

function get_sync() {
    if (( sync_start )) ; then
	if [[ ${1:-} != -q ]] ; then
	    echo "${sync_host:-service-${namespace}-sync}:$sync_port"
	fi
	return 0
    else
	return 1
    fi
}

function create_sync_service_if_needed() {
    local namespace=$1
    local sync_count=$2
    local sync_clients=$3
    if get_sync -q ; then
	create_service "$namespace" "${namespace}-sync" "$sync_port"
	create_sync_deployment "$namespace" "$sync_count" "$sync_clients"
    fi
}

################################################################
# Logging
################################################################

function get_prometheus_pod_timestamp() {
    _OC exec -n openshift-monitoring prometheus-k8s-0 -c prometheus -- /bin/sh -c 'date +%s'
}

function readable_timestamp() {
    local rawtime=$1
    date -u '+%Y_%m_%dT%H_%M_%S%z' "--date=@$rawtime"
}

function start_prometheus_snapshot() {
    ((doit)) || return 0
    echo "Starting Prometheus snapshot" | echo_if_desired 1>&2
    _OC delete pod -n openshift-monitoring prometheus-k8s-0
    _OC wait --for=condition=Ready -n openshift-monitoring pod/prometheus-k8s-0 || fatal "Prometheus pod did not go ready"
    prometheus_starting_timestamp=$(get_prometheus_pod_timestamp)
    # Wait for prometheus pod to fully initialize
    sleep 60
    echo "Prometheus snapshot started" | echo_if_desired 1>&2
}

function retrieve_prometheus_snapshot() {
    ((doit)) || return 0
    echo "Retrieving Prometheus snapshot" | echo_if_desired 1>&2
    local dir=${1:-$artifactdir}
    sleep 60
    prometheus_ending_timestamp=$(get_prometheus_pod_timestamp)
    local promdb_name
    promdb_name="promdb_$(readable_timestamp "$prometheus_starting_timestamp")_$(readable_timestamp "$prometheus_ending_timestamp")"
    local promdb_path="${dir:+${dir}/}${promdb_name}.tar"
    __OC exec -n openshift-monitoring prometheus-k8s-0 -c prometheus -- /bin/sh -c "tar cf - . -C /prometheus --transform 's,^[.],./${promdb_name},' .; true" > "$promdb_path"
    echo "Prometheus snapshot retrieved" | echo_if_desired 1>&2
}

function ts() {
    local dt
    dt=$(date '+%s.%N')
    local sec=${dt%.*}
    local ns=${dt#*.}
    echo "${sec}.${ns:0:6}"
}

function timestamp() {
    while IFS= read -r 'LINE' ; do
	printf "%s %s\n" "$(TZ=GMT-0 date '+%Y-%m-%dT%T.%N' | cut -c1-26)" "$LINE"
    done
}

trap exit INT

function monitor_pods() {
    exec 0</dev/null 1>/dev/null
    local arg
    local -i timeout=-1
    local -i ppid=0
    local OPTIND=0
    while getopts "t:p:" arg "$@" ; do
	case "$arg" in
	    p) ppid="$OPTARG"    ;;
	    t) timeout="$OPTARG" ;;
	    *)                   ;;
	esac
    done
    shift $((OPTIND - 1))
    if ((timeout > 0)) ; then
	timeout=$(($(date +%s) + timeout))
    fi
    while : ; do
	# shellcheck disable=2034
	while read -r namespace name rdy status rest ; do
	    case "${status,,}" in
		error|failed)
		    echo "Pod -n $namespace $name $status!" 1>&2
		    echo "Tail end of logs:" 1>&2
		    oc logs -n "$namespace" "$name" |tail -20 1>&2
		    if (( ppid > 0 )) ; then
			exec 2>/dev/null
			kill -USR1 "$ppid"
		    fi
		    return 1
		    ;;
		*)  ;;
	    esac
	done <<< "$(__OC get pods -A --no-headers "$@" 2>/dev/null)"
	if ((timeout > 0 && $(date +%s) > timeout)) ; then
	    echo "Monitor pods timeout!" 1>&2
	    if (( ppid > 0 )) ; then
		exec 2>/dev/null
		kill -USR1 "$ppid"
	    fi
	    return 1
	fi
	sleep 5
    done
}

function run_logger() {
    local -a pids_to_kill=()
    local OPTIND=0
    while getopts "p:" arg "$@" ; do
	case "$arg" in
	    p) pids_to_kill+=("$OPTARG") ;;
	    *)                           ;;
	esac
    done
    shift $((OPTIND-1))
    "$@" </dev/null &
    trap '[[ -n "${pids_to_kill[*]}" ]] && exec 2>/dev/null && kill $(jobs -p) && kill "${pids_to_kill[@]}" && return 1' TERM INT HUP USR1 USR2
    # shellcheck disable=2207
    pids_to_kill+=($(jobs -p))
    wait
    # shellcheck disable=SC2046
    if [[ -n "${pids_to_kill[*]}" ]] ; then
	exec 2>/dev/null
	kill -USR2 "${pids_to_kill[@]}"
	[[ -n "$(jobs -p)" ]] && kill $(jobs -p)
    fi
}

function get_logs_poll() {
    local pod
    local status
    local counter=$#
    for pod in "$@" ; do
	pod_status_found=0
	while : ; do
	    # shellcheck disable=SC2086
	    status=$(__OC get pod $pod -o jsonpath="{.status.phase}" 2>/dev/null)
	    case "$status" in
		Running)
		    break
		    ;;
		Pending)
		    pod_status_found=1
		    sleep 1
		    ;;
		Error|Failed)
		    pod_status_found=1
		    echo "Status of pod $pod failed!" 1>&2
		    return 1
		    ;;
		''|Succeeded)	# Might be an old pod not yet deleted
		    if ((pod_status_found)) ; then
			echo "Unexpected status '$status' of pod $pod!" 1>&2
			return 1
		    else
			sleep 1
		    fi
		    ;;
		*)
		    pod_status_found=1
		    echo "Unexpected status '$status' of pod $pod!" 1>&2
		    return 1
		    ;;
	    esac
	done
	# shellcheck disable=SC2086
	local cleanup_command="; sleep 1; rm -f '$sync_flag_file'"
	local mypid=$BASHPID
	monitor_pods -p "$mypid" -- -l "${basename}-id=$run_uuid" &
	# shellcheck disable=SC2086
	run_logger -p "$mypid" "$OC" exec $pod -- sh -c "while [[ ! -f '$sync_flag_file' ]] ; do sleep 5; done; cat '$sync_flag_file' $cleanup_command" &
	# We need to kill our parent, since it's capturing output in a way that it can't
	# see that we failed.
	trap '[[ -n "$(jobs -p)" ]] && kill $(jobs -p) ${__PARENT_PID__}; return 2' INT TERM HUP
	trap 'kill $(jobs -p) ${__PARENT_PID__}; return 3' USR1
	trap 'kill $(jobs -p) && wait' USR2
	wait
	# shellcheck disable=SC2046
	if [[ -n "$(jobs -p)" ]] ; then
	    exec 3>&2 2>/dev/null
	    kill $(jobs -p)
	    wait
	    exec 2>&3 3>&-
	fi
	if ((--counter > 0)) ; then
	    echo ','
	fi
    done
}

function get_logs_listen() {
    "${__libdir__}/get_logs" "$@"
}

function __report_one_volume() {
    local volname=$1
	cat <<EOF
  {
     "name": "$volname",
     "mount_path": "${volume_mount_paths[$volname]}",
     "type": "${volume_types[$vname]}",
     "type_key": "${volume_types[$vname]}",
     "scoped_name": "${volume_scoped_names[$vname]}"
  }
EOF
}

function _report_volumes() {
    echo '"volumes": ['
    local -a vols=()
    for volname in "${volumes[@]}" ; do
	vols+=("$(__report_one_volume "$volname")")
    done
    (IFS=$',\n'; echo "${vols[*]}")
    echo '],'
}

function _report_emptydirs() {
    if ((${#emptydirs[@]})) ; then
	echo "emptydirVolumes: ["
	local -a ed=("${emptydirs[@]}")
	ed=("${ed[@]/#/}")
	ed=("${ed[@]/%/}")
	(IFS=$',\n'; echo "${ed[*]}")
	echo "],"
    fi
}

function quote_list() {
    local -a a=("$@")
    a=("${a[@]//\"/\\\"}")
    a=("${a[@]//\$'\n'/ }")
    a=("${a[@]/#/\"}")
    a=("${a[@]/%/\"}")
    (IFS=$',\n '; echo "${a[*]}")
}

function _report_pin_nodes() {
    local -a nodes
    local class
    for class in "${!pin_nodes[@]}" ; do
	nodes+=("\"$class\": \"${pin_nodes[$class]}\"")
    done
    (IFS=$',\n '; echo "${nodes[*]}")
}

function _extract_metrics() {
    if [[ -r "$metrics_file" ]] ; then
	cat <<EOF
,
"metrics": $("${__mydir__}/experimental/prom-extract" -m "$metrics_file" --metrics-only --start_time="$prometheus_starting_timestamp" --post-settling-time=0)
EOF
    fi
}

function _get_logs() {
    # We need to be killed by our logger, since we're capturing output
    # in a way that we can't see that it failed.
    export __PARENT_PID__=${BASHPID}
    cat <<EOF
{
"metadata": {
  "kind": "clusterbusterResults",
  "job_start_time": "$(readable_timestamp "${prometheus_starting_timestamp}")",
  "job_name": "$job_name",
  "run_uuid": "$run_uuid",
  "workload": "$requested_workload",
  "kubernetes_version": $(__OC version -ojson),
  "command_line": [$(quote_list "${saved_argv[@]}")],
  "expanded_command_line": [$(quote_list "${processed_options[@]}")],
  "runHost": "$(hostname -f)",
  "options": {
    "basename": "$basename",
    "containers_per_pod": $containers_per_pod,
    "deployments_per_namespace": $deps_per_namespace,
    "namespaces": $namespaces,
    "bytes_transfer": $bytes_transfer,
    "bytes_transfer_max": $bytes_transfer_max,
    "workload_run_time": $workload_run_time,
    "workload_run_time_max": $workload_run_time_max,
    "pin_nodes": {$(_report_pin_nodes)},
    $(indent 4 _report_volumes)
    "secrets": $secrets,
    "replicas": $replicas,
    "container_image": "$container_image",
    "node_selector": "$node_selector",
    $(indent 4 container_resources_json)
    "runtime_class": "$runtime_class",
    $(indent 4 _report_emptydirs)
    "target_data_rate": $target_data_rate,
    "workloadOptions": {
    $(indent 4 call_api "report_options")
    }
  }
},
"nodes": $(__OC get nodes -ojson),
"api_objects": $( __OC get all -A -l "clusterbuster-id=$run_uuid" -ojson | jq -r '[foreach .items[]? as $item ([[],[]];0; {kind: $item.kind, namespace: $item.metadata.namespace, name: $item.metadata.name, nodeName: $item.spec.nodeName, labels: $item.metadata.labels})]'),
"csvs": $(__OC get csv -A -ojson | jq -r '[foreach .items[]? as $item ([[],[]];0; {name: $item.metadata.name, namespace: $item.metadata.namespace, version: $item.spec.version})]'),
"Results": [
$("$@")
]
$(_extract_metrics)
}
EOF
    if [[ -n "$artifactdir" ]] ; then
	while read -r namespace pod container ; do
	    mkdir -p "$artifactdir/Logs"
	    __OC logs -n "$namespace" -c "$container" "$pod" > "$artifactdir/Logs/${namespace}:${pod}:${container}"
	done <<< "$(_OC get pod -A -l "clusterbuster-id=$run_uuid" -ojson | jq -r '[foreach .items[]? as $item ([[],[]];0; (if ($item.kind == "Pod") then ([foreach $item.spec.containers[]? as $container ([[],[]];0; $item.metadata.namespace + " " + $item.metadata.name + " " + $container.name)]) else null end))] | flatten | map (select (. != null))[]')"
    fi
}

function print_report() {
    if [[ $report_format == raw ]] ; then
	cat
    else
	"${pathdir:-.}/clusterbuster-report" ${report_format:+-o "$report_format"}
    fi
}

function get_logs() {
    local extra_arg=
    if [[ ${report_format,,} = none ]] ; then
	return
    fi
    local log_cmd
    if supports_api supports_reporting ; then
	log_cmd=get_logs_poll
	if [[ ${log_strategy,,} = 'listen'* ]] ; then
	    log_cmd=get_logs_listen
	fi
    else
	log_cmd=true
    fi
    local report_cmd=cat
    if [[ -z "$*" ]] ; then
	echo "Workload $requested_workload does not support reporting" 1>&2
    elif [[ -n "$artifactdir" ]] ; then
	_get_logs "$log_cmd" "$@" | tee "$artifactdir/clusterbuster-report.json" | print_report
    else
	_get_logs "$log_cmd" "$@" | print_report
    fi
    return $((PIPESTATUS[0] || PIPESTATUS[1]))
}

################################################################
# YAML fragment generation
################################################################

function _indent() {
    local -i column="$1"
    local line
    while IFS='' read -r 'line' ; do
	[[ -z "$line" ]] || printf "%${column}s%s\n" ' ' "$line"
    done
}

function indent() {
    local -i column="$1"
    shift
    if [[ -n "$*" ]] ; then
	# "$@" | _indent "$column" strips whitespace with bash 4.2
	_indent "$column" < <("$@")
    else
	_indent "$column"
    fi
}

function tolerations_yaml() {
    local toleration
    (( ${#tolerations[@]} )) || return
    echo "tolerations:"
    for toleration in "${tolerations[@]}" ; do
	# shellcheck disable=SC2206
	local -a tol=(${toleration//:/ })
	cat <<EOF
- key: "${tol[0]:-}"
  operator: "${tol[1]:-}"
  effect: "${tol[2]:-}"
EOF
    done
}

function container_resources_yaml() {
    function resources() {
	local token
	for token in "$@" ; do
	    local resource=${token%%=*}
	    local value=${token#*=}
	    echo "$resource: $value"
	done
    }
    if (( ${#resource_limits[@]} + ${#resource_requests[@]} )) ; then
	echo "resources:"
	if (( ${#resource_limits[@]} )) ; then
	    echo "  limits:"
	    indent 4 resources "${resource_limits[@]}"
	fi
	if (( ${#resource_requests[@]} )) ; then
	    echo "  requests:"
	    indent 4 resources "${resource_requests[@]}"
	fi
    fi
}

function resources() {
    local token
    for token in "$@" ; do
	local resource=${token%%=*}
	local value=${token#*=}
	echo "\"$resource\": $value"
    done
}

function resource_json() {
    local request_type=$1
    shift
    echo "\"$request_type\": {"
    local -a resources=()
    local resource
    for resource in "$@" ; do
	resources+=("($(resources "$resource"))")
    done
    (IFS=$',\n'; echo "${resources[*]}")
    echo "}"
}

function container_resources_json() {
    local -a res_data=()
    if (( ${#resource_limits[@]} + ${#resource_requests[@]} )) ; then
	echo '"resources": {'
	if (( ${#resource_limits[@]} )) ; then
	    res_data+=("$(resource_json limits "${resource_limits[@]}")")
	fi
	if (( ${#resource_requests[@]} )) ; then
	    res_data+=("$(resource_json requests "${resource_requests[@]}")")
	fi
	(IFS=$',\n'; echo "${res_data[@]}")
	echo "},"
    fi
}

function standard_labels_yaml() {
    local workload=${1:-}
    local namespace=${2:-}
    local instance=${3:-}
    local logger=${4:+logger}
    # Different versions of bash expand quotes inside the ${logger:+...} differently.
    local true='"true"'
    cat <<EOF
labels:
  clusterbuster-run_uuid: "$run_uuid"
  ${basename}-id: "$run_uuid"
  ${basename}: "true"
  clusterbusterbase: "true"
EOF
    if [[ -n "$workload" ]] ; then
	cat <<EOF
  name: "${namespace}-${workload}-${instance}"
  app: "${namespace}-${workload}-${instance}"
  ${basename}-${workload}: "true"
${logger:+  ${basename}-logger: $true}
EOF
    fi
}

function bootstrap_command_yaml() {
    local command=$1
    cat <<EOF
command:
- bootstrap.sh
args:
- "$system_configmap_mount_dir/$command"
EOF
}

function volume_mounts_yaml() {
    local namespace=$1
    local deployment=${2:-1}
    local secrets=${3:-1}
    (( secrets + ${#emptydirs[@]} + ${#configmap_files[@]} + ${#volumes[@]} + has_system_configmap )) || return;
    local -i i
    echo volumeMounts:
    for i in $(seq 0 $((secrets - 1))) ; do
	local name="secret-${namespace}-${deployment}-$i"
	cat <<EOF
- name: $name
  mountPath: /etc/$name
  readOnly: true
EOF
    done
    if [[ -n "${emptydirs[*]:-}" ]] ; then
	local vol
	for vol in "${emptydirs[@]}" ; do
	    cat <<EOF
- name: ${vol##*/}
  mountPath: "$vol"
EOF
	done
    fi
    if [[ -n "${configmap_files[*]:-}" ]] ; then
	cat <<EOF
- name: "configmap-${namespace}"
  mountPath: "${configmap_mount_dir}"
  readOnly: true
EOF
    fi
    if [[ -n "${volumes[*]:-}" ]] ; then
	local vol
	for vol in "${volumes[@]:-}" ; do
	    cat <<EOF
- name: "$vol"
  mountPath: "${volume_mount_paths[$vol]}"
EOF
	done
    fi
    if (( has_system_configmap )) ; then
	cat <<EOF
- name: "systemconfigmap-${namespace}"
  mountPath: "${system_configmap_mount_dir}"
  readOnly: true
EOF
    fi
}

function pbench_agent_yaml() {
    if (( pbench_controller_port > 0 )) ; then
	# This isn't ideal, as there still can be collisions.
	# Without a port server, this is certainly the most
	# expeditious.
	local ssh_port=0
	while (( ssh_port < 2023 )) ; do
	    ssh_port=$RANDOM
	done
	cat <<EOF
- name: "pbench-agent"
  image: "$container_image"
  imagePullPolicy: $image_pull_policy
  securityContext:
    privileged: false
    capabilities:
      add:
      - AUDIT_WRITE
  env:
  - name: VERBOSE
    value: "0"
  command:
  - "/usr/local/bin/run-pbench-agent-container"
  args:
  - "$pbench_controller_port"
  - "$pbench_controller_address"
#hostPID: true
serviceAccount: pbench
EOF
    fi
}

function privilege_yaml() {
    if (( pbench_controller_port > 0)) ; then
	cat <<EOF
openshift.io/scc: privileged
EOF
    fi
}

function namespace_yaml() {
    local namespace=$1
    (( use_namespaces )) && echo "namespace: \"$namespace\""
}

function expand_volume() {
    local scoped_name=$1
    local namespace=${2:+-$2}
    local instance=${3:+-$3}
    scoped_name=${scoped_name//%N/$namespace}
    scoped_name=${scoped_name//%i/$instance}
    echo "$scoped_name"
}

function volumes_yaml() {
    local namespace=$1
    local instance=${2:-1}
    local secrets=${3:-1}
    (( secrets + ${#emptydirs[@]} + ${#configmap_files[@]} + ${#volumes[@]} + has_system_configmap )) || return;
    local -i i
    echo "volumes:"
    for i in $(seq 0 $((secrets - 1))) ; do
	local name="secret-${namespace}-${instance}-$i"
	cat<<EOF
- name: $name
  secret:
    secretName: $name
EOF
    done

    if [[ -n "${emptydirs[*]:-}" ]] ; then
	local vol
	for vol in "${emptydirs[@]}" ; do
	    cat <<EOF
- name: ${vol##*/}
  emptydDir: {}
EOF
	done
    fi
    if [[ -n "${configmap_files[*]:-}" ]] ; then
	cat <<EOF
- name: "configmap-${namespace}"
  configMap:
    name: "configmap-${namespace}"
EOF
    fi
    if [[ -n "${volumes[*]:-}" ]] ; then
	local vol
	for vol in "${volumes[@]:-}" ; do
	    cat <<EOF
- name: "$vol"
EOF
	    if [[ -n "${volume_type_keys[$vol]}" ]] ; then
		cat <<EOF
  ${volume_types[$vol]}:
    ${volume_type_keys[$vol]}: "$(expand_volume "${volume_scoped_names[$vol]}" "$namespace" "$instance")"
EOF
	    else
		cat <<EOF
  ${volume_types[$vol]}: {}
EOF
	    fi
	done
    fi
    if (( has_system_configmap )) ; then
	cat <<EOF
- name: "systemconfigmap-${namespace}"
  configMap:
    name: "systemconfigmap-${namespace}"
EOF
    fi
}

function create_spec() {
    local affinity_yaml=
    local node_class=worker
    local OPTIND=0
    local OPTARG
    local ns_yaml=${node_selector:+"nodeSelector:${nl}  $node_selector: \"\""}
    local rc=${runtime_class:+runtimeClassName: "$runtime_class"}
    local use_pbench=1
    while getopts "A:c:r:Pp" opt "$@" ; do
	case "$opt" in
	    A) affinity_yaml=$OPTARG ;;
	    c) node_class=$OPTARG    ;;
	    r) rc=$OPTARG	    ;;
	    P) use_pbench=	    ;;
	    p) use_pbench=1	    ;;
	    *)                      ;;
	esac
    done
    shift $((OPTIND-1))
    if [[ -z "$(type -t "$1")" ]] ; then
	fatal "Cannot run $1: command not found"
    fi
    # Node specifiers override affinity
    if [[ -n "$node_class" && -n "${pin_nodes[$node_class]:-}" ]] ; then
	affinity_yaml=
	ns_yaml="nodeSelector:${nl}  kubernetes.io/hostname: \"${pin_nodes[$node_class]}\""
    fi
    if (( $# < 4 )) ; then
	echo "Usage: create_spec <container_func> namespace instance secret_count args..." 1>&2
	exit 1
    fi
    local create_container_function=$1; shift
    local namespace=$1
    local instance=$2
    local secret_count=$3
    cat <<EOF
spec:
$(indent 2 <<< "$ns_yaml")
  terminationGracePeriodSeconds: 1
$(indent 2 <<< "$affinity_yaml")
$(indent 2 tolerations_yaml ${tolerations+"${tolerations[@]}"})
$(indent 2 <<< "$rc")
  containers:
$(indent 2 "$create_container_function" "$@")
$(indent 2 <<< "${use_pbench:+$(pbench_agent_yaml)}")
$(indent 2 volumes_yaml "$namespace" "$instance" "$secret_count")
EOF
}

################################################################
# Object YAML creation
################################################################

function really_create_objects() {
    if [[ $doit -ne 0 && -n $accumulateddata ]] ; then
	(_OC ${KUBECTL_VALIDATE} apply -f - <<< "$accumulateddata" | timestamp 1>&2) || exit 1
	accumulateddata=
	objs_item_count=0
    fi
}

function create_object() {
    local OPTIND=0
    local namespace=
    local objtype=${deployment_type^}
    local objname=
    while getopts 'n:t:' opt "$@" ; do
	case "$opt" in
	    n) namespace=$OPTARG ;;
	    t) objtype=$OPTARG   ;;
	    *)                   ;;
	esac
    done
    shift $((OPTIND-1))
    objname=${1:-UNKNOWN}
    local data=
    local line=
    local -i data_found=0
    while IFS='' read -r 'line' ; do
	line=${line%% }
	if [[ -n $line ]] ; then
	    data_found=1
	    [[ -n $data ]] || data="---$nl"
	    data+="$line$nl"
	fi
    done
    ((! data_found)) && return
    data+="$nl"
    if (( doit )) ; then
	if [[ -n "$artifactdir" ]] ; then
	    mkdir -p "$artifactdir/$objtype"
	    echo "$data" > "$artifactdir/${objtype}/${namespace}:$objname"
	fi
	accumulateddata+="$data"
	if (( ++objs_item_count >= objs_per_call )) ; then
	    really_create_objects
	    (( !sleeptime )) || sleep "$sleeptime"
	fi
    else
	IFS= echo "$data"
    fi
}

function echo_if_desired() {
    if (( report_object_creation )) ; then
	cat
    else
	cat >/dev/null
    fi
}

function create_object_from_file() {
    function _ns() {
	((use_namespaces)) && echo -n "-n $namespace"
    }
    local objtype=$1; shift
    local namespace=$1; shift
    local objname=$1; shift
    local ns=${use_namespaces:+-n "$namespace"}
    local ns1=${use_namespaces:+"$namespace"}
    local file
    for file in "$@" ; do
	[[ -r "$file" ]] || fatal "Can't read file $file!"
    done
    # shellcheck disable=SC2046
    (_OC create "$objtype" $(_ns) "$objname" "${@/#/--from-file=}" | echo_if_desired 1>&2) || exit 1
    # shellcheck disable=SC2046
    (_OC label "$objtype" $(_ns) "$objname" 'clusterbusterbase=true' "clusterbuster-run_uuid=${run_uuid}" "${basename}-id=$run_uuid" "${basename}=true" | echo_if_desired 1>&2) || exit 1
    if (( doit )) ; then
	if [[ -n "$artifactdir" ]] ; then
	    mkdir -p "$artifactdir/$objtype"
	    local outfile="$artifactdir/${objtype}/${ns1}:${objname}"
	    rm -f "$outfile"
	    for file in "$@" ; do
		echo '---' >> "$outfile"
		cat "$file" >> "$outfile"
	    done
	fi
	total_objects_created=$((total_objects_created+1))
	if [[ -z ${objects_created[$objtype]:-} ]] ; then
	    objects_created[$objtype]=1
	else
	    objects_created[$objtype]=$((${objects_created[$objtype]}+1))
	fi
	(( !sleeptime )) || sleep "$sleeptime"
    fi
}

function create_namespace() {
    local namespace=$1
    if ((use_namespaces)) ; then
	create_object -t namespace "$namespace" <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: "${namespace}"
$(indent 2 standard_labels_yaml)
EOF
    fi
}

function create_secrets() {
    local namespace=$1
    local deps_per_namespace=${2:-1}
    local secrets=${3:-1}
    local -i i
    local -i j
    (( secrets )) || return;
    for i in $(seq "$first_deployment" $((deps_per_namespace + first_deployment - 1))) ; do
	for j in $(seq 0 $((secrets - 1))) ; do
	    local secname="secret${namespace:+-$namespace}-${i}-${j}"
	    create_object -t secret -n "$namespace" "$secname" <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: "$secname"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml)
data:
  key1: "$(base64 <<< "${namespace}X${i}Y${j}Z1")"
  key2: "$(base64 <<< "${namespace}X${i}Y${j}Z2")"
type: Opaque
EOF
	done
    done
}

function check_configmaps() {
    local errors=0
    for mapfile in "$@" ; do
	if [[ ! -r "$mapfile" ]] ; then
	    echo "Can't find configmap file $mapfile"
	    errors=$((errors+1))
	fi
    done
    if (( errors )) ; then
	exit 1
    fi
}

function create_configmaps() {
    local namespace=$1; shift
    local mapname=$1; shift
    (( $# )) || return;
    create_object_from_file configmap "$namespace" "${mapname}-${namespace}" "$@"
}

function create_service() {
    local namespace=$1
    local deployment=$2
    local portnum=$3
    create_object -t Service -n "$namespace" "service-${deployment}" <<EOF
apiVersion: v1
kind: Service
metadata:
  name: "service-${deployment}"
$(indent 2 namespace_yaml "$namespace")
$(indent 2 standard_labels_yaml)
spec:
  ports:
  - port: $portnum
    targetPort: $portnum
  selector:
    name: "${deployment}"
EOF
}

function create_container_sync() {
    local namespace=$1
    local sync_count=${4:-1}
    local expected_clients=$5
    cat <<EOF
- name: ${namespace}-sync
  image_pull_policy: $image_pull_policy
  image: "$container_image"
  ports:
  - containerPort: $port
$(indent 2 bootstrap_command_yaml sync.pl)
  - "-f"
  - "$sync_flag_file"
  - "$sync_port"
  - "$expected_clients"
  - "$sync_count"
$(indent 2 volume_mounts_yaml "$namespace" 0 0)
EOF
}

function create_sync_deployment() {
    local namespace=$1; shift
    local sync_count=$1; shift
    if [[ $deployment_type = pod ]] ; then
	create_object -t Pod -n "$namespace" "${namespace}-sync" <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: ${namespace}-sync
$(indent 2 namespace_yaml "$namespace")
$(indent 2 privilege_yaml)
  selector:
    matchLabels:
      app: ${namespace}-sync
$(indent 2 standard_labels_yaml)
    name: ${namespace}-sync
    app: ${namespace}-sync
    k8s-app: ${namespace}-sync
    ${basename}-sync: "$run_uuid"
$(create_spec -P -c sync -r '' create_container_sync "$namespace" 0 0 "$sync_count" "$@")
  restartPolicy: Never
EOF
    else
	create_object -t Deployment -n "$namespace" "${namespace}-sync" <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ${namespace}-sync
$(indent 2 namespace_yaml "$namespace")
$(indent 2 privilege_yaml)
  labels:
$(indent 4 standard_labels_yaml)
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: ${namespace}-sync
  template:
    metadata:
$(indent 6 standard_labels_yaml)
        name: ${namespace}-sync
        app: ${namespace}-sync
        k8s-app: ${namespace}-sync
        ${basename}-sync: "$run_uuid"
$(indent 4 create_spec -P -c sync -r '' create_container_sync "$namespace" 0 0 "$sync_count" "$@")
EOF
    fi
}

################################################################
# Generic Object creation
################################################################

function create_objects_n() {
    trap exit INT
    local objtype=$1; shift
    local parallel=$1; shift
    local objs_per_call=$1; shift
    local rotor=$1; shift
    local sleeptime=$1; shift
    while (( rotor < namespaces )) ; do
	local j=0
	while (( j < objs_per_call && j + rotor < namespaces )) ; do
	    "create_${objtype}" "${namespaces_to_create[$((rotor + j))]}" "$@"
	    if (( sleeptime )) ; then sleep "$sleeptime"; fi
	    j=$((j+1))
	done
	rotor=$((rotor + (parallel * objs_per_call)))
    done
    really_create_objects
}

function allocate_namespaces() {
    local -i ns_count=0
    local -i ns_idx=0
    if (( scale_ns )) ; then
        while read -r ns ; do
	    if [[ -n "$ns" ]] ; then
		namespaces_in_use[${ns#namespace/}]=1
	    fi
        done < <(_OC get ns -l "$basename" --no-headers -o name 2>/dev/null)
    fi
    while (( ns_count++ < namespaces )) ; do
        while [[ -n "${namespaces_in_use[${basename}-$ns_idx]:-}" ]] ; do
            ns_idx=$((ns_idx+1))
        done
        namespaces_to_create+=("${basename}-$((ns_idx++))")
    done
}

function find_first_deployment() {
    if (( scale_deployments )) ; then
	local ns
	local deployment
	local stuff
	# shellcheck disable=SC2034
        while read -r ns deployment stuff ; do
	    if [[ -n "$deployment" ]] ; then
		deployment=${deployment#${ns}-}
		deployment=${deployment%-*}
		echo _OC get deployments -l "$basename" -A --no-headers 1>&2
		echo "|$basename|$deployment|$first_deployment|" 1>&2
		if (( deployment + 1 > first_deployment )) ; then
		    first_deployment=$((deployment + 1))
		fi
	    fi
        done < <(_OC get deployments -l "$basename" -A --no-headers 2>/dev/null)
    fi
}

function create_all_namespaces() {
    local i
    trap 'kill -9 $(jobs -p); exit' INT
    for i in $(seq 0 "$((parallel_namespaces - 1))") ; do
	create_objects_n namespace "$parallel_namespaces" "$objs_per_call_namespaces" "$((i * objs_per_call_namespaces))" "$sleep_between_namespaces" &
    done
    wait || exit 1
}

function create_all_secrets() {
    local i
    trap 'kill -9 $(jobs -p); exit' INT
    for i in $(seq 0 "$((parallel_secrets - 1))") ; do
	create_objects_n secrets "$parallel_secrets" "$objs_per_call_secrets" "$((i * objs_per_call_secrets))" "$sleep_between_secrets" "$deps_per_namespace" "$secrets" &
    done
    wait || exit 1
}

function create_system_configmap() {
    if supports_api list_configmaps ; then
	local -a systemfiles
	readarray -t systemfiles < <(list_configmaps)
	trap 'kill -9 $(jobs -p); exit' INT
	for i in $(seq 0 "$((parallel_configmaps - 1))") ; do
	    create_objects_n configmaps "$parallel_configmaps" "$objs_per_call_configmaps" "$((i * objs_per_call_configmaps))" "$sleep_between_configmaps" "systemconfigmap" "${systemfiles[@]}"&
	done
	wait || exit 1
    fi
}

function create_all_configmaps() {
    create_system_configmap
    (( ${#configmap_files[@]} )) || return;
    local i
    trap 'kill -9 $(jobs -p); exit' INT
    for i in $(seq 0 "$((parallel_configmaps - 1))") ; do
	create_objects_n configmaps "$parallel_configmaps" "$objs_per_call_configmaps" "$((i * objs_per_call_configmaps))" "$sleep_between_configmaps" "configmap" "${configmap_files[@]}"&
    done
    wait || exit 1
}

function create_all_deployments() {
    local i
    trap 'kill -9 $(jobs -p); exit' INT
    for i in $(seq 0 "$((parallel_deployments - 1))") ; do
	export PARALLEL_ROTOR=$i
	create_objects_n deployment "$parallel_deployments" "$objs_per_call_deployments" "$((i * objs_per_call_deployments))" "$sleep_between_deployments" "$deps_per_namespace" "$secrets" "$replicas" "$containers_per_pod" "$log_host" "$log_port" &
    done
    wait || exit 1
}

function create_all_objects() {
    local found_objtype
    local objtype=$1
    shift
    while IFS= read -r 'line' ; do
	if [[ $line = *'__KUBEFAIL__ '* ]] ; then
	    echo "${line/__KUBEFAIL__ /}" 1>&2
	    return 1
	elif [[ $line =~ ^[[:digit:]]{4}(-[[:digit:]]{2}){2}T([[:digit:]]{2}:){2}[[:digit:]]{2}\.[[:digit:]]{6}\ +([-a-z0-9]+)/([-a-z0-9]+)\ +created$ ]] ; then
	    if (( report_object_creation )) ; then
		echo "$line" 1>&2
	    fi
	    total_objects_created=$((total_objects_created+1))
	    found_objtype=${BASH_REMATCH[3]}
	    if [[ -z ${objects_created[$found_objtype]:-} ]] ; then
		objects_created[$found_objtype]=1
	    else
		objects_created[$found_objtype]=$((${objects_created[$found_objtype]:-}+1))
	    fi
	elif [[ -n "$line" ]] ; then
	    (IFS=''; echo "$line" 1>&2)
	fi
    done < <("create_all_${objtype}" "$@" 2>&1)
}

function do_cleanup() {
    local -a objects_to_clean
    local objtype
    (( doit )) || return
    if ((use_namespaces)) ; then
	objects_to_clean=(namespace)
    else
	objects_to_clean=("$deployment_type" configmap secret service)
    fi
    for objtype in "${objects_to_clean[@]}" ; do
	if (( report_object_creation )) ; then
	    _OC delete "$objtype" -l "$basename" 1>&2
	else
	    local objects_deleted
	    objects_deleted=$(_OC delete "$objtype" -l "$basename" |grep -c "^${objtype}.*deleted\$")
	    if ((objects_deleted > 0 && report_object_creation)) ; then
		printf "Deleted %d %s%s\n" "$objects_deleted" "$objtype" "${plurals[$((objects_deleted == 1))]}" 1>&2
	    fi
	fi
    done
}

################################################################
# Main work loop
################################################################

function create_secrets_top_level() {
    # If previous secrets weren't all created, this will yield
    # the wrong result.
    if (( doit && wait_for_secrets )) ; then
	local -i i
	local -i j
	local -i k
	local ns
	local nsd
	for i in $(seq 0 $((namespaces - 1))) ; do
	    ns="secret/secret-${basename}-${namespaces_to_create[$i]##*-}"
	    for j in $(seq "$first_deployment" $((deps_per_namespace + first_deployment - 1))) ; do
		nsd="${ns}-$j"
		for k in $(seq 0 $((secrets - 1))) ; do
		    expected_secrets["${nsd}-$k"]=1
		done
	    done
	done
    fi
    create_all_objects secrets
    if (( doit )) ; then
	local secname=""
	while (( ${#expected_secrets[@]} )) ; do
	    echo "Expect ${#secrets[@]}" 1>&2
	    while read -r secname ; do
		[[ -n "${secname:-}" ]] && unset "expected_secrets[$secname]"
	    done < <(_OC get secret -oname --no-headers -l "$basename" -A)
	    if (( ${#expected_secrets[@]} )) ; then
		echo "Still waiting for ${#expected_secrets[@]} to be created."
		sleep 10
	    else
		break
	    fi
	done
    fi
}

function do_logging() {
    ((report)) || return 0
    local -i logs_expected=0
    logs_expected="$(calculate_logs_required "$namespaces" "$deps_per_namespace" "$replicas" "$containers_per_pod")"
    ((logs_expected > 0)) || return 0

    if [[ $log_strategy = poll ]] ; then
	local -a sync_pods=()
	local namespace
	local pod
	local ignore
	while read -r namespace pod ignore ; do
	    if [[ -n "$namespace" && -n "$pod" ]] ; then
		sync_pods+=("-n $namespace $pod")
	    fi
	done <<< "$(oc get pod -A --no-headers -l "${basename}-sync=${run_uuid}")"
	get_logs "${sync_pods[@]}" &
    else
	log_host=$(hostname -f)
	log_port=$("${__libdir__}/find_free_port")
	(( log_port )) || fatal "Can't get port to listen for logs on!"
	get_logs -p "$log_port" -c "$logs_expected" &
    fi
    local -i get_logs_pid=$!
    trap '[[ -n "$(jobs -p)" ]] && kill -HUP $(jobs -p)' INT TERM HUP
    if ((timeout > 0)) ; then
	local -i finis=0
	while (( timeout < 0 || timeout-- )) ; do
	    # CreateContainerError is not necessarily fatal
	    if _OC get pods -A -l "$basename" |grep -q -E -e '([^r]Error|Evicted|Crash)' ; then
		echo "Run failed:" 1>&2
		_OC get pods -A -l "$basename" |grep -E -e '([^r]Error|Evicted|Crash)'
		break
	    fi
	    # Commands in a pipeline are each run in subshells.  Background jobs are child
	    # of the parent, not any children, so running "jobs -l" inside the pipeline
	    # will not reap any exited subjobs.  The child still has the job table, so
	    # it knows what subjobs exist, but the "jobs -l" must be run at top level
	    # to actually reap them.
	    jobs -l >/dev/null
	    if jobs -l |grep -q . ; then
		sleep 1
	    else
		finis=1
		break
	    fi
	done
	if (( ! finis )) ; then
	    if [[ -n "$get_logs_pid" ]] ; then
		echo "Killing logger '$get_logs_pid'"
		exec 3>&2 2>/dev/null
		kill "$get_logs_pid"
		sleep 3
		kill -9 "$get_logs_pid"
		get_logs_pid=
		exec 2>&3 3>&-
		wait "$get_logs_pid"
	    fi
	    echo "Run did not terminate" 1>&2
	    status=1
	fi
    fi
    if [[ -n "${get_logs_pid:-}" ]] ; then
	wait "$get_logs_pid"
	status=$?
    fi
}

function report_object_creation() {
    (( report_object_creation )) || return
    local objtype
    while read -r objtype ; do
	[[ -n "${objtype:-}" ]] && printf "Created %${#total_objects_created}d %s%s\n" "${objects_created[$objtype]}" "$objtype" "${plurals[$((${objects_created[$objtype]} == 1))]}" 1>&2
    done < <( (IFS=$'\n'; echo "${!objects_created[*]}") | sort)
    printf "Created %d object%s total\n" "$total_objects_created" "${plurals[$((total_objects_created == 1))]}" 1>&2
}

function run_clusterbuster_1() {
    local -A expected_secrets=()
    local -i status=0
    (( precleanup && doit )) && do_cleanup
    if [[ $take_prometheus_snapshot -gt 0 && -n "$artifactdir" ]] ; then
	start_prometheus_snapshot
    else
	prometheus_starting_timestamp=$(get_prometheus_pod_timestamp)
    fi
    if [[ $doit -gt 0 && -n "${artifactdir:-}" ]] ; then
	artifactdir=${artifactdir//%s/$(readable_timestamp "$prometheus_starting_timestamp")}
	artifactdir=${artifactdir//%w/$requested_workload}
	artifactdir=${artifactdir//%n/$job_name}
	mkdir -p "$artifactdir" || fatal "Can't create log directory $artifactdir"
	echo "${saved_argv[@]@Q}" > "$artifactdir/commandline"
	exec 2> >(tee "$artifactdir/stderr.log" >&2)
    fi
    allocate_namespaces
    find_first_deployment
    trap 'if [[ -n "$(jobs -p)" ]] ; then kill -INT $(jobs -p); fi; exit' INT
    create_all_objects namespaces

    (( ${#configmap_files[@]} )) && check_configmaps "${configmap_files[@]}"
    create_all_objects configmaps
    supports_api list_configmaps && has_system_configmap=1

    create_secrets_top_level

    basetime=$(printf '%(%s)T' -1)
    create_all_objects deployments || exit 1
    (( doit )) || return

    do_logging || status=1
    if ((! status)) ; then
	report_object_creation
    fi
    ((cleanup)) && do_cleanup

    if ((status)) ; then
	fatal "Clusterbuster failed!"
    else
	if [[ $take_prometheus_snapshot -gt 0 && -n "$artifactdir" ]] ; then
	    retrieve_prometheus_snapshot "$artifactdir"
	fi
    fi
    return $status
}

################################################################
# Top level
################################################################

load_workloads

while getopts ":B:Eef:HhnP:Qqv-:" opt ; do
    case "$opt" in
	B) process_option "basename=$OPTARG"	 ;;
	E) process_option "exit_at_end=0"	 ;;
	e) process_option "exit_at_end=1"	 ;;
	f) process_job_file "$OPTARG"		 ;;
	h) help					 ;;
	H) help_extended			 ;;
	n) process_option "doit=0"		 ;;
	P) process_option "workload=$OPTARG"     ;;
	Q) process_option "reportobjectcreate=0" ;;
	q) process_option "verbose=0"		 ;;
	v) process_option "verbose=1"		 ;;
	-) process_option "$OPTARG"		 ;;
	*) help "$OPTARG"			 ;;
    esac
done

if [[ -n "${workload_aliases[${requested_workload,,}]:-}" ]] ; then
    requested_workload="${workload_aliases[${requested_workload,,}]:-}"
else
    help_extended "(Unknown workload '$requested_workload')"
fi

call_api -s "process_options" "${unknown_opts[@]}" || help "${unknown_opt_names[@]}"

if supports_api list_user_configmaps ; then
    readarray -t userfiles < <(list_user_configmaps)
    configmap_files+=("${userfiles[@]}")
fi

if (( namespaces <= 0 )) ; then
    namespaces=1
    use_namespaces=0
fi
(( !parallel_configmaps )) && parallel_configmaps=$parallel
(( !parallel_secrets )) && parallel_secrets=$parallel
(( !parallel_namespaces )) && parallel_namespaces=$parallel
(( !parallel_deployments )) && parallel_deployments=$parallel

(( !objs_per_call_configmaps )) && objs_per_call_configmaps=$objs_per_call
(( !objs_per_call_secrets )) && objs_per_call_secrets=$objs_per_call
(( !objs_per_call_namespaces )) && objs_per_call_namespaces=$objs_per_call
(( !objs_per_call_deployments )) && objs_per_call_deployments=$objs_per_call

[[ -n $runtime_class ]]
[[ -n "$report_format" ]] && report=1

if [[ $deployment_type != pod && $deployment_type != deployment ]] ; then
    echo "--deployment_type must be either pod or deployment"
    help
fi

if [[ -z $OC && $doit -gt 0 ]] ; then
    fatal "Cannot find oc or kubectl command, exiting!"
fi

if [[ -z "$pbench_controller_address" && $pbench_controller_port -ne 0 ]] ; then
    fatal "Both pbench_controller_address and pbench_controller_port must be specified"
fi

if [[ -n "$pbench_controller_address" && $pbench_controller_port -eq 0 ]] ; then
    fatal "Both pbench_controller_address and pbench_controller_port must be specified"
fi

log_strategy=${log_strategy,,}
if [[ $log_strategy != poll && $log_strategy != listen ]] ; then
    fatal "Unknown --log_strategy (must be 'poll' or 'listen')"
fi

resource_validation_failed=0
validate_resource resource_request "${resource_requests[@]}" || resource_validation_failed=1
validate_resource resource_limit "${resource_limits[@]}" || resource_validation_failed=1
if ((resource_validation_failed)) ; then
    exit 1
fi

for i in $(seq 0 $((emptydir_volumes-1)) ) ; do
    emptydirs+=("/mnt/volume-$i")
done

job_name=${job_name:-${requested_workload:-}}

shift $((OPTIND - 1))
if [[ -n ${1:-} ]] ; then
    basename="$1"
    shift
fi

[[ -z "$*" ]] || warn "Warning: extraneous arguments $* after basename will be ignored!"

(run_clusterbuster_1)
