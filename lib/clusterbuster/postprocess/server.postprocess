#!/usr/bin/env python3

import json
import sys
import argparse
import textwrap
from copy import deepcopy

parser = argparse.ArgumentParser(description='Generate ClusterBuster report')

parser.add_argument('-o', '--format', '--output_format', '--output', default='summary', type=str,
                    choices={'summary', 'verbose',
                    'json-summary', 'json', 'json-verbose'})
args = parser.parse_args()

jdata = json.load(sys.stdin)

class Reporter:
    def are_clients_all_on_same_node(self, api_objects: list):
        node = None
        for obj in api_objects:
            if obj['kind'] == 'Pod' and 'clusterbuster-client' in obj['labels'] and obj['labels']['clusterbuster-client']:
                if not node:
                    node = obj['nodeName']
                elif obj['nodeName'] != node:
                    return False
        return True

    def __init__(self, jdata: dict, report_format: str):
        self._jdata = deepcopy(jdata)
        self._format = report_format
        self._all_clients_are_on_the_same_node = self.are_clients_all_on_same_node(jdata['api_objects'])
        self._summary = {'user_cpu_seconds': 0,
                         'system_cpu_seconds': 0,
                         'cpu_seconds': 0,
                         'first_start': None,
                         'last_start': None,
                         'first_end': None,
                         'last_end': None,
                         'first_pod_start': None,
                         'last_pod_start': None,
                         'first_pod_create': None,
                         'last_pod_create': None,
                         'total_elapsed_time': 0,
                         'total_instances': 0}
        self._rows = []
                      
    def row_name(self, row: dict):
        return f'{row["namespace"]}~{row["pod"]}~{row["container"]}'

    def create_row(self, row: dict):
        rowhash = {}
        rowhash['namespace'] = row['namespace']
        rowhash['pod'] = row['pod']
        rowhash['container'] = row['container']
        rowhash['user_cpu_seconds'] = row['user_cpu_time']
        self._summary['user_cpu_seconds'] += row['user_cpu_time']
        rowhash['system_cpu_seconds'] = row['system_cpu_time']
        self._summary['system_cpu_seconds'] += row['system_cpu_time']
        rowhash['cpu_seconds'] = row['user_cpu_time'] + row['system_cpu_time']
        self._summary['cpu_seconds'] += row['user_cpu_time'] + row['system_cpu_time']
        rowhash['runtime'] = row['data_elapsed_time']
        # Pod create time is relative to the host
        rowhash['pod_create'] = row['pod_create_time_offset_from_base']
        if self._summary['first_pod_create'] is None or rowhash['pod_create'] < self._summary['first_pod_create']:
            self._summary['first_pod_create'] = rowhash['pod_create']
        if self._summary['last_pod_create'] is None or rowhash['pod_create'] > self._summary['last_pod_create']:
            self._summary['last_pod_create'] = rowhash['pod_create']
        self._summary['total_elapsed_time'] += rowhash['runtime']
        self._summary['total_instances'] += 1
        # I'd like to do this, but if the nodes are out of sync time-wise, this will not
        # function correctly.
        if self._all_clients_are_on_the_same_node:
            rowhash['run_start'] = row['data_start_time_offset_from_base']
            rowhash['run_end'] = row['data_end_time_offset_from_base']
            rowhash['pod_start'] = row['pod_start_time_offset_from_base']
            if self._summary['first_start'] is None or rowhash['run_start'] < self._summary['first_start']:
               self._summary['first_start'] = rowhash['run_start']
            if self._summary['first_end'] is None or rowhash['run_end'] < self._summary['first_end']:
               self._summary['first_end'] = rowhash['run_end']
            if self._summary['last_start'] is None or rowhash['run_start'] > self._summary['last_start']:
               self._summary['last_start'] = rowhash['run_start']
            if self._summary['last_end'] is None or rowhash['run_end'] > self._summary['last_end']:
               self._summary['last_end'] = rowhash['run_end']
            if self._summary['first_pod_start'] is None or rowhash['pod_start'] < self._summary['first_pod_start']:
               self._summary['first_pod_start'] = rowhash['pod_start']
            if self._summary['last_pod_start'] is None or rowhash['pod_start'] > self._summary['last_pod_start']:
               self._summary['last_pod_start'] = rowhash['pod_start']
        return rowhash

    def print_summary(self):
        self._summary['elapsed_time_average'] = self._summary['total_elapsed_time'] / self._summary['total_instances']
        self._summary['pod_create_span'] = self._summary['last_pod_create'] - self._summary['first_pod_create']
        if self._all_clients_are_on_the_same_node:
            self._summary['elapsed_time_net'] = self._summary['last_end'] - self._summary['first_start']
            self._summary['pod_start_span'] = self._summary['last_pod_start'] - self._summary['first_pod_start']
            self._summary['overlap_error'] = ((((self._summary['last_start'] - self._summary['first_start']) +
                                         (self._summary['last_end'] - self._summary['first_end'])) / 2) /
                                        self._summary['elapsed_time_average'])
        print(f"""Summary:
    Total Clients:              {self._summary['total_instances']}
    Elapsed Time Average:       {round(self._summary['elapsed_time_average'], 3)}
    Pod creation span:          {round(self._summary['pod_create_span'], 5)}
    User CPU seconds:           {round(self._summary['user_cpu_seconds'], 3)}
    System CPU seconds:         {round(self._summary['system_cpu_seconds'], 3)}
    CPU seconds:                {round(self._summary['cpu_seconds'], 5)}""")
        if self._all_clients_are_on_the_same_node:
            print(f"""    CPU utilization:            {round(self._summary['cpu_seconds'] / self._summary['elapsed_time_net'], 5)}
    First run start:            {round(self._summary['first_start'], 3)}
    First run end:              {round(self._summary['first_end'], 3)}
    Last run start:             {round(self._summary['last_start'], 3)}
    Last run end:               {round(self._summary['last_end'], 3)}
    Net elapsed time:           {round(self._summary['elapsed_time_net'], 3)}
    Overlap error:              {round(self._summary['overlap_error'], 5)}
    Pod start span:             {round(self._summary['pod_start_span'], 5)}""")
        else:
            print(f'''
    *** Run start/end not available when client pods are not all on the same node ***''')

    def print_verbose(self):
        pass

    def create_report(self):
        if 'Results' in jdata:
            rows = jdata['Results']
            for row in rows:
                self.create_row(row)

            if args.format == 'json-summary':
                self._summary['metadata'] = jdata['metadata']
                json.dump(self._summary, sys.stdout, sort_keys=True, indent=4)
            elif args.format == 'json':
                answer = {
                    'summary': self._summary,
                    'metadata': self._metadata,
                    'rows': self._rows
                    }
                json.dump(answer, sys.stdout, sort_keys=True, indent=4)
            elif args.format == 'json-verbose':
                answer = deepcopy(self._jdata)
                answer['processed_results'] = {
                    'summary': self._summary,
                    'rows': self._rows
                    }
                json.dump(answer, sys.stdout, sort_keys=True, indent=4)
            else:
                print(f"""Clusterbuster run report for job {jdata['metadata']['job_name']} at {jdata['metadata']['job_start_time']}

    Workload: {jdata['metadata']['workload']}
    Command line:  {textwrap.fill(' '.join(jdata['metadata']['expanded_command_line']), width=72, subsequent_indent='                ', break_long_words=False, break_on_hyphens=False)}
""")
                if args.format == 'verbose':
                    self.print_verbose()
                self.print_summary()


class server_postprocessor(Reporter):
    def __init__(self, jdata: dict, report_format: str):
        Reporter.__init__(self, jdata, report_format)
        self._summary['total_max_round_trip_time'] = 0
        self._summary['round_trip_time_accumulator'] = 0
        self._summary['total_data_xfer_bytes'] = 0
        self._summary['total_iterations'] = 0

    def create_row(self, row: dict):
        rowhash = Reporter.create_row(self, row)
        rowhash['mean_round_trip_time_msec'] = row['mean_latency_sec'] * 1000
        rowhash['max_round_trip_time_msec'] = row['max_latency_sec'] * 1000
        rowhash['iterations'] = row['passes']
        rowhash['data_xfer'] = row['data_sent_bytes']
        rowhash['data_rate_mb_sec'] = rowhash['data_xfer'] / rowhash['runtime'] / 1000000
        if rowhash['max_round_trip_time_msec'] > self._summary['total_max_round_trip_time']:
            self._summary['total_max_round_trip_time'] = rowhash['max_round_trip_time_msec']
        self._summary['total_data_xfer_bytes'] += rowhash['data_xfer']
        self._summary['total_iterations'] += rowhash['iterations']
        self._summary['round_trip_time_accumulator'] += rowhash['mean_round_trip_time_msec']
        self._rows.append(rowhash)

    def print_summary(self):
        # I'd like to do this, but if the nodes are out of sync time-wise, this will not
        # function correctly.
        Reporter.print_summary(self)
        if self._all_clients_are_on_the_same_node:
            self._summary['average_data_rate_mb_sec'] = self._summary['total_data_xfer_bytes'] / (self._summary['last_end'] - self._summary['first_start']) / 1000000
        else:
            self._summary['average_data_rate_mb_sec'] = self._summary['total_data_xfer_bytes'] / self._summary['elapsed_time_average'] / 1000000
        self._summary['max_round_trip_time_msec'] = self._summary['total_max_round_trip_time']
        self._summary['average_data_rate_mb_sec'] = self._summary['total_data_xfer_bytes'] / self._summary['elapsed_time_average'] / 1000000
        self._summary['average_round_trip_time_msec'] = self._summary['round_trip_time_accumulator'] / self._summary['total_instances']
        print(f"""    Total Messages Sent:        {self._summary['total_iterations']}
    Total Data Sent (MB):       {round(self._summary['total_data_xfer_bytes'] / 1000000, 3)}
    Average Data Rate (MB/sec): {round(self._summary['average_data_rate_mb_sec'], 3)}
    Average RTT msec:           {round(self._summary['average_round_trip_time_msec'], 3)}
    Max RTT msec:               {round(self._summary['max_round_trip_time_msec'], 3)}""")

    def print_verbose(self):
        lastNamespace = None
        lastPod = None
        lastContainer = None
        self._rows.sort(key=self.row_name)
        for row in self._rows:
            Reporter.print_verbose(row)
            if row['namespace'] != lastNamespace:
                print(f"""Namespace: {row['namespace']}
    Pod: {row['pod']}
        Container: {row['container']}""")
                lastNamespace = row['namespace']
                lastPod = row['pod']
                lastContainer = row['container']
            elif row['pod'] != lastPod:
                print(f"""    Pod: {row['pod']}
        Container: {row['container']}""")
                lastPod = row['pod']
                lastContainer = row['container']
            elif row['container'] != lastContainer:
                print(f"        Container: {row['container']}")
                lastContainer = row['container']
            print(f"""            Elapsed Time:       {round(row['runtime'], 3)}
            Messages Sent:      {row['iterations']}
            Data Sent:          {round(row['data_xfer'] / 1000000, 3)}
            Data Rate (MB/sec): {round(row['data_rate_mb_sec'], 3)}
            Avg RTT msec:       {round(row['mean_round_trip_time_msec'], 3)}
            Max RTT msec:       {round(row['max_round_trip_time_msec'], 3)}
""")

        
server_postprocessor(jdata, args.format).create_report()
