#!/usr/bin/env python3

import json
import sys
import argparse
import textwrap

parser = argparse.ArgumentParser(description='Generate ClusterBuster report')

parser.add_argument('-o', '--format', '--output_format', '--output', default='summary', type=str,
                    choices={'summary', 'verbose',
                    'json-summary', 'json', 'json-verbose'})
args = parser.parse_args()

jdata = json.load(sys.stdin)

class postprocessor:
    def __init__(self):
        pass

    def create_summary(self, summary: dict):
        pass

    def create_row(self, row: dict, rowhash: dict, summary: dict):
        pass

    def print_summary(self, summary: dict):
        pass

    def print_verbose(self, data: dict):
        pass

class server_postprocessor(postprocessor):
    def __init__(self):
        pass

    def create_summary(self, summary: dict):
        summary['total_max_round_trip_time'] = 0
        summary['round_trip_time_accumulator'] = 0
        summary['total_data_xfer_bytes'] = 0
        summary['total_iterations'] = 0

    def create_row(self, row: dict, rowhash: dict, summary: dict):
        rowhash['mean_round_trip_time_msec'] = row['mean_latency_sec'] * 1000
        rowhash['max_round_trip_time_msec'] = row['max_latency_sec'] * 1000
        rowhash['iterations'] = row['passes']
        rowhash['data_xfer'] = row['data_sent_bytes']
        rowhash['data_rate_mb_sec'] = rowhash['data_xfer'] / rowhash['runtime'] / 1000000
        if rowhash['max_round_trip_time_msec'] > summary['total_max_round_trip_time']:
            summary['total_max_round_trip_time'] = rowhash['max_round_trip_time_msec']
        summary['total_data_xfer_bytes'] += rowhash['data_xfer']
        summary['total_iterations'] += rowhash['iterations']
        summary['round_trip_time_accumulator'] += rowhash['mean_round_trip_time_msec']

    def print_summary(self, summary: dict):
        # I'd like to do this, but if the nodes are out of sync time-wise, this will not
        # function correctly.
#        if True or all_clients_are_on_same_node:
        if True:
            summary['average_data_rate_mb_sec'] = summary['total_data_xfer_bytes'] / (summary['last_end'] - summary['first_start']) / 1000000
        else:
            summary['average_data_rate_mb_sec'] = summary['total_data_xfer_bytes'] / summary['elapsed_time_average'] / 1000000
        summary['max_round_trip_time_msec'] = summary['total_max_round_trip_time']
        summary['average_data_rate_mb_sec'] = summary['total_data_xfer_bytes'] / summary['elapsed_time_average'] / 1000000
        summary['average_round_trip_time_msec'] = summary['round_trip_time_accumulator'] / summary['total_instances']
        print(f"""    Total Messages Sent:        {summary['total_iterations']}
    Total Data Sent (MB):       {round(summary['total_data_xfer_bytes'] / 1000000, 3)}
    Average Data Rate (MB/sec): {round(summary['average_data_rate_mb_sec'], 3)}
    Average RTT msec:           {round(summary['average_round_trip_time_msec'], 3)}
    Max RTT msec:               {round(summary['max_round_trip_time_msec'], 3)}""")

    def print_verbose(self, data: dict):
        def rowName(e):
            return f'{e["namespace"]}~{e["pod"]}~{e["container"]}'
        lastNamespace = None
        lastPod = None
        lastContainer = None
        data['rows'].sort(key=rowName)
        for row in data['rows']:
            if row['namespace'] != lastNamespace:
                print(f"""Namespace: {row['namespace']}
    Pod: {row['pod']}
        Container: {row['container']}""")
                lastNamespace = row['namespace']
                lastPod = row['pod']
                lastContainer = row['container']
            elif row['pod'] != lastPod:
                print(f"""    Pod: {row['pod']}
        Container: {row['container']}""")
                lastPod = row['pod']
                lastContainer = row['container']
            elif row['container'] != lastContainer:
                print(f"        Container: {row['container']}")
                lastContainer = row['container']
            print(f"""            Elapsed Time:       {round(row['runtime'], 3)}
            Messages Sent:      {row['iterations']}
            Data Sent:          {round(row['data_xfer'] / 1000000, 3)}
            Data Rate (MB/sec): {round(row['data_rate_mb_sec'], 3)}
            Avg RTT msec:       {round(row['mean_round_trip_time_msec'], 3)}
            Max RTT msec:       {round(row['max_round_trip_time_msec'], 3)}
""")

        
def are_clients_all_on_same_node(api_objects: list):
    node = None
    for obj in api_objects:
        if obj['kind'] == 'Pod' and 'clusterbuster-client' in obj['labels'] and obj['labels']['clusterbuster-client']:
            if not node:
                node = obj['nodeName']
            elif obj['nodeName'] != node:
                return False
    return True

class reporter:
    def __init__(self, jdata: dict, postprocessor: postprocessor):
        self._jdata = jdata
        self._postprocessor = postprocessor

    def create_summary(self):
        summary = {'user_cpu_seconds': 0,
               'system_cpu_seconds': 0,
               'cpu_seconds': 0,
               'first_start': None,
               'last_start': None,
               'first_end': None,
               'last_end': None,
               'first_pod_start': None,
               'last_pod_start': None,
               'first_pod_create': None,
               'last_pod_create': None,
               'total_elapsed_time': 0,
               'total_instances': 0}
        self._postprocessor.create_summary(summary)
        return summary

    def create_row(self, row: dict, summary: dict):
        rowhash = {}
        rowhash['namespace'] = row['namespace']
        rowhash['pod'] = row['pod']
        rowhash['container'] = row['container']
        rowhash['user_cpu_seconds'] = row['user_cpu_time']
        summary['user_cpu_seconds'] += row['user_cpu_time']
        rowhash['system_cpu_seconds'] = row['system_cpu_time']
        summary['system_cpu_seconds'] += row['system_cpu_time']
        rowhash['cpu_seconds'] = row['user_cpu_time'] + row['system_cpu_time']
        summary['cpu_seconds'] += row['user_cpu_time'] + row['system_cpu_time']
        rowhash['runtime'] = row['data_elapsed_time']
        # Pod create time is relative to the host
        rowhash['pod_create'] = row['pod_create_time_offset_from_base']
        if summary['first_pod_create'] is None or rowhash['pod_create'] < summary['first_pod_create']:
            summary['first_pod_create'] = rowhash['pod_create']
        if summary['last_pod_create'] is None or rowhash['pod_create'] > summary['last_pod_create']:
            summary['last_pod_create'] = rowhash['pod_create']
        summary['total_elapsed_time'] += rowhash['runtime']
        summary['total_instances'] += 1
        # I'd like to do this, but if the nodes are out of sync time-wise, this will not
        # function correctly.
        if self._all_clients_are_on_same_node:
            rowhash['run_start'] = row['data_start_time_offset_from_base']
            rowhash['run_end'] = row['data_end_time_offset_from_base']
            rowhash['pod_start'] = row['pod_start_time_offset_from_base']
            if summary['first_start'] is None or rowhash['run_start'] < summary['first_start']:
               summary['first_start'] = rowhash['run_start']
            if summary['first_end'] is None or rowhash['run_end'] < summary['first_end']:
               summary['first_end'] = rowhash['run_end']
            if summary['last_start'] is None or rowhash['run_start'] > summary['last_start']:
               summary['last_start'] = rowhash['run_start']
            if summary['last_end'] is None or rowhash['run_end'] > summary['last_end']:
               summary['last_end'] = rowhash['run_end']
            if summary['first_pod_start'] is None or rowhash['pod_start'] < summary['first_pod_start']:
               summary['first_pod_start'] = rowhash['pod_start']
            if summary['last_pod_start'] is None or rowhash['pod_start'] > summary['last_pod_start']:
               summary['last_pod_start'] = rowhash['pod_start']
        self._postprocessor.create_row(row, rowhash, summary)
        return rowhash

    def print_summary(self, summary: dict):
        summary['elapsed_time_average'] = summary['total_elapsed_time'] / summary['total_instances']
        summary['pod_create_span'] = summary['last_pod_create'] - summary['first_pod_create']
        if self._all_clients_are_on_same_node:
            summary['elapsed_time_net'] = summary['last_end'] - summary['first_start']
            summary['pod_start_span'] = summary['last_pod_start'] - summary['first_pod_start']
            summary['overlap_error'] = ((((summary['last_start'] - summary['first_start']) +
                                         (summary['last_end'] - summary['first_end'])) / 2) /
                                        summary['elapsed_time_average'])
        print(f"""Summary:
    Total Clients:              {summary['total_instances']}
    Elapsed Time Average:       {round(summary['elapsed_time_average'], 3)}
    Pod creation span:          {round(summary['pod_create_span'], 5)}
    User CPU seconds:           {round(summary['user_cpu_seconds'], 3)}
    System CPU seconds:         {round(summary['system_cpu_seconds'], 3)}
    CPU seconds:                {round(summary['cpu_seconds'], 5)}""")
        self._postprocessor.print_summary(summary)
        if self._all_clients_are_on_same_node:
            print(f"""    CPU utilization:            {round(summary['cpu_seconds'] / summary['elapsed_time_net'], 5)}
    First run start:            {round(summary['first_start'], 3)}
    First run end:              {round(summary['first_end'], 3)}
    Last run start:             {round(summary['last_start'], 3)}
    Last run end:               {round(summary['last_end'], 3)}
    Net elapsed time:           {round(summary['elapsed_time_net'], 3)}
    Overlap error:              {round(summary['overlap_error'], 5)}
    Pod start span:             {round(summary['pod_start_span'], 5)}""")
        else:
            print(f'''
    *** Run start/end not available when client pods are not all on the same node ***''')

    def print_verbose(self, data: dict):
        self._postprocessor.print_verbose(data)

    def create_report(self):
        if 'Results' in jdata:
            rows = jdata['Results']
            answer = {}
            answer['rows'] = []
            self._all_clients_are_on_same_node = are_clients_all_on_same_node(jdata['api_objects'])
            summary = self.create_summary()
            for row in rows:
                answer['rows'].append(self.create_row(row, summary))

            answer['Summary'] = summary

            if args.format == 'json-summary':
                summary['metadata'] = jdata['metadata']
                json.dump(summary, sys.stdout, sort_keys=True, indent=4)
            elif args.format == 'json':
                answer['metadata'] = jdata['metadata']
                json.dump(answer, sys.stdout, sort_keys=True, indent=4)
            elif args.format == 'json-verbose':
                jdata['processed_results'] = answer
                json.dump(jdata, sys.stdout, indent=4)
            else:
                print(f"""Clusterbuster run report for job {jdata['metadata']['job_name']} at {jdata['metadata']['job_start_time']}

    Workload: {jdata['metadata']['workload']}
    Command line:  {textwrap.fill(' '.join(jdata['metadata']['expanded_command_line']), width=72, subsequent_indent='                ', break_long_words=False, break_on_hyphens=False)}
""")
                if args.format == 'verbose':
                    self.print_verbose(answer)
            self.print_summary(summary)

reporter(jdata, server_postprocessor()).create_report()
