#!/usr/bin/env python3

import json
import sys
import argparse
import textwrap

parser = argparse.ArgumentParser(description='Generate ClusterBuster report')

parser.add_argument('-o', '--format', '--output_format', '--output', default='summary', type=str,
                    choices={'summary', 'verbose',
                    'json-summary', 'json', 'json-verbose'})
args = parser.parse_args()

def are_clients_all_on_same_node(api_objects: list):
    node = None
    for obj in api_objects:
        if obj['kind'] == 'Pod' and 'clusterbuster-client' in obj['labels'] and obj['labels']['clusterbuster-client']:
            if not node:
                node = obj['nodeName']
            elif obj['nodeName'] != node:
                return False
    return True

jdata = json.load(sys.stdin)
if 'Results' in jdata:
    rows = jdata['Results']
    answer = {}
    answer['rows'] = []
    total_max_round_trip_time = 0
    round_trip_time_accumulator = 0
    total_data_xfer = 0
    total_iterations = 0
    total_et = 0.0
    first_start = None
    last_start = None
    first_end = None
    last_end = None
    all_clients_are_on_same_node = are_clients_all_on_same_node(jdata['api_objects'])
    for row in rows:
        rowhash = {}
        rowhash['namespace'] = row['namespace']
        rowhash['pod'] = row['pod']
        rowhash['container'] = row['container']
        rowhash['runtime'] = row['data_elapsed_time']

        rowhash['mean_round_trip_time_msec'] = round(row['mean_latency_sec'] * 1000, 3)
        rowhash['max_round_trip_time_msec'] = round(row['max_latency_sec'] * 1000, 3)
        rowhash['iterations'] = row['passes']
        rowhash['data_xfer'] = row['data_sent_bytes']
        rowhash['data_rate_mb_sec'] = round(rowhash['data_xfer'] / rowhash['runtime'] / 1000000, 3)
        if rowhash['max_round_trip_time_msec'] > total_max_round_trip_time:
            total_max_round_trip_time = rowhash['max_round_trip_time_msec']
        total_data_xfer += rowhash['data_xfer']
        total_iterations += rowhash['iterations']
        total_et += rowhash['runtime']
        round_trip_time_accumulator += rowhash['mean_round_trip_time_msec']
        # I'd like to do this, but if the nodes are out of sync time-wise, this will not
        # function correctly.
        if all_clients_are_on_same_node:
            rowhash['run_start'] = row['data_start_time_offset_from_base']
            rowhash['run_end'] = row['data_end_time_offset_from_base']
            if first_start is None or rowhash['run_start'] < first_start:
                first_start = rowhash['run_start']
            if first_end is None or rowhash['run_end'] < first_end:
                first_end = rowhash['run_end']
            if last_start is None or rowhash['run_start'] > last_start:
                last_start = rowhash['run_start']
            if last_end is None or rowhash['run_end'] > last_end:
                last_end = rowhash['run_end']
        answer['rows'].append(rowhash)

    summary = {}
    answer['Summary'] = summary
    summary['total_iterations'] = total_iterations
    summary['elapsed_time_average'] = round(total_et / len(rows), 3)
    summary['max_round_trip_time_msec'] = round(total_max_round_trip_time, 3)
    summary['total_data_xfer_bytes'] = total_data_xfer
    summary['average_data_rate_mb_sec'] = round(total_data_xfer / summary['elapsed_time_average'] / 1000000, 3)
    summary['average_round_trip_time_msec'] = round(round_trip_time_accumulator / len(rows), 3)
    summary['total_clients'] = len(rows)

    # I'd like to do this, but if the nodes are out of sync time-wise, this will not
    # function correctly.
    if all_clients_are_on_same_node:
        summary['first_run_start'] = first_start
        summary['first_run_end'] = first_end
        summary['last_run_start'] = last_start
        summary['last_run_end'] = last_end
        summary['elapsed_time_net'] = last_end - first_start
        summary['overlap_error'] = round(((((last_start - first_start) +
                                           (last_end - first_end)) / 2) /
                                         (total_et / len(rows))), 5)
        summary['average_data_rate_mb_sec'] = round(total_data_xfer / (last_end - first_start) / 1000000, 3)
    else:
        summary['average_data_rate_mb_sec'] = round(total_data_xfer / summary['elapsed_time_average'] / 1000000, 3)

    if args.format == 'json-summary':
        summary['metadata'] = jdata['metadata']
        json.dump(summary, sys.stdout, sort_keys=True, indent=4)
    elif args.format == 'json':
        answer['metadata'] = jdata['metadata']
        json.dump(answer, sys.stdout, sort_keys=True, indent=4)
    elif args.format == 'json-verbose':
        jdata['processed_results'] = answer
        json.dump(jdata, sys.stdout, indent=4)
    else:
        print(f"""Clusterbuster run report for job {jdata['metadata']['job_name']} at {jdata['metadata']['job_start_time']}

Workload: {jdata['metadata']['workload']}
Command line:  {textwrap.fill(' '.join(jdata['metadata']['expanded_command_line']), width=72, subsequent_indent='                ', break_long_words=False, break_on_hyphens=False)}
""")
        if args.format == 'verbose':
            def rowName(e):
                return f'{e["namespace"]}~{e["pod"]}~{e["container"]}'
            lastNamespace = None
            lastPod = None
            lastContainer = None
            answer['rows'].sort(key=rowName)
            for row in answer['rows']:
                if row['namespace'] != lastNamespace:
                    print(f"Namespace: {row['namespace']}, Pod: {row['pod']}, Container: {row['container']}")
                    lastNamespace = row['namespace']
                    lastPod = row['pod']
                    lastContainer = row['container']
                elif row['pod'] != lastPod:
                    print(f"    Pod: {row['pod']}, Container: {row['container']}")
                    lastPod = row['pod']
                    lastContainer = row['container']
                elif row['container'] != lastContainer:
                    print(f"        Container: {row['container']}")
                    lastContainer = row['container']
                print(f"""
            Elapsed Time:       {round(row['runtime'], 3)}
            Messages Sent:      {row['iterations']}
            Data Sent:          {round(row['data_xfer'] / 1000000, 3)}
            Data Rate (MB/sec): {round(row['data_rate_mb_sec'], 3)}
            Avg RTT msec:       {round(row['mean_round_trip_time_msec'], 3)}
            Max RTT msec:       {round(row['max_round_trip_time_msec'], 3)}
""")
        print(f"""Summary:
    Total Clients:              {summary['total_clients']}
    Elapsed Time Average:       {round(summary['elapsed_time_average'], 3)}
    Total Messages Sent:        {summary['total_iterations']}
    Total Data Sent (MB):       {round(summary['total_data_xfer_bytes'] / 1000000, 3)}
    Average Data Rate (MB/sec): {round(summary['average_data_rate_mb_sec'], 3)}
    Average RTT msec:           {round(summary['average_round_trip_time_msec'], 3)}
    Max RTT msec:               {round(summary['max_round_trip_time_msec'], 3)}""")
        if all_clients_are_on_same_node:
            print(f"""    First run start:            {round(summary['first_run_start'], 3)}
    First run end:              {round(summary['first_run_end'], 3)}
    Last run start:             {round(summary['last_run_start'], 3)}
    Last run end:               {round(summary['last_run_end'], 3)}
    Net elapsed time:           {round(summary['elapsed_time_net'], 3)}
    Overlap error:              {round(summary['overlap_error'], 5)}
""")
        else:
            print(f'''
    *** Run start/end not available when client pods are not all on the same node ***''')
