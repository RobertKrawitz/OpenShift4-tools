#!/usr/bin/env python3

# Copyright 2021 Robert Krawitz/Red Hat
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import print_function
from prometheus_api_client import PrometheusConnect
import argparse
from datetime import datetime, timezone, timedelta
import json
import openshift
import selectors
import subprocess
import sys
import time
import urllib3
import uuid
import yaml
import os.path
import re
from benchmark_runner.common.elasticshearch.es_operations import ESOperations


known_reporting_types = ["everything", "all", 'basic', 'summary', 'none']
report_included_types = {
    'everything': ['all'],
    'all': ['basic'],
    'basic': ['summary']
}


def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)


def efail(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)
    sys.exit(1)


def run_command(cmd, fail_on_bad_status=True, report_stderr_async=True,
                report_stdout_async=False):
    """ Run specified command, capturing stdout and stderr as array of timestamped lines.
        Optionally fail if return status is non-zero.  Also optionally report
        stdout and/or stderr to the appropriate file descriptors
    """
    with subprocess.Popen(cmd, stdin=subprocess.DEVNULL, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as command:
        stdout_data = []
        stderr_data = []

        sel = selectors.DefaultSelector()
        sel.register(command.stdout, selectors.EVENT_READ)
        sel.register(command.stderr, selectors.EVENT_READ)
        while True:
            # Keep reading until we reach EOF on both channels.
            # command.poll() is not a good criterion because the process
            # might complete before everything has been read.
            foundSomething = False
            for key, _ in sel.select():
                data = key.fileobj.readline()
                if len(data) > 0:
                    foundSomething = True
                    data = data.decode().rstrip()
                    if key.fileobj is command.stdout:
                        stdout_data.append([datetime.now(timezone.utc).isoformat(), data])
                        if report_stdout_async:
                            print(data)
                    elif key.fileobj is command.stderr:
                        stderr_data.append([datetime.now(timezone.utc).isoformat(), data])
                        if report_stderr_async:
                            print(data, file=sys.stderr)
            if not foundSomething:
                while command.poll() is None:
                    time.sleep(1)
                if fail_on_bad_status and command.poll() != 0:
                    raise RuntimeError('Command %s failed: exit status %d' % (' '.join(cmd), command.poll()))
                return (stdout_data, stderr_data, command.poll())


def get_prometheus_default_url():
    try:
        with openshift.project('openshift-monitoring'):
            return 'https://%s' % openshift.selector(['route/prometheus-k8s']).objects()[0].as_dict()['spec']['host']
    except Exception as err:
        efail("Unable to retrieve prometheus-k8s route: %s" % err)


def get_prometheus_token():
    try:
        with openshift.project('openshift-monitoring'):
            return 'Bearer %s' % openshift.get_serviceaccount_auth_token('prometheus-k8s')
    except Exception as err:
        efail("Unable to retrieve prometheus-k8s token: %s" % err)


def get_nodes():
    try:
        return json.loads([n.as_dict() for n in openshift.selector(['node']).objects()])
    except Exception as err:
        efail("Unable to retrieve cluster version: %s" % err)


def generate_uuid():
    return str(uuid.uuid4())


class HandleReportArgument(argparse.Action):
    def __call__(self, parser, namespace, values, option_string):
        answers = [option.strip().lower() for option in values.split(",")]
        for answer in answers:
            if answer not in known_reporting_types:
                raise argparse.ArgumentTypeError("Unknown report value %s; expect one or more of: %s" %
                                                 (answer, ', '.join(known_reporting_types)))
            else:
                if namespace.report is None:
                    namespace.report = []
                if answer not in namespace.report:
                    namespace.report.append(answer)


parser = argparse.ArgumentParser(description='Scrape data from Prometheus')
parser.add_argument('-u', '--url', '--prometheus-url', type=str,
                    help='Prometheus URL', metavar='URL',
                    default=get_prometheus_default_url())
parser.add_argument('-s', '--step', type=int, default=30, metavar='seconds',
                    help='Step duration')
parser.add_argument('-t', '--token', type=str,
                    help='Prometheus authentication token', metavar='token',
                    default=get_prometheus_token())
parser.add_argument('-m', '--metrics-profile', type=str, metavar='file',
                    help='Metrics profile file or URL', default='metrics.yaml')
parser.add_argument('--epoch', type=int, default=60, metavar='seconds',
                    help='Start of metrics relative to job start')
parser.add_argument('--post-settling-time', type=int, default=60, metavar='seconds',
                    help='Time to continue collecting metrics after job completion')
parser.add_argument('--json-from-command', action='store_true',
                    help='Interpret command stdout as JSON')
parser.add_argument('--uuid', type=str, metavar='UUID', default=generate_uuid(),
                    help='Index results by UUID (generate if not provided)')
parser.add_argument('--report', help='What to report, one or more of %s' % ', '.join(known_reporting_types),
                    metavar='Reporting types', action=HandleReportArgument)
parser.add_argument('--es_host', '--es-host', help='Elasticsearch host', metavar='hostname',
                    type=str)
parser.add_argument('--es_port', '--es-port', help='Elasticsearch port', metavar='port',
                    type=int)
parser.add_argument('--job_type', '--job-type', help='Type of job (fio, uperf, etc)',
                    metavar='command', type=str)
parser.add_argument('command', metavar='command', help='command [args...]',
                    type=str, nargs='*')
args = parser.parse_args()

if args.report is None:
    args.report = ['summary']
else:
    def expand_list(collection, included_collections):
        did_something = True
        while did_something:
            did_something = False
            for item in collection:
                if item in included_collections:
                    for included_item in included_collections[item]:
                        if included_item not in collection:
                            collection.append(included_item)
                            did_something = True
    expand_list(args.report, report_included_types)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
authorization = {}

if args.token != '':
    authorization['Authorization'] = args.token

prom = PrometheusConnect(url=args.url, disable_ssl=True, headers=authorization)

try:
    with open(args.metrics_profile, 'r') as metrics_yaml:
        yaml = yaml.safe_load(metrics_yaml)
except FileNotFoundError as err:
    efail('Cannot open metrics profile: %s' % err)
    sys.exit(1)

startTime = datetime.now(timezone.utc)
metricsStartTime = startTime + timedelta(seconds=-abs(args.epoch))
stdout_data = []
stderr_data = []

try:
    stdout_data, stderr_data, cmd_exit_status = run_command(args.command)
except KeyboardInterrupt:
    efail("Interrupted")
except Exception as err:
    efail(err)
endTime = datetime.now(timezone.utc)

json_output = None
if args.json_from_command and len(stdout_data) > 0:
    try:
        json_output = json.loads("\n".join(a[1] for a in stdout_data))
    except json.decoder.JSONDecodeError as err:
        eprint("Cannot decode command output as JSON: %s" % err)


def extract_pod_data(pod):
    answer = {}
    answer['kind'] = pod['kind']
    answer['name'] = pod['metadata']['name']
    answer['namespace'] = pod['metadata']['namespace']
    answer['phase'] = pod['status']['phase']
    answer['nodeName'] = pod['spec']['nodeName']
    if 'all' in args.report:
        answer['containerStatus'] = pod['status']['containerStatuses']
    return answer


def extract_controller_data(controller):
    answer = {}
    answer['kind'] = controller['kind']
    answer['name'] = controller['metadata']['name']
    answer['namespace'] = controller['metadata']['namespace']
    return answer


def extract_service_data(service):
    answer = {}
    answer['kind'] = service['kind']
    answer['name'] = service['metadata']['name']
    answer['namespace'] = service['metadata']['namespace']
    if 'all' in args.report:
        answer['spec'] = service['spec']


object_handler_map = {
    'Pod': extract_pod_data,
    'Deployment': extract_controller_data,
    'ReplicaSet': extract_controller_data,
    'DaemonSet': extract_controller_data,
    'Service': extract_service_data
}


def get_object(kind, name):
    api_object = openshift.selector('%s/%s' % (kind, name)).objects()[0].as_dict()
    if kind in object_handler_map:
        return object_handler_map[kind](api_object)
    else:
        return api_object


def cleanupNode(node):
    if 'everything' in args.report:
        return node
    else:
        answer = {}
        answer['name'] = node['metadata']['name']
        if 'all' in args.report:
            answer['labels'] = {}
            for label in node['metadata']['labels']:
                if re.compile('^node-role.kubernetes.io/').search(label):
                    answer['labels'][label] = node['metadata']['labels'][label]

            answer['nodeInfo'] = node['status']['nodeInfo']
            answer['allocatable'] = node['status']['allocatable']
            answer['capacity'] = node['status']['capacity']
        return answer


json_results = {}
json_api_objects = []
if json_output is not None:
    if 'results' in json_output:
        json_results = json_output['results']
        del json_output['results']

    if 'api_objects' in json_output:
        for api_object in json_output['api_objects']:
            bad_object = False
            for tag in ['kind', 'namespace', 'name']:
                if tag not in api_object:
                    eprint("API object %s does not contain a %s" % (api_object, tag))
                    bad_object = True
            if not bad_object:
                try:
                    with openshift.project(api_object['namespace']):
                        try:
                            apiobj = get_object(api_object['kind'], api_object['name'])
                            if apiobj is not None:
                                json_api_objects.append(apiobj)
                        except Exception as get_err:
                            eprint("Unable to retrieve object %s/%s in namespace %s: %s" %
                                   (api_object['kind'], api_object['name'], api_object['namespace'], get_err))
                except Exception as ns_err:
                    eprint("Unable to set namespace %s: %s" % (api_object['namespace'], ns_err))
        del json_output['api_objects']

    json_output.pop('results', None)

if args.post_settling_time > 0:
    eprint("Waiting %d seconds for complete metrics results" % args.post_settling_time)
    time.sleep(args.post_settling_time)
metricsEndTime = datetime.now(timezone.utc)

metric_results = {}

eprint("Collecting metrics...")
for metric in yaml['metrics']:
    if 'query' not in metric:
        continue
    metric_data = []
    if 'instant' not in metric or metric['instant'] is not True:
        metric_data = prom.custom_query_range(metric['query'], start_time=metricsStartTime, end_time=metricsEndTime, step=args.step)
    else:
        metric_data = prom.custom_query(metric['query'])
    for metric_instance in metric_data:
        try:
            if 'values' in metric_instance:
                for point in metric_instance['values']:
                    point[1] = round(float(point[1]))
            if 'value' in metric_instance:
                metric_instance['value'][1] = float(metric_instance['value'][1])
        except Exception as error:
            eprint('Unable to parse metric query %s: %s' % (metric['query'], error))
    name = metric['query']
    if 'metricName' in metric:
        name = metric['metricName']
    metric_results[name] = {
        'query': metric['query'],
        'name': name,
        'data': metric_data
    }

results = {
    'metadata': {
        'jobStart': startTime.isoformat(timespec='seconds'),
        'jobEnd': endTime.isoformat(timespec='seconds'),
        'uuid': args.uuid,
        'cluster_version': openshift.get_server_version(),
        'command': args.command,
    },
    'rundata': {
        'metrics': metric_results,
    }
}

if 'summary' in args.report:
    results['metadata']['nodes'] = [cleanupNode(n.as_dict()) for n in openshift.selector('nodes').objects()]

if 'everything' in args.report:
    results['rundata']['stderr'] = stderr_data,


if json is not None:
    if json_results:
        results['rundata']['results'] = json_results
    if json_api_objects:
        results['rundata']['api_objects'] = json_api_objects
    if json_output:
        results['rundata']['run_data'] = json_output
else:
    if stdout_data:
        results['rundata']['stdout'] = stdout_data


if args.es_host is not None or args.es_port is not None:
    if args.es_host is None or args.es_port is None:
        efail("Both host and port must be specified for Elasticsearch")
    es_operations = ESOperations(es_host=args.es_host, es_port=args.es_port)
    job_type = args.job_type
    if job_type is None:
        job_type = os.path.basename(args.command[0])
    print(str(job_type))
    es_operations.upload_to_es(data=results['rundata'], index=str(job_type),
                               es_add_items=results['metadata'])
else:
    print(json.dumps(results, indent=4))
